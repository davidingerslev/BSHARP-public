[
  {
    "objectID": "4.4_durations.html",
    "href": "4.4_durations.html",
    "title": "4.4 Results - Durations",
    "section": "",
    "text": "Code\n# Setup code\nimport pandas as pd\nfrom python_scripts import setup\nfrom python_scripts import services\nfrom python_scripts import helper\ndf, dffp = setup.setup(verbose=False)\n\nfrom python_scripts import distinct_pathways\ndpw_pls = distinct_pathways.get_distinct_pathways_routes(dffp)\nqrows = dpw_pls[\"rt_end_cat\"] == \"[Not ended or invalid end reason]\"\ndpw_pls.loc[qrows, \"rt_end_cat\"] = dpw_pls.loc[qrows, \"pl_end_dt\"].isna().map({True: \"[Not ended]\", False: \"Missing data/error\"})\ndpw_pls[\"rt_end_cat\"] = dpw_pls[\"rt_end_cat\"].str.replace(\"To care/hosp.\", \"To care/hospital\")\ndpw_rt_starts = dpw_pls.groupby(\"route_id\").head(1)\ndpw_rt_ends = dpw_pls.groupby(\"route_id\").tail(1)\ndpw_clis = dpw_pls.groupby([\"o_cli_id\"]).head(1)  # 1 duplicate client but first entry has more detailed answers\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nmatplotlib.rcParams.update({\n    'figure.figsize': (6.2, 4.1),\n    'font.size': 12,\n    'figure.constrained_layout.use': True\n})\n\nexcel_output = {}"
  },
  {
    "objectID": "4.4_durations.html#durations-rq4",
    "href": "4.4_durations.html#durations-rq4",
    "title": "4.4 Results - Durations",
    "section": "Durations (RQ4)",
    "text": "Durations (RQ4)\n\nFigure 4.7: Route durations\n\nCode\n\n\nCode\nimport numpy as np\nimport seaborn as sns\nimport seaborn.objects as so\nsns.set()\n# Set up data\n#############\ndurations = dpw_pls.set_index(\"route_id\")[[\"dur\", \"pl_start_dt\"]]\ndurations[\"dur_days\"] = durations[\"dur\"].dt.days\ndurations[\"dur_days\"] = durations[\"dur_days\"].fillna(1 + (distinct_pathways.dpw_end_dt - durations.pl_start_dt).dt.days)\ndurations = durations.drop(columns=[\"dur\", \"pl_start_dt\"])\nhist_data = (\n    durations.groupby(level=0).dur_days.sum().to_frame()\n    .join(dpw_rt_ends.set_index(\"route_id\")[\"pl_end_dt\"].notna().rename(\"ended\").astype(\"category\")))\nhist_data[\"ended\"] = pd.Categorical(hist_data[\"ended\"], categories=[True, False], ordered=True)\nhist_data[\"dur_years\"] = hist_data[\"dur_days\"] / 365.25\n# Utility function to add quantile markers\nhist_quantiles = hist_data.drop(columns=\"ended\").quantile([0.10, 0.90])  # To be used as markers later\ncolours = sns.color_palette(as_cmap=False)\ndef add_quantiles(p, field, x_limits, y_limits):\n    for q, row in hist_quantiles.iterrows():\n        pct = f\"{100 * q:.0f}%\"\n        x = row[field]\n        p = p.add(so.Line(linewidth=0.7, linestyle=(10, 6), alpha=0.35, color=colours[4]),\n                  data=pd.DataFrame({\"x\": [x, x], \"y\": y_limits}),\n                  x=\"x\", y=\"y\")\n        p = p.add(so.Text({\"clip_on\": False}, valign=\"bottom\", alpha=0.7, color=colours[4], fontsize=9, offset=0),\n                  data=pd.DataFrame({\"x\": [x], \"y\": [y_limits[1]], \"text\": [pct]}),\n                  x=\"x\", y=\"y\", text=\"text\")\n    p = p.add(so.Text({\"clip_on\": False}, valign=\"bottom\", halign=\"right\", alpha=0.7, color=colours[4], fontsize=9, offset=0),\n                data=pd.DataFrame({\"x\": [x_limits[1]], \"y\": [y_limits[1]], \"text\": \"percentile\"}),\n                x=\"x\", y=\"y\", text=\"text\")\n    return p\n# Generate human-readable units and tick positions for log timescales\n#####################################################################\ndays_logunits = {\"day\": 1, \"week\": 7, \"month\": 365.25/12, \"year\": 365.25}\ndays_loglists = {\n    \"days\": [1, 2, 4],\n    \"weeks\": [1, 2],\n    \"months\": [1, 2, 4, 8],\n    \"years\": [1, 2, 4, 8, 16]\n}\ndays_logscale = []\ndays_loglabels_u = []\ndays_loglabels_full = []\ndays_loglabels_grp = []\nfor u, l in days_loglists.items():\n    days_logscale.extend(days_logunits[u[:-1]] * i for i in l)\n    days_loglabels_u.extend(l)\n    days_loglabels_full.extend(f\"{i} {u if i &gt; 1 else u[:-1]}\" for i in l)\n    days_loglabels_grp.append(u)\ndays_logformatter = matplotlib.ticker.FixedFormatter(days_loglabels_u)\n# Calculate positions of separators between groups for log unit labels\ndays_logsep = []\nfor (a_u, a_l), (b_u, b_l) in zip(list(days_loglists.items())[:-1], list(days_loglists.items())[1:]):\n    a_end = days_logunits[a_u[:-1]] * a_l[-1]\n    b_start = days_logunits[b_u[:-1]] * b_l[0]\n    log_midpoint = 10 ** ((np.log10(a_end) + np.log10(b_start))/2)\n    days_logsep.append(log_midpoint)\n# Calculate positions of group labels for log units\ndays_loggrp_edges = [0.8] + days_logsep + [30 * days_logunits[\"year\"]]\ndays_logpos_grp = list(10 ** ((np.log10(a) + np.log10(b))/2) for a, b in zip(days_loggrp_edges[:-1], days_loggrp_edges[1:]))\n# Set up two subfigures\n#######################\nfig = plt.figure(figsize=(6.2, 2*4.1), layout=\"constrained\")\nfig0, fig1 = fig.subfigures(2)\n# Set up two subplots\n#####################\n# f, axs = plt.subplots(nrows=2, figsize=(6.2, 2*4.1), dpi=150)\n# Plot the log scale chart\n##########################\n# ax = axs[1]\nlimits = {\"x\": (0.7, 10**4), \"y\": (0, 831)}\n# plt.subplots_adjust(right=0.95, top=0.98, bottom=0.09, hspace=0.3)\np = (\n    so.Plot(data=hist_data, x=\"dur_days\")\n    .label(x=\"Duration (log scale)\", y=\"Routes\")\n    .add(so.Bars(), so.Hist(bins=20), so.Stack(), color=\"ended\", legend=False)\n    .scale(x=so.Continuous(trans=\"log\").tick(at=days_logscale).label(formatter=days_logformatter), color=so.Nominal(order=[True, False]))\n    .limit(x=limits[\"x\"], y=limits[\"y\"])\n)\n# Add extra readable unit labels (rotated 90 degrees) at the top\np = p.add(\n        so.Text({\"rotation\": 90, \"rotation_mode\": \"anchor\"}, color=\".7\", fontsize=8, halign=\"right\", valign=\"center\", offset=0),\n        data=pd.DataFrame({\"x\": days_logscale, \"y\": [limits[\"y\"][1]-15] * len(days_logscale), \"text\": days_loglabels_full}),\n        x=\"x\", y=\"y\", text=\"text\", legend=False\n)\n# Add quantile indicators\np = add_quantiles(p, \"dur_days\", limits[\"x\"], limits[\"y\"])\n# p = p.on(ax).plot()\nfig_4_7b = \"img_output/Figure_4.7_b.svg\"\np.save(fig_4_7b)\np = p.on(fig1).plot()\nax = fig1.axes[0]\nfor line in ax.get_lines():\n    line.zorder=1.1\n# Lines between unit groups\nax.vlines(days_logsep, -0.04, -0.15, color=\"black\", lw=0.5, clip_on=False, transform=ax.get_xaxis_transform())\n# Grouped unit labels\nfor pos, text in zip(days_logpos_grp, days_loglabels_grp):\n    ax.text(pos, -0.14, text, ha=\"center\", clip_on=False, transform=ax.get_xaxis_transform())\n# Move x-axis label down\nax.xaxis.labelpad = 20\n# Add rug plot\nsns.rugplot(ax=ax, data=hist_data, x=\"dur_days\", hue=\"ended\", height=-.03, clip_on=False, alpha=0.7, legend=False)\n# Plot the linear scale chart\n#############################\n# ax = axs[0]\nhist_data_lt_10y = hist_data[hist_data[\"dur_years\"] &lt;= 10]\nlimits = {\"x\": (0, 10.5), \"y\": (0, 2000)}\np = (\n    so.Plot(data=hist_data_lt_10y, x=\"dur_years\")\n    .label(x=\"Duration (linear: years)\", y=\"Routes\")\n    .add(so.Bars(), so.Hist(binwidth=0.5), so.Stack(), color=\"ended\", legend=False)\n    .scale(x=so.Continuous().tick(every=1).label(like=\"{x:.0f}\"), color=so.Nominal(order=[True, False]))\n    .limit(x=limits[\"x\"], y=limits[\"y\"])\n    .add(\n        so.Text({\"rotation\": 90, \"rotation_mode\": \"anchor\"}, color=\".7\", fontsize=9, halign=\"left\", valign=\"top\", offset=5),\n        data=pd.DataFrame({\"x\": [10], \"y\": [100], \"text\": [\"2 routes over 10 years not shown\"]}),\n        x=\"x\", y=\"y\", text=\"text\", legend=False)\n)\np = add_quantiles(p, \"dur_years\", limits[\"x\"], limits[\"y\"])\n# p = p.on(ax).plot()\nfig_4_7a = \"img_output/Figure_4.7_a.svg\"\np.save(fig_4_7a)\np = p.on(fig0).plot()\nax = fig0.axes[0]\nsns.rugplot(ax=ax, data=hist_data_lt_10y, x=\"dur_years\", hue=\"ended\", height=-.03, clip_on=False, alpha=0.7, legend=False)\n# Add legend\ncolours = sns.color_palette(as_cmap=False)\nT = matplotlib.patches.Patch(facecolor=colours[0], edgecolor=\"white\", alpha=0.8, label=\"Ended\")\nF = matplotlib.patches.Patch(facecolor=colours[1], edgecolor=\"white\", alpha=0.8, label=\"Current\")\nax.legend(handles=[F, T], loc=\"upper right\")\n# Output the chart\n##################\nfig_4_7 = \"img_output/Figure_4.7.svg\"\nplt.savefig(fig_4_7)\n\n\n\n\nFigure 4.7\n\nNotes: 3,755 people; 5,072 routes. 10% at 35 days, 90% at 3.08 years. Descriptive statistics are in Table A.6.\n\n\nTable A.6\n\n\nCode\ntbl_data = hist_data.copy()\n# tbl_data = hist_data[[\"ended\", \"dur_years\"]]\ntbl_data[\"ended\"] = hist_data[\"ended\"].map({True: \"Ended\", False: \"Current\"})\npercentiles = [0.1, 0.5, 0.9]\nta6 = (\n    pd.concat([\n        tbl_data.groupby(\"ended\", observed=True).describe(percentiles=percentiles),\n        tbl_data.describe(percentiles=percentiles).T.assign(ended=\"All\").set_index(\"ended\", append=True).stack().unstack(level=[0,2])\n    ])\n)\nta6 = ta6.astype(\"object\")\nint_cols = {\"dur_days\": [\"count\", \"min\", \"max\"], \"dur_years\": [\"count\"]}\nfloat_cols = {k: ta6.columns.levels[1].difference(v) for k, v in int_cols.items()}\nfloat_prec = {\"dur_days\": 1, \"dur_years\": 3}\nfor dt in [\"dur_days\", \"dur_years\"]:\n    fp = float_prec[dt]\n    ta6.loc[:, (dt, float_cols[dt])] = (ta6.loc[:, (dt, float_cols[dt])].map(lambda x: f\"{x:,.{fp}f}\"))\n    ta6.loc[:, (dt, int_cols[dt])] = (ta6.loc[:, (dt, int_cols[dt])].map(lambda x: f\"{x:,.0f}\"))\nta6 = (\n    ta6.T.reindex((a, b)\n                    for a in [\"dur_years\", \"dur_days\"]\n                    for b in [x for x in ta6[\"dur_days\"].columns if x != \"count\"] + [\"count\"]).T\n    .rename(columns={\"50%\": \"median\"})\n)\nta6 = pd.concat([ta6[\"dur_years\"], ta6[\"dur_days\"]], keys=[\"Years\", \"Days\"])\ndur_6y = tbl_data[tbl_data[\"dur_years\"] &gt; 6].ended.value_counts().to_frame()\ndur_0d = tbl_data[tbl_data[\"dur_days\"] == 0].ended.value_counts().to_frame()\nexcel_output[\"Table A.6: Descriptive statistics for route durations by whether ended/current\"] = ta6\ndisplay(ta6)\n\n\n\n\n\n\n\n\n\n\nmean\nstd\nmin\n10%\nmedian\n90%\nmax\ncount\n\n\n\nended\n\n\n\n\n\n\n\n\n\n\n\n\nYears\nEnded\n1.115\n1.223\n0.000\n0.090\n0.713\n2.669\n20.118\n4,334\n\n\nCurrent\n2.118\n2.129\n0.003\n0.151\n1.269\n5.472\n9.919\n738\n\n\nAll\n1.261\n1.436\n0.000\n0.096\n0.769\n3.080\n20.118\n5,072\n\n\nDays\nEnded\n407.2\n446.6\n0\n33.0\n260.5\n975.0\n7,348\n4,334\n\n\nCurrent\n773.6\n777.7\n1\n55.0\n463.5\n1,998.8\n3,623\n738\n\n\nAll\n460.5\n524.4\n0\n35.0\n281.0\n1,124.8\n7,348\n5,072\n\n\n\n\n\n\n\n\n\n\nFigure 4.8: Route durations by end reason\n\nCode\n\n\nCode\nimport numpy as np\nimport seaborn as sns\nimport seaborn.objects as so\nsns.set()\n# Set up data\n#############\ndurations = dpw_pls.set_index(\"route_id\")[[\"dur\", \"pl_start_dt\"]]\ndurations[\"dur_days\"] = durations[\"dur\"].dt.days\ndurations[\"dur_days\"] = durations[\"dur_days\"].fillna(1 + (distinct_pathways.dpw_end_dt - durations.pl_start_dt).dt.days)\ndurations = durations.drop(columns=[\"dur\", \"pl_start_dt\"])\nhist_data = (\n    durations.groupby(level=0).dur_days.sum().to_frame()\n    .join(dpw_rt_ends.set_index(\"route_id\")[\"pl_end_dt\"].notna().rename(\"ended\").astype(\"category\"))\n    .join(dpw_rt_ends.set_index(\"route_id\")[[\"rt_end_cat\", \"pl_end_dt\"]])\n)\nhist_data[\"ended\"] = pd.Categorical(hist_data[\"ended\"], categories=[True, False], ordered=True)\nhist_data[\"dur_years\"] = hist_data[\"dur_days\"] / 365.25\nqrows = hist_data[\"rt_end_cat\"] == \"[Not ended or invalid end reason]\"\nhist_data.loc[qrows, \"rt_end_cat\"] = hist_data.loc[qrows, \"pl_end_dt\"].isna().map({True: \"[Not ended]\", False: \"Missing data/error\"})\nhist_data = hist_data.drop(columns=\"pl_end_dt\")\ncolours = sns.color_palette(as_cmap=False)\n# Calculate positions of group labels for log units\ndays_loggrp_edges = [0.8] + days_logsep + [30 * days_logunits[\"year\"]]\ndays_logpos_grp = list(10 ** ((np.log10(a) + np.log10(b))/2) for a, b in zip(days_loggrp_edges[:-1], days_loggrp_edges[1:]))\n# Calculate order for facets\nfacet_order = (hist_data.groupby(\"rt_end_cat\").size().to_frame().reset_index()\n               .sort_values(0, ascending=False)[\"rt_end_cat\"].to_list())\n# Generate human-readable units and tick positions for log timescales\n#####################################################################\ndays_logunits = {\"day\": 1, \"week\": 7, \"month\": 365.25/12, \"year\": 365.25}\ndays_loglists = {\n    \"weeks\": [1],\n    \"months\": [1, 4],\n    \"years\": [1, 3, 9]\n}\ndays_logscale = []\ndays_loglabels_u = []\ndays_loglabels_full = []\ndays_loglabels_grp = []\nfor u, l in days_loglists.items():\n    days_logscale.extend(days_logunits[u[:-1]] * i for i in l)\n    # days_loglabels_u.extend(l)\n    days_loglabels_u.extend(f\"{x}{u[0]}\" for x in l)\n    days_loglabels_full.extend(f\"{i} {u if i &gt; 1 else u[:-1]}\" for i in l)\n    days_loglabels_grp.append(u)\ndays_logformatter = matplotlib.ticker.FixedFormatter(days_loglabels_u)\n# Plot log scale charts\n#######################\nlimits = {\"x\": (0.014*365.25, 10*365.25), \"y\": (None, 100)}\nfig_4_8_a = \"img_output/Figure_4.8_a.svg\"\np = (\n    so.Plot(data=hist_data, x=\"dur_days\")\n    # .label(x=\"Duration (log scale)\", y=\"Routes\")\n    .add(so.Bars(), so.Hist(bins=20), so.Stack(), color=\"ended\", legend=False)\n    .scale(x=so.Continuous(trans=\"log\").tick(at=days_logscale).label(formatter=days_logformatter),\n           color=so.Nominal(order=[True, False]))\n    .facet(\"rt_end_cat\", wrap=4, order=facet_order)\n    .limit(x=limits[\"x\"])\n    .save(fig_4_8_a)\n)\nfig_4_8_b = \"img_output/Figure_4.8_b.svg\"\np = (\n    so.Plot(data=hist_data, x=\"dur_days\")\n    # .label(x=\"Duration (log scale)\", y=\"Routes\")\n    .add(so.Bars(), so.Hist(bins=20), so.Stack(), color=\"ended\", legend=False)\n    .scale(x=so.Continuous(trans=\"log\").tick(at=days_logscale).label(formatter=days_logformatter),\n           color=so.Nominal(order=[True, False]))\n    .facet(\"rt_end_cat\", wrap=4, order=facet_order)\n    .limit(x=limits[\"x\"], y=limits[\"y\"])\n    .save(fig_4_8_b)\n)\n\n\n\n\nFigure 4.8\n\nIncreased-scale y-axis (zoomed in):\n\nNote: Descriptive statistics are in Table A.7.\n\n\nTable A.7\n\n\nCode\ntbl_data = hist_data.copy()\n# tbl_data = hist_data[[\"ended\", \"dur_years\"]]\ntbl_data[\"ended\"] = hist_data[\"ended\"].map({True: \"Ended\", False: \"Current\"})\npercentiles = [0.1, 0.5, 0.9]\nta7 = (\n    pd.concat([\n        (tbl_data.groupby(\"rt_end_cat\", observed=True).describe(percentiles=percentiles)\n         .sort_values((\"dur_days\", \"mean\"), ascending=False)),\n        tbl_data.describe(percentiles=percentiles).T.assign(rt_end_cat=\"All\").set_index(\"rt_end_cat\", append=True).stack().unstack(level=[0,2])\n    ])\n)\nta7 = ta7.astype(\"object\")\nint_cols = {\"dur_days\": [\"count\", \"min\", \"max\"], \"dur_years\": [\"count\"]}\nfloat_cols = {k: ta7.columns.levels[1].difference(v) for k, v in int_cols.items()}\nfloat_prec = {\"dur_days\": 1, \"dur_years\": 3}\nfor dt in [\"dur_days\", \"dur_years\"]:\n    fp = float_prec[dt]\n    ta7.loc[:, (dt, float_cols[dt])] = (ta7.loc[:, (dt, float_cols[dt])].map(lambda x: f\"{x:,.{fp}f}\"))\n    ta7.loc[:, (dt, int_cols[dt])] = (ta7.loc[:, (dt, int_cols[dt])].map(lambda x: f\"{x:,.0f}\"))\nta7 = (\n    ta7.T.reindex((a, b)\n                    for a in [\"dur_years\", \"dur_days\"]\n                    for b in [x for x in ta7[\"dur_days\"].columns if x != \"count\"] + [\"count\"]).T\n    .rename(columns={\"50%\": \"median\"})\n)\nta7 = pd.concat([ta7[\"dur_years\"], ta7[\"dur_days\"]], keys=[\"Years\", \"Days\"])\nexcel_output[\"Table A.7: Descriptive statistics for route durations by end reasons\"] = ta7\ndisplay(ta7)\n\n\n\n\n\n\n\n\n\n\nmean\nstd\nmin\n10%\nmedian\n90%\nmax\ncount\n\n\n\nrt_end_cat\n\n\n\n\n\n\n\n\n\n\n\n\nYears\n[Not ended]\n2.118\n2.129\n0.003\n0.151\n1.269\n5.472\n9.919\n738\n\n\nTo social housing\n1.877\n1.265\n0.027\n0.482\n1.643\n3.569\n7.291\n999\n\n\nDied\n1.757\n2.299\n0.008\n0.151\n1.054\n3.595\n20.118\n117\n\n\nTo private rented\n1.205\n1.053\n0.000\n0.199\n0.883\n2.471\n5.591\n278\n\n\nTo care/hospital\n1.190\n1.581\n0.003\n0.077\n0.665\n2.919\n12.394\n171\n\n\nTo external supported\n1.079\n1.246\n0.011\n0.090\n0.665\n2.494\n7.294\n167\n\n\nOther\n0.928\n0.956\n0.000\n0.095\n0.575\n2.225\n5.172\n289\n\n\nEvicted\n0.923\n1.101\n0.000\n0.118\n0.537\n2.200\n7.885\n867\n\n\nMissing data/error\n0.881\n1.126\n0.000\n0.047\n0.444\n2.317\n5.481\n95\n\n\nTo friends/family\n0.729\n0.734\n0.003\n0.060\n0.490\n1.789\n3.751\n376\n\n\nAbandoned\n0.624\n0.855\n0.000\n0.036\n0.268\n1.691\n6.486\n639\n\n\nCustody\n0.616\n0.781\n0.000\n0.051\n0.337\n1.577\n5.325\n336\n\n\nAll\n1.261\n1.436\n0.000\n0.096\n0.769\n3.080\n20.118\n5,072\n\n\nDays\n[Not ended]\n773.6\n777.7\n1\n55.0\n463.5\n1,998.8\n3,623\n738\n\n\nTo social housing\n685.4\n462.1\n10\n176.2\n600.0\n1,303.6\n2,663\n999\n\n\nDied\n641.7\n839.6\n3\n55.0\n385.0\n1,313.2\n7,348\n117\n\n\nTo private rented\n440.3\n384.5\n0\n72.7\n322.5\n902.6\n2,042\n278\n\n\nTo care/hospital\n434.8\n577.5\n1\n28.0\n243.0\n1,066.0\n4,527\n171\n\n\nTo external supported\n394.1\n455.0\n4\n33.0\n243.0\n910.8\n2,664\n167\n\n\nOther\n339.1\n349.3\n0\n34.6\n210.0\n812.6\n1,889\n289\n\n\nEvicted\n337.3\n402.3\n0\n43.0\n196.0\n803.6\n2,880\n867\n\n\nMissing data/error\n321.6\n411.3\n0\n17.0\n162.0\n846.2\n2,002\n95\n\n\nTo friends/family\n266.2\n268.0\n1\n22.0\n179.0\n653.5\n1,370\n376\n\n\nAbandoned\n227.8\n312.3\n0\n13.0\n98.0\n617.6\n2,369\n639\n\n\nCustody\n225.1\n285.2\n0\n18.5\n123.0\n576.0\n1,945\n336\n\n\nAll\n460.5\n524.4\n0\n35.0\n281.0\n1,124.8\n7,348\n5,072"
  },
  {
    "objectID": "4.4_durations.html#tables-in-an-excel-file",
    "href": "4.4_durations.html#tables-in-an-excel-file",
    "title": "4.4 Results - Durations",
    "section": "Tables in an Excel file",
    "text": "Tables in an Excel file\n\nCode\nimport os\nsheetname = \"Sheet1\"\nqmdfile = os.getenv(\"QUARTO_DOCUMENT_FILE\")\nfilename = \"img_output/\"+qmdfile[:-4]+\".xlsx\"\nwith pd.ExcelWriter(filename) as writer:\n    next_row = 0\n    for k, v in excel_output.items():\n        pd.DataFrame([k]).to_excel(writer, sheet_name=sheetname, header=False, index=False, startrow=next_row)\n        next_row += 1\n        v.to_excel(writer, sheet_name=sheetname, startrow=next_row, float_format=\"%.1f\")\n        next_row += (\n            (1 if not isinstance(v.columns, pd.MultiIndex) else 1 + len(v.columns.levels))\n            + len(v) + 1)\nprint(f\"[Excel file](./{filename})\")\n\nExcel file"
  },
  {
    "objectID": "4.2_capacity_and_throughput.html",
    "href": "4.2_capacity_and_throughput.html",
    "title": "4.2 Results - Capacity and throughput",
    "section": "",
    "text": "Code\n%config InlineBackend.figure_formats = ['svg']\n# Setup code\nimport pandas as pd\nfrom python_scripts import setup\nfrom python_scripts import services\nfrom python_scripts import helper\ndf, dffp = setup.setup(verbose=False)\n\nfrom python_scripts import distinct_pathways\ndpw_pls = distinct_pathways.get_distinct_pathways_routes(dffp)\nqrows = dpw_pls[\"rt_end_cat\"] == \"[Not ended or invalid end reason]\"\ndpw_pls.loc[qrows, \"rt_end_cat\"] = dpw_pls.loc[qrows, \"pl_end_dt\"].isna().map({True: \"[Not ended]\", False: \"Missing data/error\"})\ndpw_pls[\"rt_end_cat\"] = dpw_pls[\"rt_end_cat\"].str.replace(\"To care/hosp.\", \"To care/hospital\")\ndpw_rt_starts = dpw_pls.groupby(\"route_id\").head(1)\ndpw_rt_ends = dpw_pls.groupby(\"route_id\").tail(1)\ndpw_clis = dpw_pls.groupby([\"o_cli_id\"]).head(1)  # 1 duplicate client but first entry has more detailed answers\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nmatplotlib.rcParams.update({\n    'figure.figsize': (6.2, 4.1),\n    'font.size': 12,\n    'figure.constrained_layout.use': True\n})\n\nasset_themes = {\"opaque\": \"assets_output/\",\n                \"transparent_lightbg\": \"assets_transparent_lightbg/\",\n                \"transparent_darkbg\": \"assets_transparent_darkbg/\"}\nassets_output = asset_themes[\"opaque\"]\nexcel_output = {}"
  },
  {
    "objectID": "4.2_capacity_and_throughput.html#capacity-and-throughput-rq2",
    "href": "4.2_capacity_and_throughput.html#capacity-and-throughput-rq2",
    "title": "4.2 Results - Capacity and throughput",
    "section": "Capacity and throughput (RQ2)",
    "text": "Capacity and throughput (RQ2)\n\nFigure 4.3: Allocated placements per night\n\nCode\n\n\nCode\nfirst_date = pd.Timestamp('2017-10-28')\nlast_date = pd.Timestamp('2025-04-30')\ndate_range = pd.date_range(first_date, last_date, freq=\"d\")\n\nvoidsdf = df.all[\n    (~df.all.cli_id.isin(dffp.cli_id)) &\n    df.all.svc_type.isin(services.distinct_pathways_accom_svc_types + [\"Accommodation Based - Supported Move-on\"])\n]\ntempdf = dffp\ntempdf = pd.concat([\n    tempdf.loc[\n        tempdf.svc_type.isin(services.distinct_pathways_accom_svc_types + [\"Accommodation Based - Supported Move-on\"]),\n        [\"vac_start_dt\", \"pl_start_dt\", \"pl_end_dt\", \"vac_end_dt\", \"svc_type\"]\n    ],\n    voidsdf.assign(pl_start_dt=pd.NA, pl_end_dt = pd.NA)[[\"vac_start_dt\", \"pl_start_dt\", \"pl_end_dt\", \"vac_end_dt\", \"svc_type\"]]\n])\ntempdf.vac_end_dt = tempdf.vac_end_dt.fillna(last_date + pd.Timedelta(days=1))\ntempdf.pl_end_dt = tempdf.pl_end_dt.fillna(tempdf.vac_end_dt)\nnightly_vac = []\nfor day in date_range:\n    dpw_voids = (\n        tempdf.svc_type.isin(services.distinct_pathways_accom_svc_types) &\n        (tempdf.vac_start_dt &lt;= day) & (tempdf.vac_end_dt &gt; day) &\n        (\n            tempdf.pl_start_dt.isnull() |\n            (tempdf.pl_start_dt &gt; day) |\n            (tempdf.pl_end_dt &lt;= day)\n        )\n    ).sum()\n    dpw_stays = (\n        tempdf.svc_type.isin(services.distinct_pathways_accom_svc_types) &\n        (tempdf.pl_start_dt &lt;= day) & (tempdf.pl_end_dt &gt; day)\n    ).sum()\n    non_dpw_voids = (\n        (~tempdf.svc_type.isin(services.distinct_pathways_accom_svc_types)) &\n        (tempdf.vac_start_dt &lt;= day) & (tempdf.vac_end_dt &gt; day) &\n        (\n            tempdf.pl_start_dt.isnull() |\n            (tempdf.pl_start_dt &gt; day) |\n            (tempdf.pl_end_dt &lt;= day)\n        )\n    ).sum()\n    non_dpw_stays = (\n        (~tempdf.svc_type.isin(services.distinct_pathways_accom_svc_types)) &\n        (tempdf.pl_start_dt &lt;= day) & (tempdf.pl_end_dt &gt; day)\n    ).sum()\n    nightly_vac.append({\n        \"day\": day,\n        \"dpw_stays\": dpw_stays,\n        \"dpw_voids\": dpw_voids,\n        \"non_dpw_stays\": non_dpw_stays,\n        \"non_dpw_voids\": non_dpw_voids\n    })\nnightly_vac_df = pd.DataFrame.from_dict(nightly_vac)\n\n# Nightly\nnightly_ave = nightly_vac_df.copy().set_index(\"day\")\nnightly_ave = (\n    nightly_ave.groupby(nightly_ave.index.to_period(\"d\").start_time)\n    .mean().reset_index()\n)\n\n# Prepare graph\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nymax = 920\nalpha = 0.8\naxs = [None]\n\n\ndef cust_plot(output_dir, fig_filename, rc_params={}, transparent=False):\n    with plt.rc_context():\n        plt.rcParams.update({'font.size': 12,\n                             'figure.figsize': (7.1, 4.8),\n                             'figure.constrained_layout.use': True,\n                             'xtick.color': 'lightgrey',\n                             'xtick.labelcolor': 'grey',\n                             'ytick.color': 'lightgrey',\n                             'ytick.labelcolor': 'grey'\n                             } | rc_params)\n        fig, axs[0] = plt.subplots()\n        axs[0].stackplot(\n            nightly_ave.day,\n            [nightly_ave.dpw_stays],\n            colors=[\"C2\", \"C4\"],\n            alpha=alpha,\n            lw=0,\n        )\n        axs[0].set_ylim(0, ymax)\n        axs[0].yaxis.set_label_text(\"Allocated placements\")\n        axs[0].xaxis.set_label_text(\"Date\")\n        fig.legends = []\n\n        for ax in axs:\n            ax.set_xlim(first_date, last_date)\n            ax.yaxis.set_minor_locator(locator=matplotlib.ticker.MultipleLocator(100))\n            ax.yaxis.grid(True, which=\"minor\")\n            ax.xaxis.set_major_locator(locator=matplotlib.dates.YearLocator(base=1))\n            ax.xaxis.set_minor_locator(locator=matplotlib.dates.MonthLocator(bymonth=7))\n            ax.xaxis.grid(True, which=\"minor\")\n            ax.tick_params(bottom=True, left=True)\n\n        plt.savefig(output_dir + fig_filename, transparent=transparent)\n\n\nfig_4_3 = \"Figure_4.3.svg\"\nfig_filename = fig_4_3\ncust_plot(asset_themes[\"opaque\"], fig_filename)\nlighttheme = {\"axes.edgecolor\": (1,1,1,0),\n              \"grid.alpha\": 0.5,\n              \"grid.color\": \".15\",\n              \"xtick.color\": \".15\",\n              \"xtick.labelcolor\": \".15\",\n              \"ytick.color\": \".15\",\n              \"ytick.labelcolor\": \".15\"\n              }\ncust_plot(asset_themes[\"transparent_lightbg\"], fig_filename, lighttheme, transparent=True)\ndarktheme = {\"text.color\": \"white\",\n             \"axes.edgecolor\": (1,1,1,0), \"axes.labelcolor\": \"white\",\n             \"xtick.color\": \"white\", \"xtick.labelcolor\": \"white\",\n             \"ytick.color\": \"white\", \"ytick.labelcolor\": \"white\",\n             \"grid.alpha\": 0.5\n             }\ncust_plot(asset_themes[\"transparent_darkbg\"], fig_filename, darktheme, transparent=True)\n\npass\n\n\n\n\nFigure 4.3\n\nNotes: Date range 28/10/2017–30/04/2025\n\n\n\nFigure 4.4: Exits from BAHSA by calendar year\n\nCode\n\n\nCode\n# Prepare data\ndf_route_ends = dpw_pls.groupby('route_id').tail(1)\nct_df = df_route_ends.copy().assign(end_yr=df_route_ends.pl_end_dt.dt.year)\nct_df.end_yr = ct_df.end_yr.astype(\"Int64\")\nct_df.loc[\n    ct_df.rt_end_cat.isin([\"[Not ended or invalid end reason]\", \"Missing data/error\"]),\n    \"rt_end_cat\"\n] = \"Missing data/errors\"\nct_df[\"Pathway\"] = ct_df.svc_type_short.rename(\n    {\"Male Only Pathway\": \"M\", \"Female Only Pathway\": \"F\",\n    \"Mixed Pathway\": \"M/F\", \"Substance Misuse Pathway\": \"SU\"})\n\n# Plot the chart\nimport seaborn.objects as so\nplotdata = (\n    ct_df.groupby(\"end_yr\").size().drop([2017,2025]).to_frame(name=\"exits\")\n    .reset_index()\n    .rename(columns={\"end_yr\": \"x\", \"exits\": \"y\"})\n)\nlimits = (plotdata[\"y\"].min(), plotdata[\"y\"].max())\n\n\ndef cust_plot(rc_params={}, guide_color=\".6\"):\n    p = (\n        so.Plot(plotdata, x=\"x\", y=\"y\")\n        .label(x=\"Year\", y=\"Exits\")\n        .limit(y=(0, 725), x=(2018, 2024))\n        .theme(rc_params)\n    )\n    limpos = 2022.5\n    for lim in limits:\n        p = p.add(so.Line(linestyle=(3,6), color=guide_color, linewidth=0.8), data=pd.DataFrame({\"x\": [2018, 2024], \"y\": [lim] * 2}))\n    p = (\n        p.add(so.Line(linewidth=3))\n        .add(so.Dot())\n        .add(so.Area())\n        .add(\n            so.Text({\"clip_on\": False}, halign=\"right\", valign=\"center\", fontsize=9, color=guide_color, offset=1),\n            data=pd.DataFrame({\"x\": [2018] * 2, \"y\": list(limits), \"text\": [str(x) for x in limits]}),\n            x=\"x\", y=\"y\", text=\"text\"\n        )\n    )\n    return p\n\n\nfig_4_4 = \"Figure_4.4.svg\"\nfig_filename = fig_4_4\ncust_plot().save(asset_themes[\"opaque\"] + fig_filename)\nlighttheme = {\"axes.edgecolor\": (1,1,1,0),\n              \"grid.alpha\": 0.5,\n              \"grid.color\": \".15\"}\ncust_plot(lighttheme).save(asset_themes[\"transparent_lightbg\"] + fig_filename, transparent=True)\ndarktheme = {\"text.color\": \"white\",\n             \"axes.edgecolor\": (1,1,1,0), \"axes.labelcolor\": \"white\",\n             \"xtick.color\": \"white\", \"xtick.labelcolor\": \"white\",\n             \"ytick.color\": \"white\", \"ytick.labelcolor\": \"white\",\n             \"grid.alpha\": 0.5}\ncust_plot(darktheme, guide_color=\".8\").save(asset_themes[\"transparent_darkbg\"] + fig_filename, transparent=True)\npass\n\n\n\n\nFigure 4.4\n\nNotes: Full data table is Table A.2. People can have more than one exit in a year. Moves within or between services and incomplete years (2017 and 2025) are excluded.\n\n\n\nFigure 4.5: Exits from BAHSA by calendar year and end reason\n\nCode\n\n\nCode\nct = (\n    pd.crosstab(\n        ct_df.end_yr.rename(\"Year\"),\n        [ct_df.rt_end_cat.rename(\"End reason\"), ct_df[\"Pathway\"]],\n        normalize=False,\n        margins=True\n    ).drop(index=[2017, 2025])  # Drop incomplete/unusual years\n    .sort_values(by=\"All\", axis=1, ascending=False)\n    .drop(index=\"All\", columns=\"All\")\n    .unstack()\n    .rename(\"exits\")\n    .reset_index()\n)\nct[\"End reason\"] = ct[\"End reason\"].replace({\n    \"To social housing\": \"Social housing\",\n    \"To friends/family\": \"Friends/family\",\n    \"To private rented\": \"Private rented\",\n    \"To care/hospital\": \"Care/hospital\",\n    \"To external supported\": \"Supported¹\",\n})\nct_all = ct.drop(columns=\"Pathway\").groupby([\"End reason\", \"Year\"], sort=False).sum().reset_index().assign(Pathway=\"All\")\nn_ends = ct[\"End reason\"].nunique()\norder = [\"Social housing\", \"Private rented\", \"Friends/family\", \"Supported¹\",\n         \"Evicted\", \"Abandoned\", \"Custody\", \"Care/hospital\",\n         \"Other\", \"Died\", \"Missing data/errors\"]\n\n\ndef cust_plot(rc_params={}, guide_color=\".6\", guide_alpha=0.8, line_alpha=0.8):\n    return (\n        so.Plot(data=ct_all, x=\"Year\", y=\"exits\")\n        .theme({\"axes.grid.which\": \"both\"} | rc_params)\n        .label(x=\"\", y=\"\")\n        .layout(size=(7, 4.1))\n        .limit(y=(0, 200), x=(2018, 2024))\n        .add(so.Dot(pointsize=3, alpha=line_alpha), legend=False)\n        .add(so.Line(alpha=line_alpha), legend=False)\n        .add(so.Line(color=guide_color, alpha=guide_alpha, linestyle=(4, 2)), so.PolyFit(1), legend=False)\n        .scale(x=so.Continuous().tick(every=9, minor=8))\n        .facet(\"End reason\", wrap=4, order=order)\n    )\n\n\nfig_4_5 = \"Figure_4.5.svg\"\nfig_filename = fig_4_5\ncust_plot().save(asset_themes[\"opaque\"] + fig_filename)\nlighttheme = {\"axes.edgecolor\": (1,1,1,0),\n              \"grid.alpha\": 0.5,\n              \"grid.color\": \".15\"}\ncust_plot(lighttheme).save(asset_themes[\"transparent_lightbg\"] + fig_filename, transparent=True)\ndarktheme = {\"text.color\": \"white\",\n             \"axes.edgecolor\": (1,1,1,0), \"axes.labelcolor\": \"white\",\n             \"xtick.color\": \"white\", \"xtick.labelcolor\": \"white\",\n             \"ytick.color\": \"white\", \"ytick.labelcolor\": \"white\",\n             \"grid.alpha\": 0.2}\ncust_plot(darktheme, guide_color=\".8\", guide_alpha=0.65, line_alpha=1).save(asset_themes[\"transparent_darkbg\"] + fig_filename, transparent=True)\npass\n\n\n\n\nFigure 4.5\n\nNotes: Period covered is 2018-2024, vertical gridlines represent years 2019-2023, as in Figure 4.4. Full data table is Table A.2. 1. ‘Supported’ is moves to support services outside BAHSA.\n\n\nTable A.2\n\n\nCode\nta2 = (\n    pd.crosstab(\n        ct_df.rt_end_cat.rename(\"Exit reason\"), \n        ct_df.end_yr.rename(\"Year\"),\n        normalize=False,\n        margins=True\n    )\n    .sort_values(by=\"All\", axis=0, ascending=False)\n    .pipe(lambda df: df.reindex(list(df.index.drop(\"All\")) + [\"All\"]))  # Move total back to bottom\n)\nyears = ta2.columns.values[:-1]\ndays = ([(pd.Timestamp(f\"{years[0]+1}-01-01\") - distinct_pathways.dpw_start_dt).days] +\n        [(pd.Timestamp(f\"{y+1}-01-01\") - pd.Timestamp(f\"{y}-01-01\")).days for y in years[1:-1]] +\n        [(distinct_pathways.dpw_end_dt - pd.Timestamp(f\"{years[-1]}-01-01\")).days])\nta2.columns = pd.MultiIndex.from_arrays([ta2.columns.values, days + [sum(days)]], names=(\"Year\", \"Days\"))\nexcel_output[\"Table A.2: Exits from BAHSA per calendar year by end reasons\"] = ta2\ndisplay(ta2)\n\n\n\n\n\n\n\n\nYear\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n2024\n2025\nAll\n\n\nDays\n65\n365\n365\n366\n365\n365\n365\n366\n119\n2741\n\n\nExit reason\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo social housing\n35\n177\n154\n178\n134\n125\n79\n88\n29\n999\n\n\nEvicted\n21\n101\n127\n101\n112\n121\n133\n109\n42\n867\n\n\nAbandoned\n18\n87\n100\n83\n83\n76\n98\n79\n15\n639\n\n\nTo friends/family\n14\n74\n73\n49\n47\n53\n35\n15\n16\n376\n\n\nCustody\n5\n51\n44\n61\n57\n40\n41\n26\n11\n336\n\n\nOther\n8\n42\n30\n38\n44\n40\n27\n44\n16\n289\n\n\nTo private rented\n7\n30\n42\n64\n39\n35\n19\n33\n9\n278\n\n\nTo care/hospital\n4\n26\n31\n20\n26\n13\n19\n24\n8\n171\n\n\nTo external supported\n5\n40\n16\n20\n25\n19\n16\n18\n8\n167\n\n\nDied\n2\n17\n17\n16\n20\n13\n14\n15\n3\n117\n\n\nMissing data/errors\n2\n23\n15\n5\n8\n5\n12\n4\n21\n95\n\n\nAll\n121\n668\n649\n635\n595\n540\n493\n455\n178\n4334"
  },
  {
    "objectID": "4.2_capacity_and_throughput.html#tables-in-an-excel-file",
    "href": "4.2_capacity_and_throughput.html#tables-in-an-excel-file",
    "title": "4.2 Results - Capacity and throughput",
    "section": "Tables in an Excel file",
    "text": "Tables in an Excel file\n\nCode\nimport os\nsheetname = \"Sheet1\"\nqmdfile = os.getenv(\"QUARTO_DOCUMENT_FILE\")\nfilename = \"img_output/\"+qmdfile[:-4]+\".xlsx\"\nwith pd.ExcelWriter(filename) as writer:\n    next_row = 0\n    for k, v in excel_output.items():\n        pd.DataFrame([k]).to_excel(writer, sheet_name=sheetname, header=False, index=False, startrow=next_row)\n        next_row += 1\n        v.to_excel(writer, sheet_name=sheetname, startrow=next_row, float_format=\"%.1f\")\n        next_row += (\n            (1 if not isinstance(v.columns, pd.MultiIndex) else 1 + len(v.columns.levels))\n            + len(v) + 1)\nprint(f\"[Excel file](./{filename})\")\n\nExcel file"
  },
  {
    "objectID": "4.1_outcomes.html",
    "href": "4.1_outcomes.html",
    "title": "4.1 Results - Outcomes",
    "section": "",
    "text": "Code\n# Setup code\nimport pandas as pd\nfrom python_scripts import setup\nfrom python_scripts import services\nfrom python_scripts import helper\ndf, dffp = setup.setup(verbose=False)\n\nfrom python_scripts import distinct_pathways\ndpw_pls = distinct_pathways.get_distinct_pathways_routes(dffp)\nqrows = dpw_pls[\"rt_end_cat\"] == \"[Not ended or invalid end reason]\"\ndpw_pls.loc[qrows, \"rt_end_cat\"] = dpw_pls.loc[qrows, \"pl_end_dt\"].isna().map({True: \"[Not ended]\", False: \"Missing data/error\"})\ndpw_pls[\"rt_end_cat\"] = dpw_pls[\"rt_end_cat\"].str.replace(\"To care/hosp.\", \"To care/hospital\")\ndpw_rt_starts = dpw_pls.groupby(\"route_id\").head(1)\ndpw_rt_ends = dpw_pls.groupby(\"route_id\").tail(1)\ndpw_clis = dpw_pls.groupby([\"o_cli_id\"]).head(1)  # 1 duplicate client but first entry has more detailed answers\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nmatplotlib.rcParams.update({\n    'figure.figsize': (6.2, 4.1),\n    'font.size': 12,\n    'figure.constrained_layout.use': True\n})\n\nasset_themes = {\"opaque\": \"assets_output/\",\n                \"transparent_lightbg\": \"assets_transparent_lightbg/\",\n                \"transparent_darkbg\": \"assets_transparent_darkbg/\"}\nassets_output = asset_themes[\"opaque\"]\nexcel_output = {}"
  },
  {
    "objectID": "4.1_outcomes.html#outcomes-rq1",
    "href": "4.1_outcomes.html#outcomes-rq1",
    "title": "4.1 Results - Outcomes",
    "section": "Outcomes (RQ1)",
    "text": "Outcomes (RQ1)\n\nFigure 4.1: Route end reason proportions\n\nCode\n\n\nCode\n# Generate Table A.3\nta3_data = dpw_rt_ends.copy()\nta3 = ta3_data[\"rt_end_cat\"].value_counts(dropna=False).rename(\"all_routes\").to_frame()\nta3 = ta3.assign(latest_route=ta3_data.groupby(\"o_cli_id\").tail(1)[\"rt_end_cat\"].value_counts(dropna=False))\nta3_end_rows = [\"Missing data/error\", \"[Not ended]\"]\nta3 = ta3.reindex([x for x in ta3.index if x not in ta3_end_rows] + ta3_end_rows)\nprop_include = ta3.index.difference([\"Missing data/error\", \"[Not ended]\"])\nta3_prop = ta3.loc[prop_include].apply(lambda x: x/x.sum()).mul(100)\nta3 = pd.concat([ta3, ta3_prop], keys=[\"n\", \"%\"], axis=\"columns\").swaplevel(axis=\"columns\")\nlvl1 = [\"all_routes\", \"latest_route\"]\nta3 = ta3.reindex(((a, b) for a in lvl1 for b in [\"n\", \"%\"]), axis=\"columns\")\nta3_numeric = ta3.copy()\nta3_total = ta3.sum()\nta3_total.loc[slice(None), \"%\"] = pd.NA\nta3.loc[\"Valid ended (n)\"] = ta3.loc[prop_include].sum()\nta3.loc[\"Total (n)\"] = ta3_total\nta3.loc[:, (lvl1, \"n\")] = ta3.loc[:, (lvl1, \"n\")].map(lambda x: f\"{x:,.0f}\" if not pd.isna(x) else pd.NA)\nta3.loc[:, (lvl1, \"%\")] = ta3.loc[:, (lvl1, \"%\")].map(lambda x: f\"{x:.1f}%\" if not pd.isna(x) else pd.NA)\nend_props_order = ta3.index\nexcel_output[\"Table A.3: Route end reasons\"] = ta3\n\n# Generate Figure 4.1\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\nrelevant_numeric = ta3_numeric\nmissing = [\"Missing data/error\", \"[Not ended]\"]\ntotal_n = relevant_numeric.swaplevel(axis=\"columns\")[\"n\"].sum()\ntotal_na = relevant_numeric.swaplevel(axis=\"columns\").loc[missing][\"n\"].sum()\nvalid_n = total_n - total_na\nplotdata = (\n    relevant_numeric.drop(index=missing)\n    .swaplevel(axis=\"columns\")[\"%\"].unstack().rename(\"%\").to_frame().reset_index()\n    .rename(columns={\"level_0\": \"routes\"})\n    .replace({\"all_routes\": f\"Per route (all ended routes)\\n$n$={valid_n[\"all_routes\"]:,}\",\n              \"latest_route\": f\"Per person (latest route)\\n$n$={valid_n[\"latest_route\"]:,}\"})\n)\nplotlabels = pd.DataFrame({\"x\": -8.75, \"y\": plotdata[\"rt_end_cat\"], \"text\": plotdata[\"%\"].map(lambda x: f\"{x:.1f}%\")})\ncolors = sns.color_palette()\ncolors = colors[4:6]\n\n\ndef cust_plot(rc_params={}):\n    return (\n        so.Plot(data=plotdata, y=\"rt_end_cat\", x=\"%\", color=\"routes\")\n        .theme({\"ytick.major.pad\": 35, \"font.size\": 12, \"axes.titlesize\": \"medium\"} | rc_params)\n        .add(so.Bar(edgewidth=0), legend=False)\n        .add(so.Text({\"clip_on\": False}, halign=\"left\"), data=plotlabels, x=\"x\", y=\"y\", text=\"text\", legend=False)\n        .label(y=None, x=None)\n        .scale(x=so.Continuous().label(like=\"{x:.0f}%\"), color=colors)\n        .limit(x=(0, None))\n        .facet(col=\"routes\")\n        .layout(size=(7.2, 3.7), engine=\"constrained\")\n    )\n\n\nfig_4_1 = \"Figure_4.1.svg\"\nfig_filename = fig_4_1\ncust_plot().save(asset_themes[\"opaque\"] + fig_filename)\nlighttheme = {\"axes.edgecolor\": (1,1,1,0),\n              \"grid.alpha\": 0.5,\n              \"grid.color\": \".15\"}\ncust_plot(lighttheme).save(asset_themes[\"transparent_lightbg\"] + fig_filename, transparent=True)\ndarktheme = {\"text.color\": \"white\",\n             \"axes.edgecolor\": (1,1,1,0), \"axes.labelcolor\": \"white\",\n             \"xtick.color\": \"white\", \"xtick.labelcolor\": \"white\",\n             \"ytick.color\": \"white\", \"ytick.labelcolor\": \"white\",\n             \"grid.alpha\": 0.5}\ncust_plot(darktheme).save(asset_themes[\"transparent_darkbg\"] + fig_filename, transparent=True)\npass\n\n\n\n\nFigure 4.1\n\nNotes: Full data table is Table A.3. “Latest” is the most recent route for each person.\n\n\nTable A.3\n\n\n\n\n\n\n\n\n\nall_routes\nlatest_route\n\n\n\nn\n%\nn\n%\n\n\nrt_end_cat\n\n\n\n\n\n\n\n\nTo social housing\n999\n23.6%\n978\n33.2%\n\n\nEvicted\n867\n20.5%\n415\n14.1%\n\n\nAbandoned\n639\n15.1%\n339\n11.5%\n\n\nTo friends/family\n376\n8.9%\n281\n9.5%\n\n\nCustody\n336\n7.9%\n148\n5.0%\n\n\nOther\n289\n6.8%\n204\n6.9%\n\n\nTo private rented\n278\n6.6%\n249\n8.5%\n\n\nTo care/hospital\n171\n4.0%\n102\n3.5%\n\n\nTo external supported\n167\n3.9%\n113\n3.8%\n\n\nDied\n117\n2.8%\n117\n4.0%\n\n\nMissing data/error\n95\n&lt;NA&gt;\n71\n&lt;NA&gt;\n\n\n[Not ended]\n738\n&lt;NA&gt;\n738\n&lt;NA&gt;\n\n\nValid ended (n)\n4,239\n100.0%\n2,946\n100.0%\n\n\nTotal (n)\n5,072\n&lt;NA&gt;\n3,755\n&lt;NA&gt;\n\n\n\n\n\n\n\n\n\n\nFigure 4.2: End reason proportions for new people’s first routes\n\nCode\n\n\nCode\n# Generate Table A.4\npl_before_dpw_start = dffp.loc[(dffp[\"pl_start_dt\"] &lt; distinct_pathways.dpw_start_dt), \"o_cli_id\"].unique()\ndpw_clis_before_start = dpw_clis.loc[dpw_clis[\"o_cli_id\"].isin(pl_before_dpw_start), \"o_cli_id\"]\nta4_data = dpw_rt_ends[~dpw_rt_ends[\"o_cli_id\"].isin(dpw_clis_before_start)].groupby(\"o_cli_id\").head(1).copy()\nta4 = ta4_data[\"rt_end_cat\"].value_counts(dropna=False).rename(\"all_routes\").to_frame()\nta4_end_rows = [\"Missing data/error\", \"[Not ended]\"]\nta4 = ta4.reindex([x for x in ta4.index if x not in ta4_end_rows] + ta4_end_rows)\nprop_include = ta4.index.difference([\"Missing data/error\"])\nta4_prop = ta4.loc[prop_include].apply(lambda x: x/x.sum()).mul(100)\nprop_notended_include = ta4_prop.index.difference([\"[Not ended]\"])\nta4_prop_notended = ta4.loc[prop_notended_include].apply(lambda x: x/x.sum()).mul(100)\nta4 = pd.concat([ta4, ta4_prop_notended], keys=[\"n\", \"% of ended\"], axis=\"columns\").swaplevel(axis=\"columns\")\nta4 = ta4.droplevel(0, axis=1)\nta4_numeric = ta4.copy()\nta4_numeric = ta4_numeric.reindex(end_props_order).dropna(how=\"all\")\nta4_total = ta4.sum()\nta4_total.loc[\"% of ended\"] = pd.NA\nta4.loc[\"Valid ended (n)\"] = ta4.loc[prop_notended_include].sum()\nta4.loc[\"Total (n)\"] = ta4_total\nta4.loc[:, \"n\"] = ta4.loc[:, \"n\"].map(lambda x: f\"{x:,.0f}\" if not pd.isna(x) else pd.NA)\n# ta4.loc[:, \"% of all\"] = ta4.loc[:, \"% of all\"].map(lambda x: f\"{x:.1f}%\" if not pd.isna(x) else pd.NA)\nta4.loc[:, \"% of ended\"] = ta4.loc[:, \"% of ended\"].map(lambda x: f\"{x:.1f}%\" if not pd.isna(x) else pd.NA)\nta4 = ta4.reindex(end_props_order).dropna(how=\"all\")\nexcel_output[\"Table A.4: Route end reasons: first routes for people with no records of previously using BAHSA services\"] = ta4\n\n# Generate Figure 4.2\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\nrelevant_numeric = ta4_numeric\nmissing = [\"Missing data/error\", \"[Not ended]\"]\ntotal_n = relevant_numeric[\"n\"].sum()\ntotal_na = relevant_numeric.loc[missing][\"n\"].sum()\nvalid_n = (total_n - total_na).astype(\"int64\")\nplotdata = (\n    relevant_numeric.drop(index=missing)[\"% of ended\"].rename(\"%\").to_frame().reset_index()\n)\nplotlabels = pd.DataFrame({\"x\": -9, \"y\": plotdata[\"rt_end_cat\"], \"text\": plotdata[\"%\"].map(lambda x: f\"{x:.1f}%\")})\ncolors = sns.color_palette()\ncolors = colors[6:7]\n\n\ndef cust_plot(rc_params={}):\n    return (\n        so.Plot(data=plotdata, y=\"rt_end_cat\", x=\"%\", color=1)\n        .theme({\"ytick.major.pad\": 35, \"font.size\": 12, \"axes.titlesize\": \"medium\"} | rc_params)\n        .add(so.Bar(edgewidth=0), legend=False)\n        .add(so.Text({\"clip_on\": False}, halign=\"left\", fontsize=11), data=plotlabels, x=\"x\", y=\"y\", text=\"text\", legend=False)\n        .label(y=None, x=None, title=f\"Per person (first route)\\n$n$={valid_n:,}\")\n        .scale(x=so.Continuous().label(like=\"{x:.0f}%\"), color=colors)\n        .limit(x=(0, 35))\n        .layout(size=(4.2, 3.7), engine=\"constrained\")\n    )\n\n\nfig_4_2 = \"Figure_4.2.svg\"\nfig_filename = fig_4_2\ncust_plot().save(asset_themes[\"opaque\"] + fig_filename)\nlighttheme = {\"axes.edgecolor\": (1,1,1,0),\n              \"grid.alpha\": 0.5,\n              \"grid.color\": \".15\"}\ncust_plot(lighttheme).save(asset_themes[\"transparent_lightbg\"] + fig_filename, transparent=True)\ndarktheme = {\"text.color\": \"white\",\n             \"axes.edgecolor\": (1,1,1,0), \"axes.labelcolor\": \"white\",\n             \"xtick.color\": \"white\", \"xtick.labelcolor\": \"white\",\n             \"ytick.color\": \"white\", \"ytick.labelcolor\": \"white\",\n             \"grid.alpha\": 0.5}\ncust_plot(darktheme).save(asset_themes[\"transparent_darkbg\"] + fig_filename, transparent=True)\npass\n\n\n\n\nFigure 4.2\n\nNotes: Full data table is Table A.4. People with no records of previously using BAHSA services.\n\n\nTable A.4\n\n\n\n\n\n\n\n\n\nn\n% of ended\n\n\nrt_end_cat\n\n\n\n\n\n\nTo social housing\n469\n26.0%\n\n\nEvicted\n327\n18.1%\n\n\nAbandoned\n230\n12.7%\n\n\nTo friends/family\n195\n10.8%\n\n\nCustody\n98\n5.4%\n\n\nOther\n154\n8.5%\n\n\nTo private rented\n166\n9.2%\n\n\nTo care/hospital\n61\n3.4%\n\n\nTo external supported\n69\n3.8%\n\n\nDied\n38\n2.1%\n\n\nMissing data/error\n57\n&lt;NA&gt;\n\n\n[Not ended]\n451\n&lt;NA&gt;\n\n\nValid ended (n)\n1,807\n100.0%\n\n\nTotal (n)\n2,315\n&lt;NA&gt;"
  },
  {
    "objectID": "4.1_outcomes.html#tables-in-an-excel-file",
    "href": "4.1_outcomes.html#tables-in-an-excel-file",
    "title": "4.1 Results - Outcomes",
    "section": "Tables in an Excel file",
    "text": "Tables in an Excel file\n\nCode\nimport os\nsheetname = \"Sheet1\"\nqmdfile = os.getenv(\"QUARTO_DOCUMENT_FILE\")\nfilename = \"img_output/\"+qmdfile[:-4]+\".xlsx\"\nwith pd.ExcelWriter(filename) as writer:\n    next_row = 0\n    for k, v in excel_output.items():\n        pd.DataFrame([k]).to_excel(writer, sheet_name=sheetname, header=False, index=False, startrow=next_row)\n        next_row += 1\n        v.to_excel(writer, sheet_name=sheetname, startrow=next_row, float_format=\"%.1f\")\n        next_row += (\n            (1 if not isinstance(v.columns, pd.MultiIndex) else 1 + len(v.columns.levels))\n            + len(v) + 1)\nprint(f\"[Excel file](./{filename})\")\n\nExcel file"
  },
  {
    "objectID": "4.5_returns.html",
    "href": "4.5_returns.html",
    "title": "4.5 Results - Returns",
    "section": "",
    "text": "Code\n# Setup code\nimport pandas as pd\nfrom python_scripts import setup\nfrom python_scripts import services\nfrom python_scripts import helper\ndf, dffp = setup.setup(verbose=False)\n\nfrom python_scripts import distinct_pathways\ndpw_pls = distinct_pathways.get_distinct_pathways_routes(dffp)\nqrows = dpw_pls[\"rt_end_cat\"] == \"[Not ended or invalid end reason]\"\ndpw_pls.loc[qrows, \"rt_end_cat\"] = dpw_pls.loc[qrows, \"pl_end_dt\"].isna().map({True: \"[Not ended]\", False: \"Missing data/error\"})\ndpw_pls[\"rt_end_cat\"] = dpw_pls[\"rt_end_cat\"].str.replace(\"To care/hosp.\", \"To care/hospital\")\ndpw_rt_starts = dpw_pls.groupby(\"route_id\").head(1)\ndpw_rt_ends = dpw_pls.groupby(\"route_id\").tail(1)\ndpw_clis = dpw_pls.groupby([\"o_cli_id\"]).head(1)  # 1 duplicate client but first entry has more detailed answers\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nmatplotlib.rcParams.update({\n    'figure.figsize': (6.2, 4.1),\n    'font.size': 12,\n    'figure.constrained_layout.use': True\n})\n\nasset_themes = {\"opaque\": \"assets_output/\",\n                \"transparent_lightbg\": \"assets_transparent_lightbg/\",\n                \"transparent_darkbg\": \"assets_transparent_darkbg/\"}\nassets_output = asset_themes[\"opaque\"]\nexcel_output = {}"
  },
  {
    "objectID": "4.5_returns.html#returns-rq5",
    "href": "4.5_returns.html#returns-rq5",
    "title": "4.5 Results - Returns",
    "section": "Returns (RQ5)",
    "text": "Returns (RQ5)\n\nTable 4.3: Returns at periods of time after routes ended (per route)\n\n\nCode\nstable_periods = {x: f\"after_rt_ret_within_{x}\" for x in distinct_pathways.stable_offsets.keys()}\nstable_period_max_end_dts = {x: distinct_pathways.dpw_end_dt - distinct_pathways.stable_offsets[x]\n                             for x in distinct_pathways.stable_offsets.keys()}\nroute_end_grp = (dpw_rt_ends[dpw_rt_ends[\"pl_end_dt\"].notnull() & (dpw_rt_ends[\"rt_end_cat\"] != \"Died\")]\n                 .assign(All=\"All\").groupby(\"All\"))\nroute_outcomes = route_end_grp.size().sort_values(ascending=False).rename((\"outcomes\", \"n\")).to_frame()\nroute_outcomes[(\"outcomes\", \"%\")] = (100*route_outcomes[(\"outcomes\", \"n\")]\n                                     / route_outcomes[(\"outcomes\", \"n\")].sum())\nfor k, v in stable_periods.items():\n    route_outcomes[k, f\"&lt;t\"] = (route_end_grp[\"pl_end_dt\"]\n                                .apply(lambda x: (x &gt;= stable_period_max_end_dts[k]).sum()).astype(\"Int64\"))\n    route_outcomes[k, f\"&gt;=t\"] = (route_end_grp[\"pl_end_dt\"]\n                                 .apply(lambda x: (x &lt; stable_period_max_end_dts[k]).sum()).astype(\"Int64\"))\n    route_outcomes[k, f\"&lt;t ret\"] = (route_end_grp\n                                    .apply(lambda x: ((x[\"pl_end_dt\"] &gt;= stable_period_max_end_dts[k])\n                                                      & (x[v] == \"Yes\")).sum()).astype(\"Int64\"))\n    # route_outcomes[k, f\"&lt;t\"] = route_end_grp[v].apply(lambda x: (x == \"Not yet...\").sum()).astype(\"Int64\")\n    for outcome in [\"Yes\", \"No\"]:\n        route_outcomes[k, outcome] = (route_end_grp\n                                      .apply(lambda x: ((x[\"pl_end_dt\"] &lt; stable_period_max_end_dts[k])\n                                                        & (x[v] == outcome)).sum()).astype(\"Int64\"))\n    route_outcomes[k, \"Y+N\"] = (route_outcomes[k, \"Yes\"]\n                                + route_outcomes[k, \"No\"])\n    route_outcomes[k, \"% Y/(Y+N)\"] = (route_outcomes[k, \"Yes\"]\n                                      .mul(100).div(route_outcomes[k, \"Yes\"]\n                                                    + route_outcomes[k, \"No\"]))\n    route_outcomes[k, \"% No/sum(n)\"] = (route_outcomes[k, \"No\"]\n                                        .mul(100).div(route_outcomes[(\"outcomes\", \"n\")].sum()))\ntbl_periods = route_outcomes.astype(\"object\").iloc[:, 2:].stack()\nt4_3 = route_outcomes.astype(\"object\").iloc[:, 2:].stack().loc[\"All\"].loc[[\"&gt;=t\", \"Yes\", \"% Y/(Y+N)\"]]\nt4_3.loc[\"% N/(Y+N)\"] = 100 - t4_3.loc[\"% Y/(Y+N)\"]\nt4_3 = t4_3.rename(index={\"&gt;=t\": \"Eligible routes\",\n                          \"Yes\": \"Subsequent returns\",\n                          \"% Y/(Y+N)\": \"% returned\",\n                          \"% N/(Y+N)\": \"% not returned\"})\nwith pd.option_context(\"display.float_format\", \"{:,.1f}\".format):\n    display(t4_3)\nexcel_output[\"Table 4.3: Returns at periods of time after routes ended (per route)\"] = t4_3\n\n\n\n\n\n\n\n\n\n90d\n6m\n15m\n18m\n2y\n5y\n\n\n\n\nEligible routes\n4077\n3959\n3632\n3527\n3275\n1617\n\n\nSubsequent returns\n354\n607\n914\n961\n970\n625\n\n\n% returned\n8.7\n15.3\n25.2\n27.2\n29.6\n38.7\n\n\n% not returned\n91.3\n84.7\n74.8\n72.8\n70.4\n61.3\n\n\n\n\n\n\n\nNotes: 1. Return = new route that started before 30/04/2025.\n\n\nTable 4.4: Returns at periods of time after people’s first routes ended (per person)\n\n\nCode\nstable_periods = {x: f\"after_rt_ret_within_{x}\" for x in distinct_pathways.stable_offsets.keys()}\nstable_period_max_end_dts = {x: distinct_pathways.dpw_end_dt - distinct_pathways.stable_offsets[x]\n                             for x in distinct_pathways.stable_offsets.keys()}\npl_before_dpw_start = dffp.loc[(dffp[\"pl_start_dt\"] &lt; distinct_pathways.dpw_start_dt), \"o_cli_id\"].unique()\ndpw_clis_before_start = dpw_clis.loc[dpw_clis[\"o_cli_id\"].isin(pl_before_dpw_start), \"o_cli_id\"]\nfirst_dpw_routes = dpw_pls.groupby(\"o_cli_id\").head(1)[\"route_id\"]\nfirst_route_end_pls = (dpw_pls[dpw_pls[\"route_id\"].isin(first_dpw_routes)]\n                       .groupby(\"route_id\").tail(1)[lambda x: x[\"pl_end_dt\"].notnull() & (x[\"rt_end_cat\"] != \"Died\")]\n                       .assign(Group=lambda x: x[\"o_cli_id\"].isin(dpw_clis_before_start).map({True: \"Prev\", False: \"New\"})))\nfirst_route_end_grp = pd.concat([first_route_end_pls, first_route_end_pls.assign(Group=\"All\")]).groupby(\"Group\")\nfirst_route_outcomes = first_route_end_grp.size().sort_values(ascending=False).rename((\"outcomes\", \"n\")).to_frame()\nfirst_route_outcomes[(\"outcomes\", \"%\")] = (100*first_route_outcomes[(\"outcomes\", \"n\")]\n                                           / first_route_outcomes.loc[\"All\", (\"outcomes\", \"n\")])\nfor k, v in stable_periods.items():\n    first_route_outcomes[k, f\"&lt;t\"] = (first_route_end_grp[\"pl_end_dt\"]\n                                      .apply(lambda x: (x &gt;= stable_period_max_end_dts[k]).sum()).astype(\"Int64\"))\n    first_route_outcomes[k, f\"&gt;=t\"] = (first_route_end_grp[\"pl_end_dt\"]\n                                       .apply(lambda x: (x &lt; stable_period_max_end_dts[k]).sum()).astype(\"Int64\"))\n    first_route_outcomes[k, f\"&lt;t ret\"] = (first_route_end_grp\n                                          .apply(lambda x: ((x[\"pl_end_dt\"] &gt;= stable_period_max_end_dts[k])\n                                                            & (x[v] == \"Yes\")).sum()).astype(\"Int64\"))\n    # first_route_outcomes[k, f\"&lt;t\"] = first_route_end_grp[v].apply(lambda x: (x == \"Not yet...\").sum()).astype(\"Int64\")\n    for outcome in [\"Yes\", \"No\"]:\n        first_route_outcomes[k, outcome] = (first_route_end_grp\n                                            .apply(lambda x: ((x[\"pl_end_dt\"] &lt; stable_period_max_end_dts[k])\n                                                              & (x[v] == outcome)).sum()).astype(\"Int64\"))\n    first_route_outcomes[k, \"Y+N\"] = (first_route_outcomes[k, \"Yes\"]\n                                      + first_route_outcomes[k, \"No\"])\n    first_route_outcomes[k, \"% Y/(Y+N)\"] = (first_route_outcomes[k, \"Yes\"]\n                                            .mul(100).div(first_route_outcomes[k, \"Yes\"]\n                                                          + first_route_outcomes[k, \"No\"]))\n    first_route_outcomes[k, \"% No/sum(n)\"] = (first_route_outcomes[k, \"No\"]\n                                              .mul(100).div(first_route_outcomes[(\"outcomes\", \"n\")].sum()))\n\n\ntbl_n = first_route_outcomes.iloc[:, :2].sort_index(ascending=False)\ntbl_periods = first_route_outcomes.astype(\"object\").iloc[:, 2:].stack().sort_index(level=0, ascending=False, sort_remaining=False)\nt4_4 = tbl_periods.loc[\"All\"].loc[[\"&gt;=t\", \"Yes\", \"% Y/(Y+N)\"]]\nt4_4.loc[\"% N/(Y+N)\"] = 100 - t4_4.loc[\"% Y/(Y+N)\"]\nt4_4 = t4_4.rename(index={\"&gt;=t\": \"Eligible first routes\",\n                          \"Yes\": \"Subsequent returns\",\n                          \"% Y/(Y+N)\": \"% returned\",\n                          \"% N/(Y+N)\": \"% not returned\"})\nwith pd.option_context(\"display.float_format\", \"{:,.1f}\".format):\n    display(t4_4)\n\n\n\n\n\n\n\n\n\n90d\n6m\n15m\n18m\n2y\n5y\n\n\n\n\nEligible first routes\n3039\n2957\n2738\n2661\n2508\n1387\n\n\nSubsequent returns\n216\n359\n562\n586\n615\n468\n\n\n% returned\n7.1\n12.1\n20.5\n22.0\n24.5\n33.7\n\n\n% not returned\n92.9\n87.9\n79.5\n78.0\n75.5\n66.3\n\n\n\n\n\n\n\n\n\nFigure 4.9: Returns at periods of time after exits\n\n\nCode\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\nperiodMult = {\"d\": 1, \"m\": 365.25/12, \"y\": 365.25}\nperiodYears = {x: int(x[:-1]) * periodMult[x[-1]] / periodMult[\"y\"] for x in distinct_pathways.stable_offsets.keys()}\nplotdata = (\n    pd.concat([\n        t4_3.loc[\"% returned\"].to_frame()\n        .assign(years=lambda x: x.index.map(periodYears), grp=\"Per route (all exits)\")\n        .reset_index(),\n        t4_4.loc[\"% returned\"].to_frame()\n        .assign(years=lambda x: x.index.map(periodYears), grp=\"Per person (first exits)\")\n        .reset_index()\n    ])\n)\ngrps = plotdata[\"grp\"].unique()\nplotdata = (\n    pd.concat([pd.DataFrame({\"index\": \"\", \"% returned\": 0, \"years\": 0, \"grp\": grps}, index=[\"\"]*len(grps)), plotdata])\n    .reset_index(drop=True))\n\n\ndef cust_plot(rc_params={}, annot_color=\"black\"):\n    return (\n        so.Plot(data=plotdata, y=\"% returned\", x=\"years\", text=\"index\")\n        .theme({\"axes.grid.which\": \"both\"} | rc_params)\n        .facet(col=\"grp\")\n        .label(x=\"years since exit\")\n        .add(so.Dot(), data=plotdata[plotdata[\"index\"] != \"\"])\n        .add(so.Line())\n        .add(so.Area(edgewidth=0))\n        .add(so.Text(halign=\"left\", valign=\"top\", color=annot_color))\n        .limit(x=(0, 5.5), y=(0, 40))\n        .scale(x=so.Continuous().tick(minor=1))\n        .layout(size=(6.2, 3.6), engine=\"constrained\")\n    )\n\n\nfig_4_9 = \"Figure_4.9.svg\"\nfig_filename = fig_4_9\ncust_plot().save(asset_themes[\"opaque\"] + fig_filename)\nlighttheme = {\"axes.edgecolor\": (1,1,1,0),\n              \"grid.alpha\": 0.5,\n              \"grid.color\": \".15\"}\ncust_plot(lighttheme).save(asset_themes[\"transparent_lightbg\"] + fig_filename, transparent=True)\ndarktheme = {\"text.color\": \"white\",\n             \"axes.edgecolor\": (1,1,1,0), \"axes.labelcolor\": \"white\",\n             \"xtick.color\": \"white\", \"xtick.labelcolor\": \"white\",\n             \"ytick.color\": \"white\", \"ytick.labelcolor\": \"white\",\n             \"grid.alpha\": 0.5}\ncust_plot(darktheme, annot_color=\"white\").save(asset_themes[\"transparent_darkbg\"] + fig_filename, transparent=True)\npass\n\n\n\nNote: Data tables are Table 4.3 and Table 4.4.\n\n\nTable A.8: Returns at periods of time after exit by end reason\n\n\nCode\n# Generate table\nstable_periods = {x: f\"after_rt_ret_within_{x}\" for x in distinct_pathways.stable_offsets.keys()}\nstable_period_max_end_dts = {x: distinct_pathways.dpw_end_dt - distinct_pathways.stable_offsets[x]\n                             for x in distinct_pathways.stable_offsets.keys()}\nroute_end_grp = (dpw_rt_ends[dpw_rt_ends[\"pl_end_dt\"].notnull() & (dpw_rt_ends[\"rt_end_cat\"] != \"Died\")]\n                 .assign(All=\"All\").groupby(\"rt_end_cat\"))\nroute_outcomes = route_end_grp.size().sort_values(ascending=False).rename((\"outcomes\", \"n\")).to_frame()\nroute_outcomes[(\"outcomes\", \"%\")] = (100*route_outcomes[(\"outcomes\", \"n\")]\n                                     / route_outcomes[(\"outcomes\", \"n\")].sum())\nfor k, v in stable_periods.items():\n    route_outcomes[k, f\"Eligible (n)\"] = (route_end_grp[\"pl_end_dt\"]\n                                 .apply(lambda x: (x &lt; stable_period_max_end_dts[k]).sum()).astype(\"Int64\"))\n    for outcome in [\"Yes\", \"No\"]:\n        route_outcomes[k, outcome] = (route_end_grp\n                                      .apply(lambda x: ((x[\"pl_end_dt\"] &lt; stable_period_max_end_dts[k])\n                                                        & (x[v] == outcome)).sum()).astype(\"Int64\"))\n    route_outcomes[k, \"% returned\"] = (route_outcomes[k, \"Yes\"]\n                                       .mul(100).div(route_outcomes[k, \"Yes\"]\n                                                     + route_outcomes[k, \"No\"]))\n    route_outcomes = route_outcomes.drop(columns=(k, \"No\"))\n    route_outcomes = route_outcomes.rename(columns={\"Yes\": \"Returned (n)\"}, level=1)\nna_cols = [\"[Not ended]\", \"Died\"]\nroute_outcomes = route_outcomes.sort_values((\"outcomes\", \"%\"), ascending=False)\nta8_n = route_outcomes.iloc[:, :2]\nroute_outcomes = route_outcomes.sort_values((\"2y\", \"% returned\"), ascending=True)\nta8_periods = (route_outcomes.astype(\"object\")\n               .loc[[x for x in route_outcomes.index.to_list() if x not in na_cols]]\n               .iloc[:, 2:].stack(sort=False))\nta8 = ta8_periods\nta8.index = ta8.index.set_names((\"End reason\", \"Statistic\"))\nwith pd.option_context(\"display.float_format\", \"{:,.1f}\".format):\n    display(ta8)\n\nexcel_output[\"Table A.8: Returns at periods of time after exit by end reason\"] = ta8\n\n\n\n\n\n\n\n\n\n\n90d\n6m\n15m\n18m\n2y\n5y\n\n\nEnd reason\nStatistic\n\n\n\n\n\n\n\n\n\n\nTo social housing\nEligible (n)\n979\n954\n885\n870\n822\n418\n\n\nReturned (n)\n1\n1\n4\n6\n8\n10\n\n\n% returned\n0.1\n0.1\n0.5\n0.7\n1.0\n2.4\n\n\nTo private rented\nEligible (n)\n273\n267\n239\n234\n224\n103\n\n\nReturned (n)\n1\n5\n9\n10\n16\n12\n\n\n% returned\n0.4\n1.9\n3.8\n4.3\n7.1\n11.7\n\n\nTo friends/family\nEligible (n)\n364\n358\n345\n339\n322\n177\n\n\nReturned (n)\n12\n26\n59\n68\n77\n60\n\n\n% returned\n3.3\n7.3\n17.1\n20.1\n23.9\n33.9\n\n\nMissing data/error\nEligible (n)\n74\n74\n70\n68\n62\n42\n\n\nReturned (n)\n11\n17\n17\n17\n15\n15\n\n\n% returned\n14.9\n23.0\n24.3\n25.0\n24.2\n35.7\n\n\nTo external supported\nEligible (n)\n161\n155\n142\n138\n130\n66\n\n\nReturned (n)\n13\n27\n36\n36\n39\n26\n\n\n% returned\n8.1\n17.4\n25.4\n26.1\n30.0\n39.4\n\n\nOther\nEligible (n)\n275\n266\n232\n227\n213\n96\n\n\nReturned (n)\n34\n48\n61\n61\n71\n42\n\n\n% returned\n12.4\n18.0\n26.3\n26.9\n33.3\n43.8\n\n\nTo care/hospital\nEligible (n)\n163\n154\n139\n138\n127\n65\n\n\nReturned (n)\n27\n42\n51\n53\n50\n31\n\n\n% returned\n16.6\n27.3\n36.7\n38.4\n39.4\n47.7\n\n\nAbandoned\nEligible (n)\n626\n609\n553\n528\n477\n234\n\n\nReturned (n)\n59\n126\n218\n230\n223\n152\n\n\n% returned\n9.4\n20.7\n39.4\n43.6\n46.8\n65.0\n\n\nEvicted\nEligible (n)\n835\n801\n726\n688\n621\n292\n\n\nReturned (n)\n164\n246\n328\n334\n323\n191\n\n\n% returned\n19.6\n30.7\n45.2\n48.5\n52.0\n65.4\n\n\nCustody\nEligible (n)\n327\n321\n301\n297\n277\n124\n\n\nReturned (n)\n32\n69\n131\n146\n148\n86\n\n\n% returned\n9.8\n21.5\n43.5\n49.2\n53.4\n69.4\n\n\n\n\n\n\n\n\n\nFigure 4.10: Return proportions at periods of time after eligible exits by end reason\n\n\nCode\n# Plot chart\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\nperiodMult = {\"d\": 1, \"m\": 365.25/12, \"y\": 365.25}\nperiodYears = {x: int(x[:-1]) * periodMult[x[-1]] / periodMult[\"y\"] for x in distinct_pathways.stable_offsets.keys()}\nplotdata = (\n    ta8.unstack(0).loc[\"% returned\"].reset_index(1)\n    .assign(years=lambda x: x.index.map(periodYears))\n    .reset_index()\n)\nzeroes = pd.DataFrame({\"index\": \"\", \"End reason\": plotdata[\"End reason\"].unique(), \"% returned\": 0, \"years\": 0},\n                      index=range(len(plotdata), len(plotdata) + plotdata[\"End reason\"].nunique()))\nplotdata = pd.concat([zeroes, plotdata])\nfacet_order = ta8_n.index.to_list()\n\n\ndef cust_plot(rc_params={}, annot_color=\"black\"):\n    return (\n        so.Plot(data=plotdata, y=\"% returned\", x=\"years\", text=\"index\")\n        .facet(\"End reason\", wrap=4, order=facet_order)\n        .label(x=\"\", y=\"\")\n        .add(so.Dot(), data=plotdata[plotdata[\"index\"] != \"\"])\n        .add(so.Line())\n        .add(so.Area(edgewidth=0))\n        .limit(x=(0, 5), y=(0, None))\n        .scale(x=(so.Continuous()\n                  .tick(every=1, minor=0)\n                  .label(like=lambda x, pos: f\"{x:.0f}\" if x != 5 else \"5y\")),\n              y=so.Continuous().tick(minor=1).label(unit=(\"\", \"%\")))\n        .theme({\"axes.grid.which\": \"both\"} | rc_params)\n        .layout(size=(7.2, 4.1), engine=\"constrained\")\n    )\n\n\nfig_4_10 = \"Figure_4.10.svg\"\nfig_filename = fig_4_10\ncust_plot().save(asset_themes[\"opaque\"] + fig_filename)\nlighttheme = {\"axes.edgecolor\": (1,1,1,0),\n              \"grid.alpha\": 0.5,\n              \"grid.color\": \".15\"}\ncust_plot(lighttheme).save(asset_themes[\"transparent_lightbg\"] + fig_filename, transparent=True)\ndarktheme = {\"text.color\": \"white\",\n             \"axes.edgecolor\": (1,1,1,0), \"axes.labelcolor\": \"white\",\n             \"xtick.color\": \"white\", \"xtick.labelcolor\": \"white\",\n             \"ytick.color\": \"white\", \"ytick.labelcolor\": \"white\",\n             \"grid.alpha\": 0.5}\ncust_plot(darktheme, annot_color=\"white\").save(asset_themes[\"transparent_darkbg\"] + fig_filename, transparent=True)\npass\n\n\n\nNote: Data table is Table A.8. Figure B.1 shows return and eligible exit counts per end reason.\n\n\nFigure B.1: Number of returns at periods of time after eligible exits by end reason\n\n\nCode\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\nsns.set_theme()\nperiodMult = {\"d\": 1, \"m\": 365.25/12, \"y\": 365.25}\nperiodYears = {x: int(x[:-1]) * periodMult[x[-1]] / periodMult[\"y\"] for x in distinct_pathways.stable_offsets.keys()}\nplotdata = (\n    ta8_periods.unstack(0).loc[[\"Eligible (n)\", \"Returned (n)\"]].T.reset_index(1)\n    .assign(Yes=lambda x: x[\"Returned (n)\"], No=lambda x: x[\"Eligible (n)\"])[[\"End reason\", \"Yes\", \"No\"]]\n    .assign(years=lambda x: x.index.map(periodYears))\n)\nzeroes = pd.DataFrame({\"End reason\": ta8_n.index, \"Yes\": 0, \"No\": ta8_n[(\"outcomes\", \"n\")].values, \"years\": 0},\n                      index=[\"\"] * len(ta8_n))\nplotdata = pd.concat([zeroes, plotdata])\nfacet_order = ta8_n.index.to_list()\nplotdata = (plotdata.reset_index().set_index([\"index\", \"years\", \"End reason\"]).stack().reset_index()\n            .rename(columns={\"level_3\": \"Returned\", 0: \"n\"}))\ndef cust_plot(rc_params={}, annot_color=\"black\"):\n    fig = plt.figure(figsize=[7.2, 4.1])\n    p = (\n        so.Plot(data=plotdata, y=\"n\", x=\"years\", color=\"Returned\", text=\"index\")\n        .facet(\"End reason\", wrap=4, order=facet_order)\n        .label(x=\"\", y=\"\")\n        .add(so.Dot(), so.Stack(), legend=False)\n        .add(so.Line(), so.Stack(), legend=False)\n        .add(so.Area(edgewidth=0), so.Stack(), legend=False)\n        .limit(x=(0, 5), y=(0, None))\n        .scale(x=(so.Continuous()\n                  .tick(every=1, minor=0)\n                  .label(like=lambda x, pos: f\"{x:.0f}\" if x != 5 else \"5y\")),\n              y=so.Continuous().tick(minor=3))\n        # .scale(x=so.Continuous().tick(every=5, minor=4).label(unit=(\"\", \"y\")), y=so.Continuous().tick(minor=3))\n        .theme({\"axes.grid.which\": \"both\"} | rc_params)\n        .layout(size=(7.2, 4.1), engine=\"constrained\")\n        .on(fig)\n    )\n    with plt.rc_context(rc_params):\n        colours = sns.color_palette(as_cmap=False)\n        Y = matplotlib.patches.Patch(facecolor=colours[0], edgecolor=\"white\", alpha=0.8, label=\"$\\mathregular{n_{returned}}$\")\n        N = matplotlib.patches.Patch(facecolor=colours[1], edgecolor=\"white\", alpha=0.8, label=\"$\\mathregular{n_{eligible}}$\")\n        fig.legend(loc=\"lower right\", handles=[N, Y])\n    return p\n\n\nfig_b_1 = \"Figure_B.1.svg\"\nfig_filename = fig_b_1\ncust_plot().save(asset_themes[\"opaque\"] + fig_filename)\nlighttheme = {\"axes.edgecolor\": (1,1,1,0),\n              \"grid.alpha\": 0.5,\n              \"grid.color\": \".15\"}\ncust_plot(lighttheme).save(asset_themes[\"transparent_lightbg\"] + fig_filename, transparent=True)\ndarktheme = {\"text.color\": \"white\",\n             \"axes.edgecolor\": (1,1,1,0), \"axes.labelcolor\": \"white\",\n             \"xtick.color\": \"white\", \"xtick.labelcolor\": \"white\",\n             \"ytick.color\": \"white\", \"ytick.labelcolor\": \"white\",\n             \"grid.alpha\": 0.5,\n             \"legend.framealpha\": 1, \"legend.facecolor\": \".15\"\n             }\ncust_plot(darktheme, annot_color=\"white\").save(asset_themes[\"transparent_darkbg\"] + fig_filename, transparent=True)\npass\n\n\n\nFigure B.1 shows eligible exits and returns, highlighting the effect of lower numbers of eligible exits over longer time periods. Figure 4.9 is derived from this set of charts by dividing the number of returns by the number of eligible exits, i.e. normalising by neligible, to show the proportions of returns at different times after exits for different end reasons.\n\n\nFigure B.2: Number of returns (cumulative) by time since exits for preceding end reasons\n\n\nCode\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\nfrom python_scripts import routes\nhist_data = ((dpw_rt_starts.loc[dpw_rt_starts[\"gap\"].notna(), \"gap\"].dt.days.astype(\"float64\")).rename(\"Days\").to_frame()\n             .assign(Previous_End_Reason=dpw_rt_starts[\"prev_pl_end_reason\"].map(routes.get_end_cats_map())))\nplotdata = hist_data.groupby([\"Previous_End_Reason\", \"Days\"]).size().rename(\"Count\").groupby(\"Previous_End_Reason\").cumsum().to_frame()\nmax_counts = (plotdata.reset_index().groupby(\"Previous_End_Reason\")[\"Count\"].max()\n              .to_frame().assign(Days=plotdata.reset_index()[\"Days\"].max())\n              .set_index(\"Days\", append=True))\nplotdata = pd.concat([plotdata, max_counts]).reset_index().drop_duplicates().set_index([\"Previous_End_Reason\", \"Days\"])\nmarkers = []\nfor x_val in [x * 365.25 for x in [0.5, 1, 2, 3, 4]]:\n    markers = markers + [(hist_data.groupby(\"Previous_End_Reason\")[\"Days\"].agg(lambda x: (x &lt;= x_val).sum())\n                         .rename(\"Count\").to_frame().assign(Days=x_val).set_index(\"Days\", append=True))]\nmarkers = pd.concat(markers)\nmarkers = markers.assign(Pct=(100 * markers / max_counts.droplevel(\"Days\")).map(lambda x: f\"{x:.0f}%\"))\nzeroes = pd.DataFrame(plotdata.index.levels[0]).assign(Days=0, Count=0, Marker=False).set_index([\"Previous_End_Reason\", \"Days\"])\nplotdata = pd.concat([zeroes, plotdata.assign(Pct=\"\", Marker=False), markers.assign(Marker=True)])\nfacet_order=hist_data[\"Previous_End_Reason\"].value_counts().index.to_list()\nfigs = []\nfor y_max, y_tick, suffix, fig in [(None, 150, \"\", \"Figure_B.2_a\"), (100, 25, \" (zoomed)\", \"Figure_B.2_b\")]:\n    limits = {\"x\": (0, hist_data[\"Days\"].max()), \"y\": (None, y_max)}\n    markers_max = y_max if y_max is not None else markers[\"Count\"].max()\n    markers_data = plotdata[plotdata[\"Marker\"] & (plotdata[\"Count\"] &lt; markers_max)]\n    p = (\n        so.Plot(data=plotdata, x=\"Days\", y=\"Count\")\n        .label(x=\"\", y=\"\")\n        .add(so.Area(edgewidth=0))\n        .add(so.Line())\n        .scale(x=so.Continuous().tick(every=365.25).label(like=lambda x, y: f\"{x/365.25:.0f}\"),\n            y=so.Continuous().tick(every=y_tick))\n        .limit(x=limits[\"x\"], y=limits[\"y\"])\n        .layout(size=(7.2, 4.1), engine=\"constrained\")\n        .facet(col=\"Previous_End_Reason\", wrap=4, order=facet_order)\n    )\n    p.save(f\"img_output/{fig}.svg\")\n    figs.append(f\"img_output/{fig}.svg\")\nextrafigs = []\nfor y_max, y_tick, suffix, plotdata, markers in [(None,\n                                                  5,\n                                                  \" (social or private)\",\n                                                  plotdata.loc[[\"To private rented\", \"To social housing\"]].reset_index().set_index([\"Previous_End_Reason\", \"Days\"]),\n                                                  markers.loc[[\"To private rented\", \"To social housing\"]].reset_index().set_index([\"Previous_End_Reason\", \"Days\"]))]:\n    limits = {\"x\": (0, hist_data[\"Days\"].max()), \"y\": (None, y_max)}\n    markers_max = y_max if y_max is not None else markers[\"Count\"].max()\n    markers_data = plotdata[plotdata[\"Marker\"] & (plotdata[\"Count\"] &lt; markers_max)]\n    p = (\n        so.Plot(data=plotdata, x=\"Days\", y=\"Count\")\n        .label(x=\"Preceding gap in years\", y=\"Returns (cumulative count)\")\n        .add(so.Area(edgewidth=0))\n        .add(so.Line())\n        # .add(so.Dot(pointsize=4.5), data=markers_data)\n        # .add(so.Text({\"clip_on\": False}, fontsize=7, halign=\"left\", valign=\"top\", offset=2), data=markers_data, text=\"Pct\")\n        .scale(x=so.Continuous().tick(every=365.25).label(like=lambda x, y: f\"{x/365.25:.0f}\"),\n            y=so.Continuous().tick(every=y_tick))\n        .limit(x=limits[\"x\"], y=limits[\"y\"])\n        .layout(size=(7.2, 4.1), engine=\"constrained\")\n        .facet(col=\"Previous_End_Reason\")\n    )\n    p.save(\"img_output/Figure_B.3.svg\")\n    extrafigs.append(\"img_output/Figure_B.3.svg\")\n\n\n\nIncreased y-scale (zoomed-in)\n\n\n\nExtra figure: Number of returns (cumulative) by time since exits for social and private rented housing outcomes\nFurther increased y-scale (zoomed-in to maximum)"
  },
  {
    "objectID": "4.5_returns.html#tables-in-an-excel-file",
    "href": "4.5_returns.html#tables-in-an-excel-file",
    "title": "4.5 Results - Returns",
    "section": "Tables in an Excel file",
    "text": "Tables in an Excel file\n\nCode\nimport os\nsheetname = \"Sheet1\"\nqmdfile = os.getenv(\"QUARTO_DOCUMENT_FILE\")\nfilename = \"img_output/\"+qmdfile[:-4]+\".xlsx\"\nwith pd.ExcelWriter(filename) as writer:\n    next_row = 0\n    for k, v in excel_output.items():\n        pd.DataFrame([k]).to_excel(writer, sheet_name=sheetname, header=False, index=False, startrow=next_row)\n        next_row += 1\n        v.to_excel(writer, sheet_name=sheetname, startrow=next_row, float_format=\"%.1f\")\n        next_row += (\n            (1 if not isinstance(v.columns, pd.MultiIndex) else 1 + len(v.columns.levels))\n            + len(v) + 1)\nprint(f\"[Excel file](./{filename})\")\n\nExcel file"
  },
  {
    "objectID": "4.3_number_of_routes.html",
    "href": "4.3_number_of_routes.html",
    "title": "4.3 Results - Number of routes",
    "section": "",
    "text": "Code\n# Setup code\nimport pandas as pd\nfrom python_scripts import setup\nfrom python_scripts import services\nfrom python_scripts import helper\ndf, dffp = setup.setup(verbose=False)\n\nfrom python_scripts import distinct_pathways\ndpw_pls = distinct_pathways.get_distinct_pathways_routes(dffp)\nqrows = dpw_pls[\"rt_end_cat\"] == \"[Not ended or invalid end reason]\"\ndpw_pls.loc[qrows, \"rt_end_cat\"] = dpw_pls.loc[qrows, \"pl_end_dt\"].isna().map({True: \"[Not ended]\", False: \"Missing data/error\"})\ndpw_pls[\"rt_end_cat\"] = dpw_pls[\"rt_end_cat\"].str.replace(\"To care/hosp.\", \"To care/hospital\")\ndpw_rt_starts = dpw_pls.groupby(\"route_id\").head(1)\ndpw_rt_ends = dpw_pls.groupby(\"route_id\").tail(1)\ndpw_clis = dpw_pls.groupby([\"o_cli_id\"]).head(1)  # 1 duplicate client but first entry has more detailed answers\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nmatplotlib.rcParams.update({\n    'figure.figsize': (6.2, 4.1),\n    'font.size': 12,\n    'figure.constrained_layout.use': True\n})\n\nasset_themes = {\"opaque\": \"assets_output/\",\n                \"transparent_lightbg\": \"assets_transparent_lightbg/\",\n                \"transparent_darkbg\": \"assets_transparent_darkbg/\"}\nassets_output = asset_themes[\"opaque\"]\nexcel_output = {}"
  },
  {
    "objectID": "4.3_number_of_routes.html#number-of-routes-rq3",
    "href": "4.3_number_of_routes.html#number-of-routes-rq3",
    "title": "4.3 Results - Number of routes",
    "section": "Number of routes (RQ3)",
    "text": "Number of routes (RQ3)\n\nFigure 4.6: Numbers of people who experienced different quantities of routes by whether they were ‘new’ or had returned after previously using services\n\nCode\n\n\nCode\n# Generate Table A.5\nknown_apw = dffp.loc[\n    dffp.svc_type.isin(services.adult_pathways_accom_svc_types)\n    & (dffp.pl_end_dt &lt; distinct_pathways.dpw_start_dt),\n    \"o_cli_id\",\n]\ndpw_pls[\"known_apw\"] = dpw_pls[\"o_cli_id\"].isin(known_apw).map({False: \"New\", True: \"Returned\"})\ndpw_pls[\"current\"] = dpw_pls[\"o_cli_id\"].isin(dpw_pls.loc[dpw_pls.pl_end_dt.isnull(), \"o_cli_id\"]).map({True: \"Current\", False: \"Ended\"})\nta5 = (\n    dpw_pls.groupby(['o_cli_id', 'known_apw', 'current']).route_id.nunique().rename(\"routes\")\n    .reset_index(level=[\"known_apw\", \"current\"]).value_counts()\n    .unstack([0, 1], fill_value=0).swaplevel(axis=1).sort_index()\n    .assign(All=lambda x: x.sum(axis=\"columns\"))\n)\nta5.index = ta5.index.astype(\"object\")\n# ta5.columns = pd.Categorical(ta5.columns, categories=[\"Returned\", \"New\", \"All\"], ordered=True)\n# ta5 = ta5.rename_axis(\"Records pre-DPW\", axis=\"columns\")\n# ta5 = ta5.sort_index(axis=\"columns\")\nta5.loc[\"n\", :] = ta5.sum()\nta5_pct_col = ta5.apply(lambda x: (x/x.loc[\"n\"]).mul(100))\ngrp_sum = ta5.T.groupby(\"current\").transform(\"sum\").T\ngrp_sum.iloc[:-1] = grp_sum.iloc[-1].values\nta5_pct_grp = 100 * ta5 / grp_sum\nta5_pct_all = ta5[[\"Ended\", \"Current\"]].apply(lambda x: (x/ta5[\"All\"].loc[\"n\"]).mul(100))\nta5_numeric = (\n    pd.concat([ta5_pct_col, ta5_pct_grp, ta5_pct_all, ta5], keys=[\"% col\", \"% grp\", \"% All\", \"n\"], axis=\"columns\")\n    .swaplevel(0, 2, axis=\"columns\").swaplevel(0, 1, axis=\"columns\")\n    .sort_index(axis=\"columns\", level=[0, 1], sort_remaining=False)\n    [[\"Ended\", \"Current\", \"All\"]]\n).copy()\nformats = {(a, b, c): \"{0:.1f}%\" if c[0] == \"%\" else \"{0:,.0f}\" for a, b, c in ta5_numeric.columns}\nta5 = ta5_numeric.apply(lambda x: x.apply(formats[x.name].format))\nexcel_output[\"Table A.5: Routes per person during studied period by current/ended and whether placed in BAHSA before studied period\"] = ta5\n\n# Generate Figure 4.6\nimport numpy as np\nimport seaborn as sns\nimport seaborn.objects as so\nimport matplotlib.pyplot as plt\nsns.set()\ncol_x = \"n\"\nplotdata = ta5_numeric.swaplevel(0, 2, axis=1).loc[:8, col_x][[\"New\", \"Returned\"]].stack([0, 1]).rename(col_x).reset_index()\nplotdata[\"known_apw\"] = plotdata[\"known_apw\"].astype(\"string\")\nmaxes = ta5_numeric.swaplevel(0, 2, axis=1)[\"n\"].loc[\"n\"].groupby(level=0).sum()\nwidth_ratios = maxes[[\"New\", \"Returned\"]]\nwidth_ratios = width_ratios - (width_ratios.sum()/52.7)\n\n\ndef cust_plot(output_dir, fig_filename, rc_params={}, legend_params={}, annot_color=\".4\", transparent=False):\n    with plt.rc_context():\n        colours = sns.color_palette(as_cmap=False)\n        plt.rcParams.update(rc_params)\n        fig_overall = plt.figure(layout=\"constrained\", figsize=(6.2, 3.1))\n        fig_l, fig_r = fig_overall.subfigures(1, 2, width_ratios=width_ratios)\n        for grp, fig, col in [(\"Returned\", fig_r, colours[1]),\n                              (\"New\",      fig_l, colours[0])]:\n            pl_d = plotdata[plotdata[\"known_apw\"] == grp].reset_index(drop=True)\n            pl_d[\"routes\"] = pl_d[\"routes\"].astype(\"category\")\n            textdata_after = (pl_d.groupby(\"routes\", observed=False)[col_x].sum().to_frame()\n                              .assign(text=lambda x: (x/x.sum()).mul(100).map(lambda y: f\"{y:,.1f}%\")))\n            textdata_after.iloc[-1, -1] += f\" (of {grp})\"\n            textdata_during = (pl_d.assign(pct_all=pl_d.groupby(\"routes\", observed=False)[col_x]\n                                          .transform(lambda x: (x/pl_d[col_x].sum()).mul(100))))\n            textdata_during = textdata_during[textdata_during[\"pct_all\"] &gt; 15]\n            textdata_during.loc[textdata_during[\"current\"] == \"Ended\", \"x\"] = (\n                textdata_during.loc[textdata_during[\"current\"] == \"Ended\", col_x] / 2)\n            textdata_during.loc[textdata_during[\"current\"] == \"Current\", \"x\"] = (\n                textdata_during.loc[textdata_during[\"current\"] == \"Current\"].join(\n                    textdata_during.loc[textdata_during[\"current\"] == \"Ended\"].set_index(\"routes\")[col_x],\n                    on=\"routes\", rsuffix=\"_end\")\n                .apply(lambda x: x[col_x+\"_end\"] + x[col_x]/2, axis=1))\n\n            textdata_during[\"text\"] = textdata_during[\"pct_all\"].map(lambda y: f\"{y:,.1f}%\")\n            if grp == \"New\":\n                textdata_during.iloc[0, -1] += f\" of {grp} ({100*textdata_during.iloc[0][col_x]/plotdata[col_x].sum():.1f}% of all)\"\n            p = (\n                so.Plot(data=pl_d, y=\"routes\", x=col_x)\n                .theme(rc_params)\n                .add(so.Bars(), so.Stack(), color=\"current\", legend=False)\n                .add(so.Text(color=annot_color, fontsize=11, halign=\"left\", offset=2, alpha=0.7), data=textdata_after, x=col_x, y=\"routes\", text=\"text\")\n                .label(x=\"People\", title=f\"{grp}\" + \" (${n}$=\" + f\"{int(maxes[grp]):,})\", y=\"\")\n                .limit(x=(0, maxes[grp]))\n                .layout(engine=\"constrained\")\n                .on(fig)\n                .plot()\n            )\n            ax = fig.axes[0]\n            if grp == \"New\":\n                colours = sns.color_palette(as_cmap=False)\n                T = matplotlib.patches.Patch(facecolor=colours[0], edgecolor=\"white\", alpha=0.8, label=\"Ended\")\n                F = matplotlib.patches.Patch(facecolor=colours[1], edgecolor=\"white\", alpha=0.8, label=\"Current\")\n                ax.legend(handles=[F, T], loc=\"lower right\")\n            minor_ticks = [((b-a)/2 + a) for a, b in zip(ax.get_yticks()[:-1], ax.get_yticks()[1:])]\n            # labels = ta5.swaplevel(axis=1)[\"% col\"][grp].iloc[:-1].map(lambda x: x[:-1])\n            ax.set_xticks([maxes[grp]], labels=[\"${n}$\"])\n            ax.set_xticks(range(0, int(maxes[grp]), 500), labels=list(range(0, int(maxes[grp]), 500))[:-1] + [\"..\"], minor=True)\n            ax.tick_params(\"x\", which=\"minor\", pad=3, bottom=True, width=0.5, length=5)\n            ax.tick_params(\"x\", which=\"major\", pad=3, bottom=True, width=0.5, length=5)\n            ax.set_yticks(ax.get_yticks(), labels=[])\n            ax.set_yticks(minor_ticks, minor=True)\n            ax.grid(which=\"minor\", axis=\"y\")\n            ax.grid(which=\"minor\", axis=\"x\")\n        yticks = fig_l.axes[0].get_yticks()\n        sec = fig_l.axes[0].secondary_yaxis(location=0)\n        sec.set_yticks(yticks, labels=[y + 1 for y in yticks])\n        sec.tick_params(\"y\", length=0, which=\"both\", pad=8)\n        sec.set_ylabel(\"Routes\", labelpad=6)\n        plt.savefig(output_dir + fig_filename, transparent=transparent)\n\n\nfig_4_6 = \"Figure_4.6.svg\"\nfig_filename = fig_4_6\ncust_plot(asset_themes[\"opaque\"], fig_filename)\nlighttheme = {\"axes.edgecolor\": (1,1,1,0),\n              \"grid.alpha\": 0.5,\n              \"grid.color\": \".15\",\n              \"xtick.color\": \".15\",\n              \"xtick.labelcolor\": \".15\",\n              \"ytick.color\": \".15\",\n              \"ytick.labelcolor\": \".15\"\n              }\ncust_plot(asset_themes[\"transparent_lightbg\"], fig_filename, lighttheme, transparent=True)\ndarktheme = {\"text.color\": \"white\",\n             \"axes.edgecolor\": (1,1,1,0), \"axes.labelcolor\": \"white\",\n             \"xtick.color\": \"white\", \"xtick.labelcolor\": \"white\",\n             \"ytick.color\": \"white\", \"ytick.labelcolor\": \"white\",\n             \"grid.alpha\": 0.5,\n             \"legend.framealpha\": 1, \"legend.facecolor\": \".15\"\n             }\ncust_plot(asset_themes[\"transparent_darkbg\"], fig_filename, darktheme, annot_color=\".85\", transparent=True)\npass\n\n\n\n\nFigure 4.6\n\nNotes: X-axis scales are proportionate to New/Returned counts. ‘New’ means no record of using services since April 2009. Full data table is Table A.5.\n\n\nTable A.5\n\n\n\n\n\n\n\n\ncurrent\nEnded\nCurrent\nAll\n\n\nknown_apw\nNew\nReturned\nNew\nReturned\n\n\n\n\n% col\n% grp\n% All\nn\n% col\n% grp\n% All\nn\n% col\n% grp\n% All\nn\n% col\n% grp\n% All\nn\n% col\n% grp\nn\n\n\nroutes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n83.4%\n56.0%\n45.0%\n1,690\n70.0%\n23.0%\n18.5%\n694\n79.3%\n63.3%\n12.4%\n467\n51.7%\n10.4%\n2.1%\n77\n78.0%\n78.0%\n2,928\n\n\n2\n12.4%\n8.3%\n6.7%\n251\n16.5%\n5.4%\n4.4%\n164\n13.8%\n11.0%\n2.2%\n81\n24.2%\n4.9%\n1.0%\n36\n14.2%\n14.2%\n532\n\n\n3\n3.0%\n2.0%\n1.6%\n60\n7.9%\n2.6%\n2.1%\n78\n3.1%\n2.4%\n0.5%\n18\n13.4%\n2.7%\n0.5%\n20\n4.7%\n4.7%\n176\n\n\n4\n0.8%\n0.6%\n0.5%\n17\n2.9%\n1.0%\n0.8%\n29\n2.4%\n1.9%\n0.4%\n14\n6.7%\n1.4%\n0.3%\n10\n1.9%\n1.9%\n70\n\n\n5\n0.3%\n0.2%\n0.2%\n6\n1.4%\n0.5%\n0.4%\n14\n1.0%\n0.8%\n0.2%\n6\n2.0%\n0.4%\n0.1%\n3\n0.8%\n0.8%\n29\n\n\n6\n0.1%\n0.1%\n0.1%\n2\n0.7%\n0.2%\n0.2%\n7\n0.5%\n0.4%\n0.1%\n3\n1.3%\n0.3%\n0.1%\n2\n0.4%\n0.4%\n14\n\n\n7\n0.0%\n0.0%\n0.0%\n0\n0.4%\n0.1%\n0.1%\n4\n0.0%\n0.0%\n0.0%\n0\n0.7%\n0.1%\n0.0%\n1\n0.1%\n0.1%\n5\n\n\n8\n0.0%\n0.0%\n0.0%\n0\n0.1%\n0.0%\n0.0%\n1\n0.0%\n0.0%\n0.0%\n0\n0.0%\n0.0%\n0.0%\n0\n0.0%\n0.0%\n1\n\n\nn\n100.0%\n67.2%\n54.0%\n2,026\n100.0%\n32.8%\n26.4%\n991\n100.0%\n79.8%\n15.7%\n589\n100.0%\n20.2%\n4.0%\n149\n100.0%\n100.0%\n3,755"
  },
  {
    "objectID": "4.3_number_of_routes.html#tables-in-an-excel-file",
    "href": "4.3_number_of_routes.html#tables-in-an-excel-file",
    "title": "4.3 Results - Number of routes",
    "section": "Tables in an Excel file",
    "text": "Tables in an Excel file\n\nCode\nimport os\nsheetname = \"Sheet1\"\nqmdfile = os.getenv(\"QUARTO_DOCUMENT_FILE\")\nfilename = \"img_output/\"+qmdfile[:-4]+\".xlsx\"\nwith pd.ExcelWriter(filename) as writer:\n    next_row = 0\n    for k, v in excel_output.items():\n        pd.DataFrame([k]).to_excel(writer, sheet_name=sheetname, header=False, index=False, startrow=next_row)\n        next_row += 1\n        v.to_excel(writer, sheet_name=sheetname, startrow=next_row, float_format=\"%.1f\")\n        next_row += (\n            (1 if not isinstance(v.columns, pd.MultiIndex) else 1 + len(v.columns.levels))\n            + len(v) + 1)\nprint(f\"[Excel file](./{filename})\")\n\nExcel file"
  },
  {
    "objectID": "Tables_A.9_A.10_A.11.html",
    "href": "Tables_A.9_A.10_A.11.html",
    "title": "A (Appendix) - Tables A.9, A.10, A.11",
    "section": "",
    "text": "Code\n# Setup code\nimport pandas as pd\nfrom python_scripts import setup\nfrom python_scripts import services\nfrom python_scripts import helper\ndf, dffp = setup.setup(verbose=False)\n\nfrom python_scripts import distinct_pathways\ndpw_pls = distinct_pathways.get_distinct_pathways_routes(dffp)\nqrows = dpw_pls[\"rt_end_cat\"] == \"[Not ended or invalid end reason]\"\ndpw_pls.loc[qrows, \"rt_end_cat\"] = dpw_pls.loc[qrows, \"pl_end_dt\"].isna().map({True: \"[Not ended]\", False: \"Missing data/error\"})\ndpw_pls[\"rt_end_cat\"] = dpw_pls[\"rt_end_cat\"].str.replace(\"To care/hosp.\", \"To care/hospital\")\ndpw_rt_starts = dpw_pls.groupby(\"route_id\").head(1)\ndpw_rt_ends = dpw_pls.groupby(\"route_id\").tail(1)\ndpw_clis = dpw_pls.groupby([\"o_cli_id\"]).head(1)  # 1 duplicate client but first entry has more detailed answers\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nmatplotlib.rcParams.update({\n    'figure.figsize': (6.2, 4.1),\n    'font.size': 12,\n    'figure.constrained_layout.use': True\n})\n\nexcel_output = {}"
  },
  {
    "objectID": "Tables_A.9_A.10_A.11.html#additional-tables",
    "href": "Tables_A.9_A.10_A.11.html#additional-tables",
    "title": "A (Appendix) - Tables A.9, A.10, A.11",
    "section": "Additional Tables",
    "text": "Additional Tables\n\nTable A.9: Cumulative returns at a sample of time periods by preceding exit reasons\n\n\nCode\nfrom python_scripts import routes\n# Generate Table A.9\nhist_data = ((dpw_rt_starts.loc[dpw_rt_starts[\"gap\"].notna(), \"gap\"].dt.days.astype(\"float64\")).rename(\"Days\").to_frame()\n             .assign(Previous_End_Reason=dpw_rt_starts[\"prev_pl_end_reason\"].map(routes.get_end_cats_map())))\nbefore_DPW_era=(dpw_rt_starts[\"prev_pl_end_dt\"] &lt; distinct_pathways.dpw_start_dt).value_counts().rename(\"Before_DPW_Era\").to_frame()\nta9_data = hist_data.groupby([\"Previous_End_Reason\", \"Days\"]).size().rename(\"Count\").groupby(\"Previous_End_Reason\").cumsum().to_frame()\nmax_counts = (ta9_data.reset_index().groupby(\"Previous_End_Reason\")[\"Count\"].max()\n              .to_frame().assign(Days=ta9_data.reset_index()[\"Days\"].max())\n              .set_index(\"Days\", append=True))\nta9_data = pd.concat([ta9_data, max_counts]).reset_index().drop_duplicates().set_index([\"Previous_End_Reason\", \"Days\"])\nta9 = []\nfor x_val in [x * 365.25 for x in [0.5, 1, 2, 3, 4, 5, 6]] + [ta9_data.reset_index()[\"Days\"].max()]:\n    ta9 = ta9 + [(hist_data.groupby(\"Previous_End_Reason\")[\"Days\"].agg(lambda x: (x &lt;= x_val).sum())\n                         .rename(\"Count\").to_frame().assign(Days=x_val).set_index(\"Days\", append=True))]\nta9 = pd.concat(ta9)\nta9 = ta9.assign(Pct=(100 * ta9 / max_counts.droplevel(\"Days\")).map(lambda x: f\"{x:.0f}%\"))\nta9 = ta9.reset_index()\nta9 = ta9.assign(Years=(ta9[\"Days\"]/365.25))\nta9 = ta9.set_index([\"Previous_End_Reason\", \"Years\"])[[\"Count\", \"Pct\"]].sort_index().stack().unstack([1, 2])\nta9 = ta9.sort_values((6, \"Count\"), ascending=False)\nta9 = ta9.rename(columns={\"Count\": \"n\", \"Pct\": \"%\"})\nwith pd.option_context(\"display.float_format\", \"{:,.1f}\".format):\n    display(ta9)\nexcel_output[\"Table A.9: Cumulative returns at a sample of time periods by preceding exit reasons\"] = ta9\n\n\n\n\n\n\n\n\nYears\n0.5\n1.0\n2.0\n3.0\n4.0\n5.0\n6.0\n6.6\n\n\n\nn\n%\nn\n%\nn\n%\nn\n%\nn\n%\nn\n%\nn\n%\nn\n%\n\n\nPrevious_End_Reason\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEvicted\n255\n56%\n340\n75%\n407\n90%\n436\n96%\n448\n99%\n451\n100%\n452\n100%\n452\n100%\n\n\nAbandoned\n126\n42%\n208\n69%\n263\n88%\n284\n95%\n290\n97%\n298\n99%\n300\n100%\n300\n100%\n\n\nCustody\n69\n37%\n116\n62%\n161\n86%\n178\n95%\n186\n99%\n187\n99%\n188\n100%\n188\n100%\n\n\nTo friends/family\n26\n27%\n48\n51%\n79\n83%\n88\n93%\n91\n96%\n94\n99%\n95\n100%\n95\n100%\n\n\nOther\n49\n58%\n62\n73%\n79\n93%\n82\n96%\n83\n98%\n85\n100%\n85\n100%\n85\n100%\n\n\nTo care/hospital\n45\n65%\n58\n84%\n64\n93%\n68\n99%\n68\n99%\n69\n100%\n69\n100%\n69\n100%\n\n\nTo external supported\n27\n50%\n38\n70%\n46\n85%\n50\n93%\n51\n94%\n51\n94%\n53\n98%\n54\n100%\n\n\nTo private rented\n5\n17%\n7\n24%\n16\n55%\n22\n76%\n27\n93%\n29\n100%\n29\n100%\n29\n100%\n\n\nMissing data/error\n17\n71%\n18\n75%\n21\n88%\n22\n92%\n23\n96%\n24\n100%\n24\n100%\n24\n100%\n\n\nTo social housing\n1\n5%\n6\n29%\n11\n52%\n15\n71%\n16\n76%\n20\n95%\n21\n100%\n21\n100%\n\n\n\n\n\n\n\n\n\nTable A.10: Descriptive statistics for durations before returning by preceding end reason\n\n\nCode\nta10_data = (dpw_rt_starts.loc[dpw_rt_starts[\"gap\"].notna(), [\"route_id\", \"o_cli_id\"]]\n            .assign(Days=dpw_rt_starts[\"gap\"].dt.days.astype(\"Int64\"),\n                    Years=dpw_rt_starts[\"gap\"].dt.days / 365.25,\n                    prev_rt_end_cat=dpw_rt_starts[\"prev_pl_end_reason\"].map(routes.get_end_cats_map())))\npercentiles = [0.1, 0.5, 0.9]\nta10 = ta10_data.groupby(\"prev_rt_end_cat\")[[\"Days\", \"Years\"]].describe(percentiles=percentiles).sort_values((\"Days\", \"mean\"))\nta10 = ta10.astype(\"object\")\nint_vals = {\"Days\": [\"count\", \"min\", \"max\"], \"Years\": [\"count\"]}\nfloat_cols = {k: ta10[k].columns.difference(v) for k, v in int_vals.items()}\nfloat_prec = {\"Days\": 1, \"Years\": 3}\nfor dt in [\"Days\", \"Years\"]:\n    fp = float_prec[dt]\n    ta10.loc[:, (dt, float_cols[dt])] = (ta10.loc[:, (dt, float_cols[dt])].map(lambda x: f\"{x:,.{fp}f}\"))\n    ta10.loc[:, (dt, int_vals[dt])] = (ta10.loc[:, (dt, int_vals[dt])].map(lambda x: f\"{x:,.0f}\"))\nta10 = (\n    ta10.T.reindex((a, b)\n                    for a in [\"Years\", \"Days\"]\n                    for b in [x for x in ta10[\"Days\"].columns if x != \"count\"] + [\"count\"]).T\n    .rename(index={\"50%\": \"median\"})\n)\nta10 = pd.concat([ta10[\"Years\"], ta10[\"Days\"]], keys=[\"Years\", \"Days\"])\nwith pd.option_context(\"display.float_format\", \"{:,.1f}\".format):\n    display(ta10)\nexcel_output[\"Table A.10: Descriptive statistics for durations before returning by preceding end reason\"] = ta10\n\n\n\n\n\n\n\n\n\n\nmean\nstd\nmin\n10%\n50%\n90%\nmax\ncount\n\n\n\nprev_rt_end_cat\n\n\n\n\n\n\n\n\n\n\n\n\nYears\nTo care/hospital\n0.600\n0.774\n0.025\n0.076\n0.315\n1.512\n4.424\n69\n\n\nEvicted\n0.733\n0.883\n0.003\n0.049\n0.409\n1.956\n5.711\n452\n\n\nOther\n0.771\n0.912\n0.005\n0.080\n0.337\n1.756\n4.821\n85\n\n\nMissing data/error\n0.831\n1.252\n0.047\n0.085\n0.282\n2.043\n4.986\n24\n\n\nAbandoned\n0.961\n1.023\n0.016\n0.152\n0.615\n2.162\n5.812\n300\n\n\nCustody\n1.034\n0.959\n0.049\n0.169\n0.682\n2.470\n5.336\n188\n\n\nTo external supported\n1.094\n1.396\n0.060\n0.165\n0.500\n2.450\n6.582\n54\n\n\nTo friends/family\n1.249\n1.090\n0.003\n0.208\n0.958\n2.763\n5.013\n95\n\n\nTo private rented\n2.044\n1.292\n0.151\n0.368\n1.766\n3.614\n4.775\n29\n\n\nTo social housing\n2.363\n1.688\n0.011\n0.621\n1.643\n4.561\n5.788\n21\n\n\nDays\nTo care/hospital\n219.3\n282.8\n9\n27.6\n115.0\n552.4\n1,616\n69\n\n\nEvicted\n267.9\n322.5\n1\n18.0\n149.5\n714.6\n2,086\n452\n\n\nOther\n281.6\n333.0\n2\n29.4\n123.0\n641.4\n1,761\n85\n\n\nMissing data/error\n303.5\n457.3\n17\n30.9\n103.0\n746.1\n1,821\n24\n\n\nAbandoned\n351.1\n373.8\n6\n55.7\n224.5\n789.8\n2,123\n300\n\n\nCustody\n377.7\n350.2\n18\n61.7\n249.0\n902.0\n1,949\n188\n\n\nTo external supported\n399.7\n509.8\n22\n60.2\n182.5\n894.8\n2,404\n54\n\n\nTo friends/family\n456.0\n398.0\n1\n76.0\n350.0\n1,009.2\n1,831\n95\n\n\nTo private rented\n746.6\n472.1\n55\n134.4\n645.0\n1,320.0\n1,744\n29\n\n\nTo social housing\n863.0\n616.5\n4\n227.0\n600.0\n1,666.0\n2,114\n21\n\n\n\n\n\n\n\n\n\nTable A.11: Descriptive statistics for backward moves within routes by end reason\n\n\nCode\nimport numpy as np\ndpw_moves = dpw_pls[dpw_pls[\"prev_svc_id\"].notnull() & (dpw_pls[\"prev_svc_id\"] != dpw_pls[\"svc_id\"])]\ndpw_moves[\"level_diff\"] = dpw_moves[[\"prev_pathway_level\", \"pathway_level\"]].astype(\"Int64\").diff(axis=1)[\"pathway_level\"]\ndpw_moves[\"level_dir\"] = dpw_moves[\"level_diff\"].apply(np.sign).map({-1: \"backwards\", 0: \"sideways\", 1: \"forwards\"})\ndpw_no_moves = dpw_pls[~dpw_pls[\"vac_id\"].isin(dpw_moves[\"vac_id\"])].assign(level_diff=pd.NA, level_dir=pd.NA)\ndpw_pls_with_moves = pd.concat([dpw_moves, dpw_no_moves])\nta11_people = (dpw_pls_with_moves.groupby([\"o_cli_id\"])[\"level_dir\"]\n               .apply(lambda x: (x == \"backwards\").sum())\n               .agg({\n                        \"1+ backwards %\": lambda x: 100 * (x &gt; 0).sum()/x.count(),\n                        \"mean backwards\": \"mean\",\n                        \"std_dev\": \"std\",\n                        \"max\": \"max\",\n                        \"n\": \"count\"})\n               .rename(\"all people\")\n               .to_frame().T\n               )\nta11_routes = (dpw_pls_with_moves.groupby([\"route_id\"])[\"level_dir\"]\n               .apply(lambda x: (x == \"backwards\").sum())\n               .agg(**{\n                        \"1+ backwards %\": lambda x: 100 * (x &gt; 0).sum()/x.count(),\n                        \"mean backwards\": \"mean\",\n                        \"std_dev\": \"std\",\n                        \"max\": \"max\",\n                        \"n\": \"count\"})\n               .rename(\"all routes\")\n               .to_frame().T\n               )\nta11 = (dpw_pls_with_moves.groupby([\"rt_end_cat\", \"route_id\"])[\"level_dir\"]\n        .apply(lambda x: (x == \"backwards\").sum())\n        .groupby(\"rt_end_cat\")\n        .agg(**{\n                 \"1+ backwards %\": lambda x: 100 * (x &gt; 0).sum()/x.count(),\n                 \"mean backwards\": \"mean\",\n                 \"std_dev\": \"std\",\n                 \"max\": \"max\",\n                 \"n\": \"count\"})\n        .sort_values(\"1+ backwards %\"))\nta11 = pd.concat([ta11, ta11_routes, ta11_people])\nwith pd.option_context(\"display.float_format\", \"{:,.3f}\".format):\n    display(ta11)\nexcel_output[\"Table A.11: Descriptive statistics for backward moves within routes by end reason\"] = ta11\n\n\n\n\n\n\n\n\n\n1+ backwards %\nmean backwards\nstd_dev\nmax\nn\n\n\n\n\nTo social housing\n7.407\n0.079\n0.288\n2.000\n999.000\n\n\n[Not ended]\n8.401\n0.084\n0.278\n1.000\n738.000\n\n\nDied\n9.402\n0.094\n0.293\n1.000\n117.000\n\n\nCustody\n9.524\n0.098\n0.308\n2.000\n336.000\n\n\nTo private rented\n9.712\n0.108\n0.344\n2.000\n278.000\n\n\nAbandoned\n11.111\n0.117\n0.341\n2.000\n639.000\n\n\nTo friends/family\n11.702\n0.120\n0.333\n2.000\n376.000\n\n\nMissing data/error\n12.863\n0.151\n0.418\n2.000\n482.000\n\n\nOther\n13.495\n0.152\n0.414\n3.000\n289.000\n\n\nEvicted\n14.072\n0.156\n0.402\n2.000\n867.000\n\n\nTo external supported\n14.970\n0.186\n0.474\n2.000\n167.000\n\n\nTo care/hospital\n18.129\n0.205\n0.459\n2.000\n171.000\n\n\nall routes\n11.711\n0.129\n0.370\n3.000\n5,072.000\n\n\nall people\n14.434\n0.174\n0.461\n4.000\n3,755.000"
  },
  {
    "objectID": "Tables_A.9_A.10_A.11.html#tables-in-an-excel-file",
    "href": "Tables_A.9_A.10_A.11.html#tables-in-an-excel-file",
    "title": "A (Appendix) - Tables A.9, A.10, A.11",
    "section": "Tables in an Excel file",
    "text": "Tables in an Excel file\n\nCode\nimport os\nsheetname = \"Sheet1\"\nqmdfile = os.getenv(\"QUARTO_DOCUMENT_FILE\")\nfilename = \"img_output/\"+qmdfile[:-4]+\".xlsx\"\nwith pd.ExcelWriter(filename) as writer:\n    next_row = 0\n    for k, v in excel_output.items():\n        pd.DataFrame([k]).to_excel(writer, sheet_name=sheetname, header=False, index=False, startrow=next_row)\n        next_row += 1\n        v.to_excel(writer, sheet_name=sheetname, startrow=next_row, float_format=\"%.1f\")\n        next_row += (\n            (1 if not isinstance(v.columns, pd.MultiIndex) else 1 + len(v.columns.levels))\n            + len(v) + 1)\nprint(f\"[Excel file](./{filename})\")\n\nExcel file"
  },
  {
    "objectID": "4.6_moves_between_services.html",
    "href": "4.6_moves_between_services.html",
    "title": "4.6 Results - Moves between services",
    "section": "",
    "text": "Code\n# Setup code\nimport pandas as pd\nfrom python_scripts import setup\nfrom python_scripts import services\nfrom python_scripts import helper\ndf, dffp = setup.setup(verbose=False)\n\nfrom python_scripts import distinct_pathways\ndpw_pls = distinct_pathways.get_distinct_pathways_routes(dffp)\nqrows = dpw_pls[\"rt_end_cat\"] == \"[Not ended or invalid end reason]\"\ndpw_pls.loc[qrows, \"rt_end_cat\"] = dpw_pls.loc[qrows, \"pl_end_dt\"].isna().map({True: \"[Not ended]\", False: \"Missing data/error\"})\ndpw_pls[\"rt_end_cat\"] = dpw_pls[\"rt_end_cat\"].str.replace(\"To care/hosp.\", \"To care/hospital\")\ndpw_rt_starts = dpw_pls.groupby(\"route_id\").head(1)\ndpw_rt_ends = dpw_pls.groupby(\"route_id\").tail(1)\ndpw_clis = dpw_pls.groupby([\"o_cli_id\"]).head(1)  # 1 duplicate client but first entry has more detailed answers\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nmatplotlib.rcParams.update({\n    'figure.figsize': (6.2, 4.1),\n    'font.size': 12,\n    'figure.constrained_layout.use': True\n})\n\nexcel_output = {}"
  },
  {
    "objectID": "4.6_moves_between_services.html#moves-between-services-rq6",
    "href": "4.6_moves_between_services.html#moves-between-services-rq6",
    "title": "4.6 Results - Moves between services",
    "section": "Moves between services (RQ6)",
    "text": "Moves between services (RQ6)\n\n\nCode\n# Set up networks\nfrom IPython.display import Image, display\nfrom python_scripts.graphics import showImg\nfrom python_scripts import networks\nfrom python_scripts import network_graphs\n\nextant = dpw_pls.groupby(\"o_cli_id\").head(1)[\n    lambda x: x.pl_start_dt &lt; distinct_pathways.dpw_start_dt\n]\nnetwork_definitions = [\n    {\"title\": \"all\", \"df\": dpw_pls},\n]\nDGs = []\nmin_n = 3\nnt = \"svc_typelvl\"\nfor i in network_definitions:\n    title_i, df_i = (i.get(key) for key in [\"title\", \"df\"])\n    sorted_end_cats = distinct_pathways.get_sorted_end_cats(df_i)\n    adj_list_i, edge_list_i = networks.mkAdjEdgeLists(dffp, df_i, [nt])\n    DG = network_graphs.mkDgraph(edge_list_i[nt], title=title_i, threshold=0.07,\n                                 min_n=min_n, sorted_end_cats=sorted_end_cats, capacities=distinct_pathways.capacities,\n                                 individuals=df_i.o_cli_id.nunique())\n    DGs.append(DG)\n    DG = network_graphs.mkDgraph(adj_list_i[nt], title=title_i, threshold=(0.020 * adj_list_i[nt].weight.max()),\n                                 min_n=min_n, sorted_end_cats=sorted_end_cats, capacities=distinct_pathways.capacities,\n                                 individuals=df_i.o_cli_id.nunique())\n    DGs.append(DG)\n\nfig_4_11_svg = \"img_output/Figure_4.11.svg\"\nfig_4_11_png = \"img_output/Figure_4.11.png\"\nDGs[0].draw(format=\"png\", prog=\"neato\", path=fig_4_11_png)\nDGs[0].draw(format=\"svg\", prog=\"neato\", path=fig_4_11_svg)\nfig_4_12_svg = \"img_output/Figure_4.12.svg\"\nfig_4_12_png = \"img_output/Figure_4.12.png\"\nDGs[1].draw(format=\"png\", prog=\"neato\", path=fig_4_12_png)\nDGs[1].draw(format=\"svg\", prog=\"neato\", path=fig_4_12_svg)\n\n\n\nFigure 4.11: Network diagram: proportions of moves out of each node for all BAHSA placements\n\nPNG file\n\n\n\nSVG file\n\nNotes: Oval nodes are levels of services in BAHSA with sizes proportionate to the number of allocated placements on night 1 of the studied period. Rounded rectangles are descriptions of starting points or outcomes. Arrows are moves, with line thickness proportionate to the count/proportion of moves represented. Pathways: F=Female-only, M/F=Mixed, SU=Substance Use, M=Male-only. Levels: L1 (except for SU) is 24-hour hostels, L2 (L1 in SU) is semi-independent supported housing, L3 and L4 (L2 in SU) is mostly accommodation with floating/visiting support.\n\n\n\nFigure 4.12: Network diagram: counts of moves for all BAHSA placements\n\nPNG file\n\n\n\nSVG file\n\nNotes: Oval nodes are levels of services in BAHSA with sizes proportionate to the number of allocated placements on night 1 of the studied period. Rounded rectangles are descriptions of starting points or outcomes. Arrows are moves, with line thickness proportionate to the count/proportion of moves represented. Pathways: F=Female-only, M/F=Mixed, SU=Substance Use, M=Male-only. Levels: L1 (except for SU) is 24-hour hostels, L2 (L1 in SU) is semi-independent supported housing, L3 and L4 (L2 in SU) is mostly accommodation with floating/visiting support."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BSHARP",
    "section": "",
    "text": "Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nAuthor\n\n\n\n\n\n\n\n\nFeb 6, 2026\n\n\n4.1 Results - Outcomes\n\n\nDavid Ingerslev\n\n\n\n\n\n\nFeb 8, 2026\n\n\n4.2 Results - Capacity and throughput\n\n\nDavid Ingerslev\n\n\n\n\n\n\nFeb 8, 2026\n\n\n4.3 Results - Number of routes\n\n\nDavid Ingerslev\n\n\n\n\n\n\nFeb 10, 2026\n\n\n4.4 Results - Durations\n\n\nDavid Ingerslev\n\n\n\n\n\n\nFeb 10, 2026\n\n\n4.5 Results - Returns\n\n\nDavid Ingerslev\n\n\n\n\n\n\nFeb 10, 2026\n\n\n4.6 Results - Moves between services\n\n\nDavid Ingerslev\n\n\n\n\n\n\nFeb 11, 2026\n\n\nA (Appendix) - Table A.1\n\n\nDavid Ingerslev\n\n\n\n\n\n\nFeb 11, 2026\n\n\nA (Appendix) - Tables A.9, A.10, A.11\n\n\nDavid Ingerslev\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Table_A.1.html",
    "href": "Table_A.1.html",
    "title": "A (Appendix) - Table A.1",
    "section": "",
    "text": "Code\n# Setup code\nimport pandas as pd\nfrom python_scripts import setup\nfrom python_scripts import services\nfrom python_scripts import helper\ndf, dffp = setup.setup(verbose=False)\n\nfrom python_scripts import distinct_pathways\ndpw_pls = distinct_pathways.get_distinct_pathways_routes(dffp)\nqrows = dpw_pls[\"rt_end_cat\"] == \"[Not ended or invalid end reason]\"\ndpw_pls.loc[qrows, \"rt_end_cat\"] = dpw_pls.loc[qrows, \"pl_end_dt\"].isna().map({True: \"[Not ended]\", False: \"Missing data/error\"})\ndpw_pls[\"rt_end_cat\"] = dpw_pls[\"rt_end_cat\"].str.replace(\"To care/hosp.\", \"To care/hospital\")\ndpw_rt_starts = dpw_pls.groupby(\"route_id\").head(1)\ndpw_rt_ends = dpw_pls.groupby(\"route_id\").tail(1)\ndpw_clis = dpw_pls.groupby([\"o_cli_id\"]).head(1)  # 1 duplicate client but first entry has more detailed answers\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nmatplotlib.rcParams.update({\n    'figure.figsize': (6.2, 4.1),\n    'font.size': 12,\n    'figure.constrained_layout.use': True\n})\n\nexcel_output = {}\n\ndatelist = list((\n    (\n        distinct_pathways.dpw_start_dt +\n        i * (distinct_pathways.dpw_end_dt - distinct_pathways.dpw_start_dt)/5\n    ).round(\"d\") for i in range(1, 6)\n))  # 6 dates between start and end (inclusive): 3 Octobers, 3 Aprils.\ndpw_snapshots = pd.concat(helper.get_snapshot(dpw_pls, d).assign(snapshot_dt=d.strftime(\"%Y-%m-%d\")) for d in datelist)\ndpw_snapshots = pd.concat([dpw_clis.assign(snapshot_dt=\"Longitudinal\"), dpw_snapshots])\ncharacteristics = {}"
  },
  {
    "objectID": "Table_A.1.html#additional-tables",
    "href": "Table_A.1.html#additional-tables",
    "title": "A (Appendix) - Table A.1",
    "section": "Additional Tables",
    "text": "Additional Tables\n\nTable A.1: Individual characteristics of BAHSA population with comparable reference data\n\nApproximate age\n\n\nCode\nref = {\"&lt;18\": 1, \"18-25\": 14, \"26-39\": 36, \"40-64\": 46, \"&gt;=65\": 3}\nref_n = 1044\ncomp_dt = dpw_snapshots.snapshot_dt\ncomp_dt[dpw_snapshots.snapshot_dt == \"Longitudinal\"] = dpw_snapshots.pl_start_dt\ncomp_dt = comp_dt.astype(\"datetime64[ns]\")\nchr = dpw_snapshots.assign(\n    age_cat=helper.age_cats_bgp(comp_dt.dt.year - dpw_snapshots.yob)\n)[[\"snapshot_dt\", \"age_cat\"]]\nn = chr.groupby(\"snapshot_dt\", sort=False).value_counts(sort=False, dropna=False).unstack(0)\npct = chr.groupby(\"snapshot_dt\", sort=False).value_counts(sort=False, normalize=True).unstack(0)\nfor t in [n, pct]: t[\"C/S Mean\"] = t[t.columns[t.columns.str.startswith(\"20\")]].mean(axis=1)\npct = pct.mul(100).map(lambda p: f\"{p:.1f}%\")\npct.columns = pct.columns.astype(\"string\")\nif n.index.isna().sum() != 0: pct.loc[\"&lt;NA&gt; (n)\", :] = n.loc[n.index.isna()].iloc[0, :].map(lambda x: f\"{x:,.0f}\")\npct.loc[\"Total (n)\",:] = n.sum().map(lambda x: f\"{x:,.0f}\")\npct = pct.assign(Ref_HSH=list(f\"{x}%\" for x in ref.values()) + [f\"{ref_n:,}\"])\nfor t in [n, pct]: c = t.pop(\"Longitudinal\"); t.insert(0, \"Longitudinal\", c)\ncharacteristics[\"Ages\"] = pct\ndisplay(pct)\n\n\n\n\n\n\n\n\nsnapshot_dt\nLongitudinal\n2019-04-29\n2020-10-28\n2022-04-30\n2023-10-30\n2025-04-30\nC/S Mean\nRef_HSH\n\n\nage_cat\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;18\n0.0%\n0.0%\n0.0%\n0.0%\n0.0%\n0.0%\n0.0%\n1%\n\n\n18-25\n11.8%\n5.7%\n6.3%\n6.2%\n5.9%\n4.6%\n5.8%\n14%\n\n\n26-39\n45.5%\n42.6%\n41.7%\n41.0%\n41.4%\n37.7%\n40.9%\n36%\n\n\n40-64\n41.4%\n50.5%\n50.9%\n51.8%\n51.4%\n55.2%\n52.0%\n46%\n\n\n&gt;=65\n1.3%\n1.1%\n1.1%\n0.9%\n1.2%\n2.5%\n1.4%\n3%\n\n\nTotal (n)\n3,755\n819\n829\n787\n741\n763\n788\n1,044\n\n\n\n\n\n\n\n\n\nGender\n\n\nCode\nref = {\"Female\": 22, \"Male\": 76, \"Non-binary\": 0, \"Other\": 1}\nref_n = 1044\nchr = dpw_snapshots[[\"snapshot_dt\", \"gender\"]]\nchr.gender = chr.gender.cat.reorder_categories([\"Female\", \"Male\", \"Non-Binary\", \"Other\", \"Transgender\", \"Prefer not to say\", \"Don't Know\"])\nn = chr.groupby(\"snapshot_dt\", sort=False).value_counts(sort=False, dropna=False).unstack(0)\npct = chr.groupby(\"snapshot_dt\", sort=False).value_counts(sort=False, normalize=True).unstack(0)\nfor t in [n, pct]: t[\"C/S Mean\"] = t[t.columns[t.columns.str.startswith(\"20\")]].mean(axis=1)\npct = pct.mul(100).map(lambda p: f\"{p:.1f}%\")\npct.columns = pct.columns.astype(\"string\")\nif n.index.isna().sum() != 0: pct.loc[\"&lt;NA&gt; (n)\", :] = n.loc[n.index.isna()].iloc[0, :].map(lambda x: f\"{x:,.0f}\")\npct.loc[\"Total (n)\",:] = n.sum().map(lambda x: f\"{x:,.0f}\")\npct = pct.assign(Ref_HSH=list(f\"{x}%\" for x in ref.values()) + [\"-\"]*4 + [f\"{ref_n:,}\"])\nfor t in [n, pct]: c = t.pop(\"Longitudinal\"); t.insert(0, \"Longitudinal\", c)\ncharacteristics[\"Gender\"] = pct\ndisplay(pct)\n\n\n\n\n\n\n\n\nsnapshot_dt\nLongitudinal\n2019-04-29\n2020-10-28\n2022-04-30\n2023-10-30\n2025-04-30\nC/S Mean\nRef_HSH\n\n\ngender\n\n\n\n\n\n\n\n\n\n\n\n\nFemale\n27.3%\n25.9%\n23.5%\n23.1%\n23.1%\n22.5%\n23.6%\n22%\n\n\nMale\n70.3%\n72.2%\n74.8%\n74.8%\n74.6%\n75.0%\n74.3%\n76%\n\n\nNon-Binary\n0.1%\n0.0%\n0.0%\n0.0%\n0.0%\n0.3%\n0.1%\n0%\n\n\nOther\n0.0%\n0.0%\n0.0%\n0.0%\n0.0%\n0.0%\n0.0%\n1%\n\n\nTransgender\n0.5%\n0.6%\n0.4%\n0.4%\n0.7%\n0.7%\n0.5%\n-\n\n\nPrefer not to say\n0.7%\n1.3%\n0.6%\n0.1%\n0.3%\n0.3%\n0.5%\n-\n\n\nDon't Know\n1.1%\n0.0%\n0.7%\n1.5%\n1.4%\n1.3%\n1.0%\n-\n\n\n&lt;NA&gt; (n)\n1\n0\n0\n0\n1\n0\n0\n-\n\n\nTotal (n)\n3,755\n819\n829\n787\n741\n763\n788\n1,044\n\n\n\n\n\n\n\n\n\nGlobal majority background\n\n\nCode\nfrom collections import defaultdict\nref = {\"Yes\": 16, \"No\": 82, \"Don't know\": 2}\nref_n = 1044\nchr = dpw_snapshots[[\"snapshot_dt\", \"ethnicity\"]]\nchr[\"global_majority\"] = chr[\"ethnicity\"].map(defaultdict(\n                                              lambda: \"Yes\",\n                                              {\n                                                \"English/Welsh/Scottish/Northern Irish/British\": \"No\",\n                                                \"White British\": \"No\",\n                                                \"White Other Origin\": \"No\",\n                                                \"Any other White background\": \"No\",\n                                                \"White European\": \"No\",\n                                                \"Eastern European\": \"No\",\n                                                \"Turkish\": \"No\",\n                                                \"White Irish\": \"No\",\n                                                \"Irish\": \"No\",\n                                                \"White Gypsy/Irish Traveller\": \"No\",\n                                                \"Gypsy (inc English, Scottish or Roma Gypsy) or Irish Traveller\": \"No\",\n                                                \"Don't know\": \"Don't know\",\n                                                \"Prefer not to say\": \"Don't know\",\n                                                \"Refused to answer question\": \"Don't know\"\n                                              }))\nchr = chr.drop(\"ethnicity\", axis=1)\nchr[\"global_majority\"] = chr[\"global_majority\"].astype(\"category\").cat.reorder_categories([\"Yes\", \"No\", \"Don't know\"])\nn = chr.groupby(\"snapshot_dt\", sort=False).value_counts(sort=False, dropna=False).unstack(0)\npct = chr.groupby(\"snapshot_dt\", sort=False).value_counts(sort=False, normalize=True).unstack(0)\nfor t in [n, pct]: t[\"C/S Mean\"] = t[t.columns[t.columns.str.startswith(\"20\")]].mean(axis=1)\npct = pct.mul(100).map(lambda p: f\"{p:.1f}%\")\npct.columns = pct.columns.astype(\"string\")\nif n.index.isna().sum() != 0: pct.loc[\"&lt;NA&gt; (n)\", :] = n.loc[n.index.isna()].iloc[0, :].map(lambda x: f\"{x:,.0f}\")\npct.loc[\"Total (n)\",:] = n.sum().map(lambda x: f\"{x:,.0f}\")\npct = pct.assign(Ref_HSH=list(f\"{x}%\" for x in ref.values()) + [\"-\"] +[f\"{ref_n:,}\"])\nfor t in [n, pct]: c = t.pop(\"Longitudinal\"); t.insert(0, \"Longitudinal\", c)\ncharacteristics[\"Global Majority\"] = pct\ndisplay(pct)\n\n\n\n\n\n\n\n\nsnapshot_dt\nLongitudinal\n2019-04-29\n2020-10-28\n2022-04-30\n2023-10-30\n2025-04-30\nC/S Mean\nRef_HSH\n\n\nglobal_majority\n\n\n\n\n\n\n\n\n\n\n\n\nYes\n24.2%\n21.6%\n23.9%\n26.0%\n23.8%\n24.4%\n23.9%\n16%\n\n\nNo\n70.8%\n75.5%\n71.7%\n69.6%\n71.9%\n68.3%\n71.4%\n82%\n\n\nDon't know\n5.0%\n2.9%\n4.5%\n4.3%\n4.3%\n7.3%\n4.7%\n2%\n\n\n&lt;NA&gt; (n)\n1\n0\n0\n0\n1\n0\n0\n-\n\n\nTotal (n)\n3,755\n819\n829\n787\n741\n763\n788\n1,044\n\n\n\n\n\n\n\n\n\nEthnicity\n\n\nCode\nimport numpy as np\nchr = dpw_snapshots[[\"snapshot_dt\", \"ethnicity\"]]\ncats = chr[\"ethnicity\"].cat.categories\ncat_map = {\n    \"White British\": [\n        \"English/Welsh/Scottish/Northern Irish/British\"\n    ],\n    \"Unknown\": [\n        \"Don't know\",\n        \"Prefer not to say\",\n        \"Refused to answer question\"\n    ],\n    \"Black/Black British - African\": [\n        \"African (non Somali)\",\n        \"Somali\",\n        \"Black/Black British - Somali\",\n        \"Black or Black British Ghanaian\"\n    ],\n    \"Black/Black British - Caribbean\": [\n        \"Caribbean\"\n    ],\n    \"White Other\": [\n        \"White Other Origin\",\n        \"Any other White background\",\n        \"White Irish\",\n        \"Irish\",\n        \"White European\",\n        \"Eastern European\",\n        \"Turkish\"\n    ],\n    \"Mixed/Dual Heritage - White+Black Caribbean\": [\n        \"White and Black Caribbean\"\n    ],\n    \"Black/Black British - Other\": [\n        \"Black/Black British - Other\",\n        \"Any other Black/African/Caribbean background\"\n    ],\n    \"Arab\": [\n        \"Iranian\",\n        \"Iraqi\",\n    ],\n    \"Asian/Asian British\": [\n        \"Asian/Asian British - Other\",\n        \"Pakistani\",\n        \"Bangladeshi\",\n        \"Asian/Asian British - Chinese\",\n        \"Asian/Asian British - Bangladeshi\",\n        \"Asian/Asian British - Indian\",\n        \"Indian\",\n        \"Asian/Asian British - Asian African\",\n        \"Chinese\",\n        \"Asian/Asian British - Pakistani\",\n        \"Asian or Asian British Sri Lankan\",\n        \"Any other Asian background\",\n    ],\n    \"Any other Mixed/multiple ethnic background\": [\n        \"Mixed/Dual Heritage - White+Asian\",\n        \"Mixed/Dual Heritage - Other\",\n        \"White and Asian\",\n        \"Mixed/Dual Heritage - White+Chinese\"\n    ],\n    \"Mixed/Dual Heritage - White+Black African\": [\n        \"White and Black African (non Somali)\"\n    ],\n    \"Other\": [\n        \"Any other ethnic group\",\n        \"Kurdish\"\n    ],\n    \"White Gypsy/Irish Traveller\": [\n        \"Gypsy (inc English, Scottish or Roma Gypsy) or Irish Traveller\"\n    ]\n}\nmapper = {k:v for v,k in pd.Series(cat_map).explode().items()}\nchr[\"ethnicity\"] = chr[\"ethnicity\"].replace(mapper).astype(\"string\").astype(\"category\")\ncat_grp_map = {\n    \"White\": [\n        \"White British\",\n        \"White Other\",\n        \"White Gypsy/Irish Traveller\",\n    ],\n    \"Black\": [\n        \"Black/Black British - African\",\n        \"Black/Black British - Caribbean\",\n        \"Black/Black British - Other\",\n    ],\n    \"Mixed Heritage\": [\n        \"Mixed/Dual Heritage - White+Black African\",\n        \"Mixed/Dual Heritage - White+Black Caribbean\",\n        \"Any other Mixed/multiple ethnic background\",\n    ]\n}\nn = chr.groupby(\"snapshot_dt\", sort=False).value_counts(sort=False, dropna=False).unstack(0)\npct = chr.groupby(\"snapshot_dt\", sort=False).value_counts(sort=False, normalize=True).unstack(0)\nfor t in [n, pct]: t[\"C/S Mean\"] = t[t.columns[t.columns.str.startswith(\"20\")]].mean(axis=1)\nfor tbl in [n, pct]:\n    mapper = {k:v for v,k in pd.Series(cat_grp_map).explode().items()}\n    idx = tbl.index.to_frame()\n    idx.insert(0, 'eth_grp', idx.ethnicity.replace(mapper))\n    tbl.index = pd.MultiIndex.from_frame(idx.astype(\"string\"), names=[\"Ethnicity\",None])\npct = pct.sort_values(\"Longitudinal\", ascending=False)\ngrp = pct.groupby(level=0)\ngrp_sum = grp.sum().sort_values(\"Longitudinal\", ascending=False)\nidx = grp_sum.index.to_frame()\nidx.insert(1, None, '')\ngrp_sum.index = pd.MultiIndex.from_frame(idx)\npct = pd.concat([grp_sum, pct])\npct = pct[pct.index.get_level_values(0).isin(grp.size()[lambda x: x &gt; 1].index) | (pct.index.get_level_values(1) == \"\")]\ngrp_order = [i for i in grp_sum.index.get_level_values(0) if i != \"Unknown\"] + [\"Unknown\"]\npct = pct.sort_index(level=0, key=lambda x: x.map(grp_order.index), sort_remaining=False)\npct = pct.mul(100).map(lambda p: f\"{p:.1f}%\")\npct.columns = pct.columns.astype(\"string\")\npct.loc[(\"&lt;NA&gt; (n)\", \"\"), :] = n.loc[n.index.get_level_values(0).isna()].iloc[0, :].map(lambda x: f\"{x:,.0f}\")\npct.loc[(\"Total (n)\", \"\"), :] = n.sum().map(lambda x: f\"{x:,.0f}\")\nfor t in [n, pct]: c = t.pop(\"Longitudinal\"); t.insert(0, \"Longitudinal\", c)\ncharacteristics[\"Ethnicity\"] = pct\ndisplay(pct)\n\n\n\n\n\n\n\n\n\nsnapshot_dt\nLongitudinal\n2019-04-29\n2020-10-28\n2022-04-30\n2023-10-30\n2025-04-30\nC/S Mean\n\n\nEthnicity\n\n\n\n\n\n\n\n\n\n\n\n\nWhite\n\n70.8%\n75.5%\n71.7%\n69.6%\n71.9%\n68.3%\n71.4%\n\n\nWhite British\n65.2%\n70.7%\n65.1%\n62.9%\n65.8%\n61.1%\n65.1%\n\n\nWhite Other\n5.3%\n4.5%\n6.3%\n6.6%\n5.9%\n7.1%\n6.1%\n\n\nWhite Gypsy/Irish Traveller\n0.3%\n0.2%\n0.2%\n0.1%\n0.1%\n0.1%\n0.2%\n\n\nBlack\n\n13.2%\n11.7%\n12.8%\n14.7%\n14.1%\n13.6%\n13.4%\n\n\nBlack/Black British - African\n7.5%\n5.7%\n7.4%\n7.9%\n7.6%\n6.8%\n7.1%\n\n\nBlack/Black British - Caribbean\n3.9%\n4.0%\n3.9%\n4.6%\n4.3%\n4.8%\n4.3%\n\n\nBlack/Black British - Other\n1.9%\n2.0%\n1.6%\n2.3%\n2.2%\n2.0%\n2.0%\n\n\nMixed Heritage\n\n5.2%\n4.8%\n5.2%\n6.0%\n5.5%\n4.5%\n5.2%\n\n\nMixed/Dual Heritage - White+Black Caribbean\n3.1%\n2.2%\n2.9%\n3.8%\n3.4%\n2.8%\n3.0%\n\n\nAny other Mixed/multiple ethnic background\n1.2%\n1.7%\n1.6%\n1.3%\n1.1%\n1.2%\n1.4%\n\n\nMixed/Dual Heritage - White+Black African\n0.9%\n0.9%\n0.7%\n0.9%\n1.1%\n0.5%\n0.8%\n\n\nAsian/Asian British\n\n2.8%\n2.4%\n3.1%\n2.8%\n1.9%\n2.6%\n2.6%\n\n\nArab\n\n1.5%\n1.6%\n1.6%\n1.0%\n0.9%\n1.6%\n1.3%\n\n\nOther\n\n1.4%\n1.1%\n1.2%\n1.5%\n1.4%\n2.1%\n1.5%\n\n\nUnknown\n\n5.0%\n2.9%\n4.5%\n4.3%\n4.3%\n7.3%\n4.7%\n\n\n&lt;NA&gt; (n)\n\n1\n0\n0\n0\n1\n0\n0\n\n\nTotal (n)\n\n3,755\n819\n829\n787\n741\n763\n788\n\n\n\n\n\n\n\n\n\nExperiences (risk: ever)\n\n\nCode\nfrom collections import defaultdict\nref = {\n    \"Offending\": 54,\n    \"&lt;NA&gt; 1\": pd.NA,\n    \"&lt;NA&gt; 2\": pd.NA,\n    \"Domestic abuse\": 26,\n    \"Women: domestic abuse\": 61,\n    \"&lt;NA&gt; 3\": pd.NA,\n    \"&lt;NA&gt; 4\": pd.NA,\n    \"&lt;NA&gt; 5\": pd.NA,\n    \"&lt;NA&gt; 6\": pd.NA,\n    \"&lt;NA&gt; 7\": pd.NA,\n    \"&lt;NA&gt; 8\": pd.NA,\n    \"&lt;NA&gt; 9\": pd.NA,\n    \"Long-term/repeat homelessness\": 19,\n    \"Looked-after-child\": 12,\n}\nref_n = 1044\n# chr = dpw_snapshots[[\"snapshot_dt\", \"risk_crime_yn\", \"hsn_risk_of_violenceabuse_victim\", \"gender\", \"hsn_rough_sleeper\", \"hsn_care_leaver\"]]\nchr = dpw_snapshots[\"snapshot_dt\"].to_frame()\nrisk_rows = []\nchr[\"Criminal conviction/investigation\"] = (\n    dpw_snapshots[\"risk_crime_yn\"].eq(\"Yes\").mask(dpw_snapshots[\"risk_crime_yn\"].isna()))\nrisk_rows.append(\"Criminal conviction/investigation\")\nchr[\"Been violent or discriminatory\"] = (\n    dpw_snapshots[\"risk_violence_hatecrime_yn\"].eq(\"Yes\").mask(dpw_snapshots[\"risk_violence_hatecrime_yn\"].isna()))\nrisk_rows.append(\"Been violent or discriminatory\")\nchr[\"Prison leaver, probation or offending\"] = dpw_snapshots[\"hsn_prison_leaveron_probationrisk_of_offending\"]\nchr[\"Domestic abuse (victim)\"] = dpw_snapshots[\"hsn_risk_of_violenceabuse_victim\"]\nchr[\"Women: domestic abuse (victim)\"] = (\n    dpw_snapshots[\"hsn_risk_of_violenceabuse_victim\"].where(dpw_snapshots[\"gender\"] == \"Female\"))\nchr[\"Men: domestic abuse (victim)\"] = (\n    dpw_snapshots[\"hsn_risk_of_violenceabuse_victim\"].where(dpw_snapshots[\"gender\"] == \"Male\"))\nchr[\"Sexual abuse/assault (perpetrator)\"] = (\n    dpw_snapshots[\"risk_sexual_yn\"].eq(\"Yes\").mask(dpw_snapshots[\"risk_sexual_yn\"].isna()))\nrisk_rows.append(\"Sexual abuse/assault (perpetrator)\")\nchr[\"Women: Sexual abuse/assault (perpetrator)\"] = (\n    dpw_snapshots[\"risk_sexual_yn\"].eq(\"Yes\").mask(dpw_snapshots[\"risk_sexual_yn\"].isna())).where(dpw_snapshots[\"gender\"] == \"Female\")\nrisk_rows.append(\"Women: Sexual abuse/assault (perpetrator)\")\nchr[\"Men: Sexual abuse/assault (perpetrator)\"] = (\n    dpw_snapshots[\"risk_sexual_yn\"].eq(\"Yes\").mask(dpw_snapshots[\"risk_sexual_yn\"].isna())).where(dpw_snapshots[\"gender\"] == \"Male\")\nrisk_rows.append(\"Women: Sexual abuse/assault (perpetrator)\")\nchr[\"Domestic abuse (perpetrator)\"] = dpw_snapshots[\"hsn_risk_of_violenceabuse_perpetrator\"]\nchr[\"Women: Domestic abuse (perpetrator)\"] = dpw_snapshots[\"hsn_risk_of_violenceabuse_perpetrator\"].where(dpw_snapshots[\"gender\"] == \"Female\")\nchr[\"Men: Domestic abuse (perpetrator)\"] = dpw_snapshots[\"hsn_risk_of_violenceabuse_perpetrator\"].where(dpw_snapshots[\"gender\"] == \"Male\")\nchr[\"&lt;NA&gt; 1\"] = -1\nchr[\"Care leaver\"] = dpw_snapshots[\"hsn_care_leaver\"]\nchr[\"Rough sleeping\"] = dpw_snapshots[\"hsn_rough_sleeper\"]\nchr[\"Self harmed or attempted suicide\"] = (\n    dpw_snapshots[\"risk_suicide_selfharm_yn\"].eq(\"Yes\").mask(dpw_snapshots[\"risk_suicide_selfharm_yn\"].isna()))\nrisk_rows.append(\"Self harmed or attempted suicide\")\nchr[\"Suicide attempts\"] = dpw_snapshots[\"hsn_suicide_attempts\"]\nchr[\"Self harm\"] = dpw_snapshots[\"hsn_self_harm\"]\nchr[\"Risk of exploitation\"] = dpw_snapshots[\"hsn_risk_of_exploitation\"]\nchr[\"Setting fires\"] = (\n    dpw_snapshots[\"risk_fire_yn\"].eq(\"Yes\").mask(dpw_snapshots[\"risk_fire_yn\"].isna()))\nrisk_rows.append(\"Setting fires\")\nchr[\"Refugee\"] = dpw_snapshots[\"hsn_refugee\"]\nchr[\"Sex working\"] = dpw_snapshots[\"hsn_sex_working\"]\nchr[\"Women: sex working\"] = dpw_snapshots[\"hsn_sex_working\"].where(dpw_snapshots[\"gender\"] == \"Female\")\nchr[\"Men: sex working\"] = dpw_snapshots[\"hsn_sex_working\"].where(dpw_snapshots[\"gender\"] == \"Male\")\nchr[\"Total (n)\"] = 1\nn = chr.groupby(\"snapshot_dt\", sort=False).sum().T\npct = chr.groupby(\"snapshot_dt\", sort=False).apply(lambda x: x.sum()/x.count()).T\nfor t in [n, pct]: t[\"C/S Mean\"] = t[t.columns[t.columns.str.startswith(\"20\")]].mean(axis=1)\npct = pct.mul(100).map(lambda p: f\"{p:.1f}%\")\nfor t in [n, pct]: c = t.pop(\"Longitudinal\"); t.insert(0, \"Longitudinal\", c)\npct.loc[\"Total (n)\"] = n.loc[\"Total (n)\"].map(lambda x: f\"{x:,.0f}\")\npct.columns = pct.columns.astype(\"string\")\npct.insert(0, \"Src\", \"S\")\npct.loc[risk_rows, \"Src\"] = \"R\"\npct[pct.index.str.startswith(\"&lt;NA&gt;\")] = \"-\"\nref_df = pd.DataFrame(\n    data=list((f\"{v}%\", k) for (k, v) in ref.items()) + [(pd.NA, \"&lt;NA&gt;\")] * 10 + [(f\"{ref_n:,}\", \"\")],\n    columns=[\"Ref_HSH\", \"Ref_HSH cat\"],\n    index=pct.index)\nref_df[ref_df[\"Ref_HSH cat\"].str.startswith(\"&lt;NA&gt;\")] = \"-\"\npct = pd.concat([pct, ref_df], axis=1)\ncharacteristics[\"Experiences\"] = pct\ndisplay(pct)\n\n\n\n\n\n\n\n\n\nSrc\nLongitudinal\n2019-04-29\n2020-10-28\n2022-04-30\n2023-10-30\n2025-04-30\nC/S Mean\nRef_HSH\nRef_HSH cat\n\n\n\n\nCriminal conviction/investigation\nR\n53.7%\n49.9%\n53.4%\n53.1%\n51.4%\n48.6%\n51.3%\n54%\nOffending\n\n\nBeen violent or discriminatory\nR\n34.6%\n32.8%\n36.6%\n37.9%\n35.5%\n31.8%\n34.9%\n-\n-\n\n\nPrison leaver, probation or offending\nS\n17.0%\n10.4%\n13.8%\n14.8%\n13.1%\n12.8%\n13.0%\n-\n-\n\n\nDomestic abuse (victim)\nS\n17.7%\n13.1%\n14.1%\n15.0%\n17.2%\n17.3%\n15.3%\n26%\nDomestic abuse\n\n\nWomen: domestic abuse (victim)\nS\n39.1%\n28.1%\n34.6%\n34.5%\n36.8%\n40.8%\n34.9%\n61%\nWomen: domestic abuse\n\n\nMen: domestic abuse (victim)\nS\n9.3%\n7.2%\n7.1%\n8.4%\n10.7%\n10.0%\n8.7%\n-\n-\n\n\nSexual abuse/assault (perpetrator)\nR\n7.6%\n8.3%\n8.8%\n8.8%\n8.4%\n8.1%\n8.5%\n-\n-\n\n\nWomen: Sexual abuse/assault (perpetrator)\nR\n4.8%\n7.2%\n7.2%\n4.4%\n2.9%\n2.9%\n4.9%\n-\n-\n\n\nMen: Sexual abuse/assault (perpetrator)\nS\n8.6%\n8.7%\n9.1%\n10.1%\n10.2%\n9.6%\n9.6%\n-\n-\n\n\nDomestic abuse (perpetrator)\nS\n6.2%\n4.6%\n6.6%\n6.3%\n6.8%\n5.0%\n5.9%\n-\n-\n\n\nWomen: Domestic abuse (perpetrator)\nS\n4.6%\n5.7%\n8.4%\n2.8%\n5.5%\n3.8%\n5.3%\n-\n-\n\n\nMen: Domestic abuse (perpetrator)\nS\n6.8%\n4.3%\n6.1%\n7.0%\n6.9%\n5.2%\n5.9%\n-\n-\n\n\n&lt;NA&gt; 1\n-\n-\n-\n-\n-\n-\n-\n-\n19%\nLong-term/repeat homelessness\n\n\nCare leaver\nS\n1.5%\n0.4%\n0.8%\n0.8%\n0.6%\n0.6%\n0.6%\n12%\nLooked-after-child\n\n\nRough sleeping\nS\n20.7%\n9.6%\n15.2%\n20.3%\n16.6%\n22.1%\n16.8%\n-\n-\n\n\nSelf harmed or attempted suicide\nR\n49.1%\n44.3%\n45.6%\n46.4%\n48.8%\n50.2%\n47.1%\n-\n-\n\n\nSuicide attempts\nS\n5.9%\n3.3%\n3.0%\n4.3%\n4.7%\n6.0%\n4.3%\n-\n-\n\n\nSelf harm\nS\n4.8%\n3.3%\n3.5%\n4.3%\n3.7%\n3.9%\n3.8%\n-\n-\n\n\nRisk of exploitation\nS\n8.0%\n10.9%\n10.0%\n8.3%\n8.2%\n8.9%\n9.3%\n-\n-\n\n\nSetting fires\nR\n4.1%\n4.4%\n4.7%\n3.8%\n4.2%\n3.3%\n4.1%\n-\n-\n\n\nRefugee\nS\n2.8%\n2.1%\n3.2%\n2.8%\n2.7%\n3.2%\n2.8%\n-\n-\n\n\nSex working\nS\n1.8%\n2.2%\n2.1%\n1.3%\n2.1%\n1.5%\n1.9%\n-\n-\n\n\nWomen: sex working\nS\n6.5%\n8.3%\n8.9%\n5.6%\n9.2%\n6.4%\n7.7%\n-\n-\n\n\nMen: sex working\nS\n0.0%\n0.0%\n0.0%\n0.0%\n0.0%\n0.0%\n0.0%\n-\n-\n\n\nTotal (n)\nS\n3,755\n819\n829\n787\n741\n763\n788\n1,044\n\n\n\n\n\n\n\n\n\n\nExperiences (current)\n\n\nCode\nfrom collections import defaultdict\nref = {\n    \"Offending\": 54,\n    \"&lt;NA&gt; 1\": pd.NA,\n    \"&lt;NA&gt; 2\": pd.NA,\n    \"Domestic abuse\": 26,\n    \"Women: domestic abuse\": 61,\n    \"&lt;NA&gt; 3\": pd.NA,\n    \"&lt;NA&gt; 4\": pd.NA,\n    \"&lt;NA&gt; 5\": pd.NA,\n    \"&lt;NA&gt; 6\": pd.NA,\n    \"&lt;NA&gt; 7\": pd.NA,\n    \"&lt;NA&gt; 8\": pd.NA,\n    \"&lt;NA&gt; 9\": pd.NA,\n    \"Long-term/repeat homelessness\": 19,\n    \"Looked-after-child\": 12,\n}\nref_n = 1044\n# chr = dpw_snapshots[[\"snapshot_dt\", \"risk_crime_current\", \"hsn_risk_of_violenceabuse_victim\", \"gender\", \"hsn_rough_sleeper\", \"hsn_care_leaver\"]]\nchr = dpw_snapshots[\"snapshot_dt\"].to_frame()\nrisk_rows = []\nchr[\"Criminal conviction/investigation\"] = (\n    dpw_snapshots[\"risk_crime_current\"].eq(\"Current Risk\").mask(dpw_snapshots[\"risk_crime_yn\"].isna()))\nrisk_rows.append(\"Criminal conviction/investigation\")\nchr[\"Been violent or discriminatory\"] = (\n    dpw_snapshots[\"risk_violence_hatecrime_current\"].eq(\"Current Risk\").mask(dpw_snapshots[\"risk_violence_hatecrime_yn\"].isna()))\nrisk_rows.append(\"Been violent or discriminatory\")\nchr[\"Prison leaver, probation or offending\"] = dpw_snapshots[\"hsn_prison_leaveron_probationrisk_of_offending\"]\nchr[\"Domestic abuse (victim)\"] = dpw_snapshots[\"hsn_risk_of_violenceabuse_victim\"]\nchr[\"Women: domestic abuse (victim)\"] = (\n    dpw_snapshots[\"hsn_risk_of_violenceabuse_victim\"].where(dpw_snapshots[\"gender\"] == \"Female\"))\nchr[\"Men: domestic abuse (victim)\"] = (\n    dpw_snapshots[\"hsn_risk_of_violenceabuse_victim\"].where(dpw_snapshots[\"gender\"] == \"Male\"))\nchr[\"Sexual abuse/assault (perpetrator)\"] = (\n    dpw_snapshots[\"risk_sexual_current\"].eq(\"Current Risk\").mask(dpw_snapshots[\"risk_sexual_yn\"].isna()))\nrisk_rows.append(\"Sexual abuse/assault (perpetrator)\")\nchr[\"Women: Sexual abuse/assault (perpetrator)\"] = (\n    dpw_snapshots[\"risk_sexual_current\"].eq(\"Current Risk\").mask(dpw_snapshots[\"risk_sexual_yn\"].isna())).where(dpw_snapshots[\"gender\"] == \"Female\")\nrisk_rows.append(\"Women: Sexual abuse/assault (perpetrator)\")\nchr[\"Men: Sexual abuse/assault (perpetrator)\"] = (\n    dpw_snapshots[\"risk_sexual_current\"].eq(\"Current Risk\").mask(dpw_snapshots[\"risk_sexual_yn\"].isna())).where(dpw_snapshots[\"gender\"] == \"Male\")\nrisk_rows.append(\"Women: Sexual abuse/assault (perpetrator)\")\nchr[\"Domestic abuse (perpetrator)\"] = dpw_snapshots[\"hsn_risk_of_violenceabuse_perpetrator\"]\nchr[\"Women: Domestic abuse (perpetrator)\"] = dpw_snapshots[\"hsn_risk_of_violenceabuse_perpetrator\"].where(dpw_snapshots[\"gender\"] == \"Female\")\nchr[\"Men: Domestic abuse (perpetrator)\"] = dpw_snapshots[\"hsn_risk_of_violenceabuse_perpetrator\"].where(dpw_snapshots[\"gender\"] == \"Male\")\nchr[\"&lt;NA&gt; 1\"] = -1\nchr[\"Care leaver\"] = dpw_snapshots[\"hsn_care_leaver\"]\nchr[\"Rough sleeping\"] = dpw_snapshots[\"hsn_rough_sleeper\"]\nchr[\"Self harmed or attempted suicide\"] = (\n    dpw_snapshots[\"risk_suicide_selfharm_current\"].eq(\"Current Risk\").mask(dpw_snapshots[\"risk_suicide_selfharm_yn\"].isna()))\nrisk_rows.append(\"Self harmed or attempted suicide\")\nchr[\"Suicide attempts\"] = dpw_snapshots[\"hsn_suicide_attempts\"]\nchr[\"Self harm\"] = dpw_snapshots[\"hsn_self_harm\"]\nchr[\"Risk of exploitation\"] = dpw_snapshots[\"hsn_risk_of_exploitation\"]\nchr[\"Setting fires\"] = (\n    dpw_snapshots[\"risk_fire_current\"].eq(\"Current Risk\").mask(dpw_snapshots[\"risk_fire_yn\"].isna()))\nrisk_rows.append(\"Setting fires\")\nchr[\"Refugee\"] = dpw_snapshots[\"hsn_refugee\"]\nchr[\"Sex working\"] = dpw_snapshots[\"hsn_sex_working\"]\nchr[\"Women: sex working\"] = dpw_snapshots[\"hsn_sex_working\"].where(dpw_snapshots[\"gender\"] == \"Female\")\nchr[\"Men: sex working\"] = dpw_snapshots[\"hsn_sex_working\"].where(dpw_snapshots[\"gender\"] == \"Male\")\nchr[\"Total (n)\"] = 1\nn = chr.groupby(\"snapshot_dt\", sort=False).sum().T\npct = chr.groupby(\"snapshot_dt\", sort=False).apply(lambda x: x.sum()/x.count()).T\nfor t in [n, pct]: t[\"C/S Mean\"] = t[t.columns[t.columns.str.startswith(\"20\")]].mean(axis=1)\npct = pct.mul(100).map(lambda p: f\"{p:.1f}%\")\nfor t in [n, pct]: c = t.pop(\"Longitudinal\"); t.insert(0, \"Longitudinal\", c)\npct.loc[\"Total (n)\"] = n.loc[\"Total (n)\"].map(lambda x: f\"{x:,.0f}\")\npct.columns = pct.columns.astype(\"string\")\npct.insert(0, \"Src\", \"S\")\npct.loc[risk_rows, \"Src\"] = \"R\"\npct[pct.index.str.startswith(\"&lt;NA&gt;\")] = \"-\"\nref_df = pd.DataFrame(\n    data=list((f\"{v}%\", k) for (k, v) in ref.items()) + [(pd.NA, \"&lt;NA&gt;\")] * 10 + [(f\"{ref_n:,}\", \"\")],\n    columns=[\"Ref_HSH\", \"Ref_HSH cat\"],\n    index=pct.index)\nref_df[ref_df[\"Ref_HSH cat\"].str.startswith(\"&lt;NA&gt;\")] = \"-\"\npct = pd.concat([pct, ref_df], axis=1)\ncharacteristics[\"Experiences\"] = pct\ndisplay(pct)\n\n\n\n\n\n\n\n\n\nSrc\nLongitudinal\n2019-04-29\n2020-10-28\n2022-04-30\n2023-10-30\n2025-04-30\nC/S Mean\nRef_HSH\nRef_HSH cat\n\n\n\n\nCriminal conviction/investigation\nR\n23.4%\n15.2%\n20.0%\n19.8%\n18.5%\n17.3%\n18.2%\n54%\nOffending\n\n\nBeen violent or discriminatory\nR\n16.5%\n14.1%\n16.8%\n15.9%\n15.1%\n12.0%\n14.8%\n-\n-\n\n\nPrison leaver, probation or offending\nS\n17.0%\n10.4%\n13.8%\n14.8%\n13.1%\n12.8%\n13.0%\n-\n-\n\n\nDomestic abuse (victim)\nS\n17.7%\n13.1%\n14.1%\n15.0%\n17.2%\n17.3%\n15.3%\n26%\nDomestic abuse\n\n\nWomen: domestic abuse (victim)\nS\n39.1%\n28.1%\n34.6%\n34.5%\n36.8%\n40.8%\n34.9%\n61%\nWomen: domestic abuse\n\n\nMen: domestic abuse (victim)\nS\n9.3%\n7.2%\n7.1%\n8.4%\n10.7%\n10.0%\n8.7%\n-\n-\n\n\nSexual abuse/assault (perpetrator)\nR\n4.2%\n2.9%\n4.1%\n4.6%\n5.0%\n4.6%\n4.2%\n-\n-\n\n\nWomen: Sexual abuse/assault (perpetrator)\nR\n2.1%\n1.5%\n3.1%\n2.2%\n1.2%\n0.6%\n1.7%\n-\n-\n\n\nMen: Sexual abuse/assault (perpetrator)\nS\n4.9%\n3.5%\n4.1%\n5.1%\n6.2%\n5.8%\n4.9%\n-\n-\n\n\nDomestic abuse (perpetrator)\nS\n6.2%\n4.6%\n6.6%\n6.3%\n6.8%\n5.0%\n5.9%\n-\n-\n\n\nWomen: Domestic abuse (perpetrator)\nS\n4.6%\n5.7%\n8.4%\n2.8%\n5.5%\n3.8%\n5.3%\n-\n-\n\n\nMen: Domestic abuse (perpetrator)\nS\n6.8%\n4.3%\n6.1%\n7.0%\n6.9%\n5.2%\n5.9%\n-\n-\n\n\n&lt;NA&gt; 1\n-\n-\n-\n-\n-\n-\n-\n-\n19%\nLong-term/repeat homelessness\n\n\nCare leaver\nS\n1.5%\n0.4%\n0.8%\n0.8%\n0.6%\n0.6%\n0.6%\n12%\nLooked-after-child\n\n\nRough sleeping\nS\n20.7%\n9.6%\n15.2%\n20.3%\n16.6%\n22.1%\n16.8%\n-\n-\n\n\nSelf harmed or attempted suicide\nR\n18.4%\n10.8%\n13.9%\n14.8%\n15.6%\n17.7%\n14.6%\n-\n-\n\n\nSuicide attempts\nS\n5.9%\n3.3%\n3.0%\n4.3%\n4.7%\n6.0%\n4.3%\n-\n-\n\n\nSelf harm\nS\n4.8%\n3.3%\n3.5%\n4.3%\n3.7%\n3.9%\n3.8%\n-\n-\n\n\nRisk of exploitation\nS\n8.0%\n10.9%\n10.0%\n8.3%\n8.2%\n8.9%\n9.3%\n-\n-\n\n\nSetting fires\nR\n1.2%\n1.5%\n1.4%\n1.5%\n1.5%\n0.8%\n1.3%\n-\n-\n\n\nRefugee\nS\n2.8%\n2.1%\n3.2%\n2.8%\n2.7%\n3.2%\n2.8%\n-\n-\n\n\nSex working\nS\n1.8%\n2.2%\n2.1%\n1.3%\n2.1%\n1.5%\n1.9%\n-\n-\n\n\nWomen: sex working\nS\n6.5%\n8.3%\n8.9%\n5.6%\n9.2%\n6.4%\n7.7%\n-\n-\n\n\nMen: sex working\nS\n0.0%\n0.0%\n0.0%\n0.0%\n0.0%\n0.0%\n0.0%\n-\n-\n\n\nTotal (n)\nS\n3,755\n819\n829\n787\n741\n763\n788\n1,044\n\n\n\n\n\n\n\n\n\n\nDisabilities or conditions (ever)\n\n\nCode\nfrom collections import defaultdict\nref = {\n    \"Physical or sensory\": 15,\n    \"&lt;NA&gt; 1\": pd.NA,\n    \"&lt;NA&gt; 2\": pd.NA,\n    \"Mental illness\": 53,\n    \"Learning disability\": 8,\n    \"Other long-term health\": 22,\n    \"&lt;NA&gt; 3\": pd.NA,\n    \"&lt;NA&gt; 4\": pd.NA,\n    \"Substance use\": 50,\n    \"&lt;NA&gt; 5\": pd.NA,\n    \"Autistic spectrum disorder\": 3,\n    \"&lt;NA&gt; 6\": pd.NA,\n}\nref_n = 1044\nchr = dpw_snapshots[\"snapshot_dt\"].to_frame()\nchr[\"Physical health\"] = dpw_snapshots[\"hsn_physical_health\"]\nchr[\"Physical health concerns\"] = (\n    dpw_snapshots[\"risk_physical_health_yn\"].eq(\"Yes\").mask(dpw_snapshots[\"risk_physical_health_yn\"].isna()))\nchr[\"Mental health\"] = dpw_snapshots[\"hsn_mental_health\"]\nchr[\"Mental health concerns\"] = (\n    dpw_snapshots[\"risk_mental_health_yn\"].eq(\"Yes\").mask(dpw_snapshots[\"risk_mental_health_yn\"].isna()))\nchr[\"Learning disabilities\"] = dpw_snapshots[\"hsn_learning_disabilities\"]\nchr[\"&lt;NA&gt; 1\"] = -1\nchr[\"Drug use\"] = dpw_snapshots[\"hsn_drug_use\"]\nchr[\"Alcohol use\"] = dpw_snapshots[\"hsn_alcohol_use\"]\nchr[\"Drug use or Alcohol use\"] = dpw_snapshots[\"hsn_drug_use\"] | dpw_snapshots[\"hsn_alcohol_use\"]\nchr[\"Drug use\"] = dpw_snapshots[\"hsn_drug_use\"]\nchr[\"Alcohol use\"] = dpw_snapshots[\"hsn_alcohol_use\"]\nchr[\"Use/used drugs or alcohol\"] = (\n    dpw_snapshots[\"risk_drugs_alcohol_yn\"].eq(\"Yes\").mask(dpw_snapshots[\"risk_drugs_alcohol_yn\"].isna()))\nchr[\"&lt;NA&gt; 2\"] = -1\nchr[\"Considers self disabled\"] = (\n    dpw_snapshots[\"considers_self_disabled\"].eq(\"Yes\").mask(dpw_snapshots[\"considers_self_disabled\"].isna()))\nfor row in [\"Physical health\", \"Physical health concerns\", \"Mental health\", \"Mental health concerns\", \"Learning disabilities\"]:\n    chr[\"Considers self disabled: \" + row] = (\n        (chr[\"Considers self disabled\"] & chr[row])\n        .mask(chr[[\"Considers self disabled\", row]].isna().any(axis=1)))\nchr[\"Considers self disabled: physical health concerns\"] = chr[\"Considers self disabled\"] & chr[\"Physical health concerns\"]\nchr[\"R &lt;N/A&gt; (n)\"] = dpw_snapshots[\"risk_mental_health_yn\"].isna()\nchr[\"S &lt;N/A&gt; (n)\"] = dpw_snapshots[\"hsn_mental_health\"].isna()\nchr[\"Total (n)\"] = 1\nn = chr.groupby(\"snapshot_dt\", sort=False).sum().T\npct = chr.groupby(\"snapshot_dt\", sort=False).apply(lambda x: x.sum()/x.count()).T\nfor t in [n, pct]: t[\"C/S Mean\"] = t[t.columns[t.columns.str.startswith(\"20\")]].mean(axis=1)\npct = pct.mul(100).map(lambda p: f\"{p:.1f}%\")\npct.columns = pct.columns.astype(\"string\")\npct[pct.index.str.contains(\"(n)\", regex=False)] = n.loc[n.index.str.contains(\"(n)\")].map(lambda x: f\"{x:,.0f}\")\npct.insert(0, \"Src\", \"S\")\npct.loc[[\"Physical health concerns\", \"Mental health concerns\", \"Use/used drugs or alcohol\", \"Considers self disabled\"], \"Src\"] = \"R\"\npct[pct.index.str.startswith(\"&lt;NA&gt;\")] = \"-\"\nref_df = pd.DataFrame(\n    data=list((f\"{v}%\", k) for (k, v) in ref.items()) + [(\"&lt;NA&gt;\", \"&lt;NA&gt;\")] * 8 + [(f\"{ref_n:,}\", \"\")],\n    columns=[\"Ref_HSH\", \"Ref_HSH cat\"],\n    index=pct.index)\nref_df[ref_df[\"Ref_HSH cat\"].str.startswith(\"&lt;NA&gt;\")] = \"-\"\npct = pd.concat([pct, ref_df], axis=1)\nfor t in [n, pct]: c = t.pop(\"Longitudinal\"); t.insert(0, \"Longitudinal\", c)\ncharacteristics[\"Disabilities/conditions\"] = pct\ndisplay(pct)\n\n\n\n\n\n\n\n\n\nLongitudinal\nSrc\n2019-04-29\n2020-10-28\n2022-04-30\n2023-10-30\n2025-04-30\nC/S Mean\nRef_HSH\nRef_HSH cat\n\n\n\n\nPhysical health\n36.8%\nS\n37.6%\n39.9%\n38.5%\n40.8%\n38.7%\n39.1%\n15%\nPhysical or sensory\n\n\nPhysical health concerns\n62.8%\nR\n62.9%\n65.4%\n64.7%\n67.6%\n64.0%\n64.9%\n-\n-\n\n\nMental health\n70.5%\nS\n64.9%\n70.8%\n70.2%\n72.9%\n74.3%\n70.6%\n-\n-\n\n\nMental health concerns\n86.6%\nR\n83.4%\n87.5%\n87.0%\n88.3%\n88.6%\n87.0%\n53%\nMental illness\n\n\nLearning disabilities\n6.1%\nS\n5.2%\n4.1%\n5.1%\n7.0%\n7.1%\n5.7%\n8%\nLearning disability\n\n\n&lt;NA&gt; 1\n-\n-\n-\n-\n-\n-\n-\n-\n22%\nOther long-term health\n\n\nDrug use\n39.6%\nS\n34.8%\n43.0%\n44.0%\n42.9%\n40.5%\n41.0%\n-\n-\n\n\nAlcohol use\n23.7%\nS\n22.6%\n24.6%\n25.5%\n26.4%\n27.0%\n25.2%\n-\n-\n\n\nDrug use or Alcohol use\n42.7%\nS\n41.8%\n53.9%\n55.1%\n54.8%\n51.5%\n51.4%\n50%\nSubstance use\n\n\nUse/used drugs or alcohol\n72.3%\nR\n73.2%\n76.8%\n77.1%\n76.4%\n74.9%\n75.7%\n-\n-\n\n\n&lt;NA&gt; 2\n-\n-\n-\n-\n-\n-\n-\n-\n3%\nAutistic spectrum disorder\n\n\nConsiders self disabled\n35.2%\nR\n45.2%\n40.0%\n33.4%\n31.5%\n28.7%\n35.8%\n-\n-\n\n\nConsiders self disabled: Physical health\n26.2%\nS\n32.5%\n30.1%\n24.3%\n22.7%\n22.0%\n26.3%\n-\n-\n\n\nConsiders self disabled: Physical health concerns\n35.2%\nS\n45.2%\n40.0%\n33.4%\n31.5%\n28.7%\n35.8%\n-\n-\n\n\nConsiders self disabled: Mental health\n25.6%\nS\n30.1%\n27.6%\n24.5%\n22.3%\n21.4%\n25.2%\n-\n-\n\n\nConsiders self disabled: Mental health concerns\n31.1%\nS\n38.6%\n36.8%\n31.2%\n28.7%\n26.5%\n32.4%\n-\n-\n\n\nConsiders self disabled: Learning disabilities\n3.1%\nS\n4.2%\n2.5%\n2.8%\n2.3%\n3.1%\n3.0%\n-\n-\n\n\nConsiders self disabled: physical health concerns\n18.6%\nS\n25.2%\n25.7%\n21.5%\n21.2%\n18.3%\n22.4%\n-\n-\n\n\nR &lt;N/A&gt; (n)\n603\nS\n94\n15\n5\n4\n2\n24\n-\n-\n\n\nS &lt;N/A&gt; (n)\n661\nS\n101\n29\n25\n37\n47\n48\n-\n-\n\n\nTotal (n)\n3,755\nS\n819\n829\n787\n741\n763\n788\n1,044\n\n\n\n\n\n\n\n\n\n\nDisabilities or conditions (current)\n\n\nCode\nfrom collections import defaultdict\nref = {\n    \"Physical or sensory\": 15,\n    \"&lt;NA&gt; 1\": pd.NA,\n    \"&lt;NA&gt; 2\": pd.NA,\n    \"Mental illness\": 53,\n    \"Learning disability\": 8,\n    \"Other long-term health\": 22,\n    \"&lt;NA&gt; 3\": pd.NA,\n    \"&lt;NA&gt; 4\": pd.NA,\n    \"Substance use\": 50,\n    \"&lt;NA&gt; 5\": pd.NA,\n    \"Autistic spectrum disorder\": 3,\n    \"&lt;NA&gt; 6\": pd.NA,\n}\nref_n = 1044\nchr = dpw_snapshots[\"snapshot_dt\"].to_frame()\nchr[\"Physical health\"] = dpw_snapshots[\"hsn_physical_health\"]\nchr[\"Physical health concerns\"] = (\n    dpw_snapshots[\"risk_physical_health_current\"].eq(\"Current Risk\").mask(dpw_snapshots[\"risk_physical_health_yn\"].isna()))\nchr[\"Mental health\"] = dpw_snapshots[\"hsn_mental_health\"]\nchr[\"Mental health concerns\"] = (\n    dpw_snapshots[\"risk_mental_health_current\"].eq(\"Current Risk\").mask(dpw_snapshots[\"risk_mental_health_yn\"].isna()))\nchr[\"Learning disabilities\"] = dpw_snapshots[\"hsn_learning_disabilities\"]\nchr[\"&lt;NA&gt; 1\"] = -1\nchr[\"Drug use\"] = dpw_snapshots[\"hsn_drug_use\"]\nchr[\"Alcohol use\"] = dpw_snapshots[\"hsn_alcohol_use\"]\nchr[\"Drug use or Alcohol use\"] = dpw_snapshots[\"hsn_drug_use\"] | dpw_snapshots[\"hsn_alcohol_use\"]\nchr[\"Drug use\"] = dpw_snapshots[\"hsn_drug_use\"]\nchr[\"Alcohol use\"] = dpw_snapshots[\"hsn_alcohol_use\"]\nchr[\"Use/used drugs or alcohol\"] = (\n    dpw_snapshots[\"risk_drugs_alcohol_current\"].eq(\"Current Risk\").mask(dpw_snapshots[\"risk_drugs_alcohol_yn\"].isna()))\nchr[\"&lt;NA&gt; 2\"] = -1\nchr[\"Considers self disabled\"] = (\n    dpw_snapshots[\"considers_self_disabled\"].eq(\"Current Risk\").mask(dpw_snapshots[\"considers_self_disabled\"].isna()))\nfor row in [\"Physical health\", \"Physical health concerns\", \"Mental health\", \"Mental health concerns\", \"Learning disabilities\"]:\n    chr[\"Considers self disabled: \" + row] = (\n        (chr[\"Considers self disabled\"] & chr[row])\n        .mask(chr[[\"Considers self disabled\", row]].isna().any(axis=1)))\nchr[\"Considers self disabled: physical health concerns\"] = chr[\"Considers self disabled\"] & chr[\"Physical health concerns\"]\nchr[\"R &lt;N/A&gt; (n)\"] = dpw_snapshots[\"risk_mental_health_current\"].isna()\nchr[\"S &lt;N/A&gt; (n)\"] = dpw_snapshots[\"hsn_mental_health\"].isna()\nchr[\"Total (n)\"] = 1\nn = chr.groupby(\"snapshot_dt\", sort=False).sum().T\npct = chr.groupby(\"snapshot_dt\", sort=False).apply(lambda x: x.sum()/x.count()).T\nfor t in [n, pct]: t[\"C/S Mean\"] = t[t.columns[t.columns.str.startswith(\"20\")]].mean(axis=1)\npct = pct.mul(100).map(lambda p: f\"{p:.1f}%\")\npct.columns = pct.columns.astype(\"string\")\npct[pct.index.str.contains(\"(n)\", regex=False)] = n.loc[n.index.str.contains(\"(n)\")].map(lambda x: f\"{x:,.0f}\")\npct.insert(0, \"Src\", \"S\")\npct.loc[[\"Physical health concerns\", \"Mental health concerns\", \"Use/used drugs or alcohol\", \"Considers self disabled\"], \"Src\"] = \"R\"\npct[pct.index.str.startswith(\"&lt;NA&gt;\")] = \"-\"\nref_df = pd.DataFrame(\n    data=list((f\"{v}%\", k) for (k, v) in ref.items()) + [(\"&lt;NA&gt;\", \"&lt;NA&gt;\")] * 8 + [(f\"{ref_n:,}\", \"\")],\n    columns=[\"Ref_HSH\", \"Ref_HSH cat\"],\n    index=pct.index)\nref_df[ref_df[\"Ref_HSH cat\"].str.startswith(\"&lt;NA&gt;\")] = \"-\"\npct = pd.concat([pct, ref_df], axis=1)\nfor t in [n, pct]: c = t.pop(\"Longitudinal\"); t.insert(0, \"Longitudinal\", c)\ncharacteristics[\"Disabilities/conditions\"] = pct\ndisplay(pct)\n\n\n\n\n\n\n\n\n\nLongitudinal\nSrc\n2019-04-29\n2020-10-28\n2022-04-30\n2023-10-30\n2025-04-30\nC/S Mean\nRef_HSH\nRef_HSH cat\n\n\n\n\nPhysical health\n36.8%\nS\n37.6%\n39.9%\n38.5%\n40.8%\n38.7%\n39.1%\n15%\nPhysical or sensory\n\n\nPhysical health concerns\n60.5%\nR\n61.5%\n63.3%\n62.5%\n64.6%\n61.2%\n62.6%\n-\n-\n\n\nMental health\n70.5%\nS\n64.9%\n70.8%\n70.2%\n72.9%\n74.3%\n70.6%\n-\n-\n\n\nMental health concerns\n81.6%\nR\n78.1%\n82.1%\n81.2%\n81.7%\n82.8%\n81.2%\n53%\nMental illness\n\n\nLearning disabilities\n6.1%\nS\n5.2%\n4.1%\n5.1%\n7.0%\n7.1%\n5.7%\n8%\nLearning disability\n\n\n&lt;NA&gt; 1\n-\n-\n-\n-\n-\n-\n-\n-\n22%\nOther long-term health\n\n\nDrug use\n39.6%\nS\n34.8%\n43.0%\n44.0%\n42.9%\n40.5%\n41.0%\n-\n-\n\n\nAlcohol use\n23.7%\nS\n22.6%\n24.6%\n25.5%\n26.4%\n27.0%\n25.2%\n-\n-\n\n\nDrug use or Alcohol use\n42.7%\nS\n41.8%\n53.9%\n55.1%\n54.8%\n51.5%\n51.4%\n50%\nSubstance use\n\n\nUse/used drugs or alcohol\n55.6%\nR\n49.4%\n56.9%\n59.3%\n59.2%\n58.7%\n56.7%\n-\n-\n\n\n&lt;NA&gt; 2\n-\n-\n-\n-\n-\n-\n-\n-\n3%\nAutistic spectrum disorder\n\n\nConsiders self disabled\n0.0%\nR\n0.0%\n0.0%\n0.0%\n0.0%\n0.0%\n0.0%\n-\n-\n\n\nConsiders self disabled: Physical health\n0.0%\nS\n0.0%\n0.0%\n0.0%\n0.0%\n0.0%\n0.0%\n-\n-\n\n\nConsiders self disabled: Physical health concerns\n0.0%\nS\n0.0%\n0.0%\n0.0%\n0.0%\n0.0%\n0.0%\n-\n-\n\n\nConsiders self disabled: Mental health\n0.0%\nS\n0.0%\n0.0%\n0.0%\n0.0%\n0.0%\n0.0%\n-\n-\n\n\nConsiders self disabled: Mental health concerns\n0.0%\nS\n0.0%\n0.0%\n0.0%\n0.0%\n0.0%\n0.0%\n-\n-\n\n\nConsiders self disabled: Learning disabilities\n0.0%\nS\n0.0%\n0.0%\n0.0%\n0.0%\n0.0%\n0.0%\n-\n-\n\n\nConsiders self disabled: physical health concerns\n0.0%\nS\n0.0%\n0.0%\n0.0%\n0.0%\n0.0%\n0.0%\n-\n-\n\n\nR &lt;N/A&gt; (n)\n1,025\nS\n214\n117\n107\n90\n89\n123\n-\n-\n\n\nS &lt;N/A&gt; (n)\n661\nS\n101\n29\n25\n37\n47\n48\n-\n-\n\n\nTotal (n)\n3,755\nS\n819\n829\n787\n741\n763\n788\n1,044\n\n\n\n\n\n\n\n\n\n\nCurrent Severe and Multiple Disadvantage - based on risk assessment, where possible\n\n\nCode\nfrom collections import defaultdict\nref = {\n    \"SMD(3D)2-3 Current Bramley England\": 5.7,\n    \"SMD(3D)2-3 Current Bramley Bristol\": 11.2,\n    \"SMD(5D)2-5 Current BCC lo\": 10.5,\n    \"SMD(5D)2-5 Current BCC hi\": 11.1,\n    # \"SMD(5D)3-5 Ever Blood Homelessness\": 366,\n}\nref_n = 1044\nchr = dpw_snapshots[\"snapshot_dt\"].to_frame()\n\nrisk_rows = []\ncols_3d = []\ncols_5d = []\ncols_pd = []\n# Offending\nchr[\"Criminal conviction/investigation\"] = (\n    dpw_snapshots[\"risk_crime_current\"].eq(\"Current Risk\").mask(dpw_snapshots[\"risk_crime_yn\"].isna()))\nfor x in [risk_rows, cols_3d, cols_5d]: x.append(\"Criminal conviction/investigation\")\nchr[\"Prison leaver, probation or offending\"] = dpw_snapshots[\"hsn_prison_leaveron_probationrisk_of_offending\"]\n# Substance misuse\nchr[\"Use/used drugs or alcohol\"] = (\n    dpw_snapshots[\"risk_drugs_alcohol_current\"].eq(\"Current Risk\").mask(dpw_snapshots[\"risk_drugs_alcohol_yn\"].isna()))\nfor x in [risk_rows, cols_3d, cols_5d, cols_pd]: x.append(\"Use/used drugs or alcohol\")\nchr[\"Drug use or Alcohol use\"] = dpw_snapshots[\"hsn_drug_use\"] | dpw_snapshots[\"hsn_alcohol_use\"]\n# Homelessness\nchr[\"Homelessness\"] = True\nfor x in [cols_3d, cols_5d, cols_pd]: x.append(\"Homelessness\")\n# Domestic violence and abuse\nchr[\"Domestic abuse (victim)\"] = dpw_snapshots[\"hsn_risk_of_violenceabuse_victim\"]\nfor x in [cols_5d, cols_pd]: x.append(\"Domestic abuse (victim)\")\n# Mental health problems\nchr[\"Mental health concerns\"] = (\n    dpw_snapshots[\"risk_mental_health_current\"].eq(\"Current Risk\").mask(dpw_snapshots[\"risk_mental_health_yn\"].isna()))\nchr[\"Mental health\"] = dpw_snapshots[\"hsn_mental_health\"]\nfor x in [risk_rows, cols_5d, cols_pd]: x.append(\"Mental health concerns\")\nchr = chr.convert_dtypes()  # Use nullable booleans so that NA propogates in &gt;=\nchr[\"SMD(3D)2 O/~SU/H\"] = chr[\"Criminal conviction/investigation\"] * (~chr[\"Use/used drugs or alcohol\"]) * chr[\"Homelessness\"]\nchr[\"SMD(3D)2 ~O/SU/H\"] = (~chr[\"Criminal conviction/investigation\"]) * chr[\"Use/used drugs or alcohol\"] * chr[\"Homelessness\"]\nchr[\"SMD(3D)3 O/SU/H\"] = chr[\"Criminal conviction/investigation\"] * chr[\"Use/used drugs or alcohol\"] * chr[\"Homelessness\"]\nchr[\"SMD(3D)2-3\"] = (chr.loc[:, cols_3d].sum(axis=1, skipna=False) &gt;= 2)\nchr[\"SMD(5D)2-5\"] = (chr.loc[:, cols_5d].sum(axis=1, skipna=False) &gt;= 2)\nchr[\"SMD(5D)3-5\"] = (chr.loc[:, cols_5d].sum(axis=1, skipna=False) &gt;= 3)\nchr[\"PD2-4\"] = (chr.loc[:, cols_pd].sum(axis=1, skipna=False) &gt;= 2)\nchr[\"R &lt;N/A&gt; (n)\"] = dpw_snapshots[\"risk_mental_health_yn\"].isna()\nchr[\"S &lt;N/A&gt; (n)\"] = dpw_snapshots[\"hsn_mental_health\"].isna()\nchr[\"Total (n)\"] = 1\nn = chr.groupby(\"snapshot_dt\", sort=False).sum().T\npct = chr.groupby(\"snapshot_dt\", sort=False).apply(lambda x: x.sum()/x.count()).T\nfor t in [n, pct]: t[\"C/S Mean\"] = t[t.columns[t.columns.str.startswith(\"20\")]].mean(axis=1)\npct = pct.mul(100).map(lambda p: f\"{p:.1f}%\")\npct.columns = pct.columns.astype(\"string\")\npct[pct.index.str.contains(\"(n)\", regex=False)] = n.loc[n.index.str.contains(\"(n)\")].map(lambda x: f\"{x:,.0f}\")\npct.insert(0, \"PD\", \" \")\npct.insert(0, \"5D\", \" \")\npct.insert(0, \"3D\", \" \")\npct.insert(0, \"Src\", \"S\")\npct.loc[risk_rows, \"Src\"] = \"R\"\npct.loc[cols_3d, \"3D\"] = \"*\"\npct.loc[cols_5d, \"5D\"] = \"*\"\npct.loc[cols_pd, \"PD\"] = \"*\"\npct.loc[\"Homelessness\", \"Src\"] = \"1\"\npct.loc[risk_rows, \"Src\"] = \"R\"\npct.loc[\"Homelessness\", \"Src\"] = \"1\"\nfor t in [n, pct]: c = t.pop(\"Longitudinal\"); t.insert(0, \"Longitudinal\", c)\ncharacteristics[\"Current SMD\"] = pct\ndisplay(pct)\n\n\n\n\n\n\n\n\nsnapshot_dt\nLongitudinal\nSrc\n3D\n5D\nPD\n2019-04-29\n2020-10-28\n2022-04-30\n2023-10-30\n2025-04-30\nC/S Mean\n\n\n\n\nCriminal conviction/investigation\n23.4%\nR\n*\n*\n\n15.2%\n20.0%\n19.8%\n18.5%\n17.3%\n18.2%\n\n\nPrison leaver, probation or offending\n17.0%\nS\n\n\n\n10.4%\n13.8%\n14.8%\n13.1%\n12.8%\n13.0%\n\n\nUse/used drugs or alcohol\n55.6%\nR\n*\n*\n*\n49.4%\n56.9%\n59.3%\n59.2%\n58.7%\n56.7%\n\n\nDrug use or Alcohol use\n42.7%\nS\n\n\n\n41.8%\n53.9%\n55.1%\n54.8%\n51.5%\n51.4%\n\n\nHomelessness\n100.0%\n1\n*\n*\n*\n100.0%\n100.0%\n100.0%\n100.0%\n100.0%\n100.0%\n\n\nDomestic abuse (victim)\n17.7%\nS\n\n*\n*\n13.1%\n14.1%\n15.0%\n17.2%\n17.3%\n15.3%\n\n\nMental health concerns\n81.6%\nR\n\n*\n*\n78.1%\n82.1%\n81.2%\n81.7%\n82.8%\n81.2%\n\n\nMental health\n70.5%\nS\n\n\n\n64.9%\n70.8%\n70.2%\n72.9%\n74.3%\n70.6%\n\n\nSMD(3D)2 O/~SU/H\n5.3%\nS\n\n\n\n3.2%\n3.6%\n3.6%\n3.5%\n4.7%\n3.7%\n\n\nSMD(3D)2 ~O/SU/H\n37.5%\nS\n\n\n\n37.4%\n40.4%\n43.1%\n44.2%\n46.1%\n42.2%\n\n\nSMD(3D)3 O/SU/H\n18.1%\nS\n\n\n\n12.0%\n16.5%\n16.2%\n14.9%\n12.6%\n14.4%\n\n\nSMD(3D)2-3\n60.9%\nS\n\n\n\n52.6%\n60.4%\n62.9%\n62.7%\n63.5%\n60.4%\n\n\nSMD(5D)2-5\n89.4%\nS\n\n\n\n85.4%\n89.7%\n90.0%\n89.8%\n91.6%\n89.3%\n\n\nSMD(5D)3-5\n61.2%\nS\n\n\n\n49.9%\n58.4%\n60.8%\n62.1%\n63.7%\n59.0%\n\n\nPD2-4\n88.5%\nS\n\n\n\n85.0%\n88.8%\n89.4%\n89.1%\n90.5%\n88.6%\n\n\nR &lt;N/A&gt; (n)\n603\nS\n\n\n\n94\n15\n5\n4\n2\n24\n\n\nS &lt;N/A&gt; (n)\n661\nS\n\n\n\n101\n29\n25\n37\n47\n48\n\n\nTotal (n)\n3,755\nS\n\n\n\n819\n829\n787\n741\n763\n788\n\n\n\n\n\n\n\n\n\nSevere and Multiple Disadvantage (Ever) - based on risk assessment, where possible\n\n\nCode\nfrom collections import defaultdict\nref = {\n    # \"SMD(3D)2-3 Current Bramley England\": 5.7,\n    # \"SMD(3D)2-3 Current Bramley Bristol\": 11.2,\n    # \"SMD(5D)2-5 Current BCC lo\": 10.5,\n    # \"SMD(5D)2-5 Current BCC hi\": 11.1,\n    \"SMD(5D)3-5 Ever Blood Homelessness\": 366,\n}\nref_n = 1044\nchr = dpw_snapshots[\"snapshot_dt\"].to_frame()\n\nrisk_rows = []\ncols_3d = []\ncols_5d = []\ncols_pd = []\n# Offending\nchr[\"Criminal conviction/investigation\"] = (\n    dpw_snapshots[\"risk_crime_yn\"].eq(\"Yes\").mask(dpw_snapshots[\"risk_crime_yn\"].isna()))\nfor x in [risk_rows, cols_3d, cols_5d]: x.append(\"Criminal conviction/investigation\")\n# Substance misuse\nchr[\"Use/used drugs or alcohol\"] = (\n    dpw_snapshots[\"risk_drugs_alcohol_yn\"].eq(\"Yes\").mask(dpw_snapshots[\"risk_drugs_alcohol_yn\"].isna()))\nfor x in [risk_rows, cols_3d, cols_5d, cols_pd]: x.append(\"Use/used drugs or alcohol\")\n# Homelessness\nchr[\"Homelessness\"] = True\nfor x in [cols_3d, cols_5d, cols_pd]: x.append(\"Homelessness\")\n# Domestic violence and abuse\nchr[\"Domestic abuse (victim)\"] = dpw_snapshots[\"hsn_risk_of_violenceabuse_victim\"]\nfor x in [cols_5d, cols_pd]: x.append(\"Domestic abuse (victim)\")\n# Mental health problems\nchr[\"Mental health concerns\"] = (\n    dpw_snapshots[\"risk_mental_health_yn\"].eq(\"Yes\").mask(dpw_snapshots[\"risk_mental_health_yn\"].isna()))\nfor x in [risk_rows, cols_5d, cols_pd]: x.append(\"Mental health concerns\")\nchr = chr.convert_dtypes()  # Use nullable booleans so that NA propogates in &gt;=\nchr[\"SMD(3D)2 O/~SU/H\"] = chr[\"Criminal conviction/investigation\"] * (~chr[\"Use/used drugs or alcohol\"]) * chr[\"Homelessness\"]\nchr[\"SMD(3D)2 ~O/SU/H\"] = (~chr[\"Criminal conviction/investigation\"]) * chr[\"Use/used drugs or alcohol\"] * chr[\"Homelessness\"]\nchr[\"SMD(3D)3 O/SU/H\"] = chr[\"Criminal conviction/investigation\"] * chr[\"Use/used drugs or alcohol\"] * chr[\"Homelessness\"]\nchr[\"SMD(3D)2-3\"] = (chr.loc[:, cols_3d].sum(axis=1, skipna=False) &gt;= 2)\nchr[\"SMD(5D)2-5\"] = (chr.loc[:, cols_5d].sum(axis=1, skipna=False) &gt;= 2)\nchr[\"SMD(5D)3-5\"] = (chr.loc[:, cols_5d].sum(axis=1, skipna=False) &gt;= 3)\nchr[\"PD2-4\"] = (chr.loc[:, cols_pd].sum(axis=1, skipna=False) &gt;= 2)\nchr[\"R &lt;N/A&gt; (n)\"] = dpw_snapshots[\"risk_mental_health_yn\"].isna()\nchr[\"S &lt;N/A&gt; (n)\"] = dpw_snapshots[\"hsn_managing_a_tenancyliving_independently\"].isna()\nchr[\"Total (n)\"] = 1\nn = chr.groupby(\"snapshot_dt\", sort=False).sum().T\npct = chr.groupby(\"snapshot_dt\", sort=False).apply(lambda x: x.sum()/x.count()).T\nfor t in [n, pct]: t[\"C/S Mean\"] = t[t.columns[t.columns.str.startswith(\"20\")]].mean(axis=1)\npct = pct.mul(100).map(lambda p: f\"{p:.1f}%\")\npct.columns = pct.columns.astype(\"string\")\npct[pct.index.str.contains(\"(n)\", regex=False)] = n.loc[n.index.str.contains(\"(n)\", regex=False)].map(lambda x: f\"{x:,.0f}\")\npct.insert(0, \"PD\", \" \")\npct.insert(0, \"5D\", \" \")\npct.insert(0, \"3D\", \" \")\npct.insert(0, \"Src\", \"S\")\npct.loc[risk_rows, \"Src\"] = \"R\"\npct.loc[cols_3d, \"3D\"] = \"*\"\npct.loc[cols_5d, \"5D\"] = \"*\"\npct.loc[cols_pd, \"PD\"] = \"*\"\npct.loc[\"Homelessness\", \"Src\"] = \"1\"\npct[\"Ref_HSH\"] = \"-\"\npct.loc[\"SMD(5D)3-5\", \"Ref_HSH\"] = \"36.6%\"\npct.loc[\"Total (n)\", \"Ref_HSH\"] = \"1,044\"\nfor t in [n, pct]: c = t.pop(\"Longitudinal\"); t.insert(0, \"Longitudinal\", c)\ncharacteristics[\"SMD (ever)\"] = pct\ndisplay(pct)\n\n\n\n\n\n\n\n\nsnapshot_dt\nLongitudinal\nSrc\n3D\n5D\nPD\n2019-04-29\n2020-10-28\n2022-04-30\n2023-10-30\n2025-04-30\nC/S Mean\nRef_HSH\n\n\n\n\nCriminal conviction/investigation\n53.7%\nR\n*\n*\n\n49.9%\n53.4%\n53.1%\n51.4%\n48.6%\n51.3%\n-\n\n\nUse/used drugs or alcohol\n72.3%\nR\n*\n*\n*\n73.2%\n76.8%\n77.1%\n76.4%\n74.9%\n75.7%\n-\n\n\nHomelessness\n100.0%\n1\n*\n*\n*\n100.0%\n100.0%\n100.0%\n100.0%\n100.0%\n100.0%\n-\n\n\nDomestic abuse (victim)\n17.7%\nS\n\n*\n*\n13.1%\n14.1%\n15.0%\n17.2%\n17.3%\n15.3%\n-\n\n\nMental health concerns\n86.6%\nR\n\n*\n*\n83.4%\n87.5%\n87.0%\n88.3%\n88.6%\n87.0%\n-\n\n\nSMD(3D)2 O/~SU/H\n6.5%\nS\n\n\n\n5.9%\n4.5%\n5.6%\n6.2%\n5.7%\n5.6%\n-\n\n\nSMD(3D)2 ~O/SU/H\n25.1%\nS\n\n\n\n29.2%\n27.9%\n29.7%\n31.2%\n31.9%\n30.0%\n-\n\n\nSMD(3D)3 O/SU/H\n47.2%\nS\n\n\n\n44.0%\n48.9%\n47.4%\n45.2%\n43.0%\n45.7%\n-\n\n\nSMD(3D)2-3\n78.8%\nS\n\n\n\n79.2%\n81.3%\n82.7%\n82.6%\n80.6%\n81.3%\n-\n\n\nSMD(5D)2-5\n94.9%\nS\n\n\n\n93.7%\n95.5%\n95.9%\n96.2%\n96.9%\n95.6%\n-\n\n\nSMD(5D)3-5\n78.3%\nS\n\n\n\n74.8%\n78.8%\n81.1%\n81.8%\n81.8%\n79.7%\n36.6%\n\n\nPD2-4\n93.7%\nS\n\n\n\n92.6%\n94.4%\n94.5%\n95.2%\n95.7%\n94.5%\n-\n\n\nR &lt;N/A&gt; (n)\n603\nS\n\n\n\n94\n15\n5\n4\n2\n24\n-\n\n\nS &lt;N/A&gt; (n)\n661\nS\n\n\n\n101\n29\n25\n37\n47\n48\n-\n\n\nTotal (n)\n3,755\nS\n\n\n\n819\n829\n787\n741\n763\n788\n1,044\n\n\n\n\n\n\n\n\n\nCurrent Severe and Multiple Disadvantage - based on current support needs\n\n\nCode\nfrom collections import defaultdict\nref = {\n    \"SMD(3D)2-3 Current Bramley England\": 5.7,\n    \"SMD(3D)2-3 Current Bramley Bristol\": 11.2,\n    \"SMD(5D)2-5 Current BCC lo\": 10.5,\n    \"SMD(5D)2-5 Current BCC hi\": 11.1,\n    # \"SMD(5D)3-5 Ever Blood Homelessness\": 366,\n}\nref_n = 1044\nchr = dpw_snapshots[\"snapshot_dt\"].to_frame()\n\nrisk_rows = []\ncols_3d = []\ncols_5d = []\ncols_pd = []\n# Offending\nchr[\"Criminal conviction/investigation\"] = (\n    dpw_snapshots[\"risk_crime_current\"].eq(\"Current Risk\").mask(dpw_snapshots[\"risk_crime_yn\"].isna()))\nfor x in [risk_rows]: x.append(\"Criminal conviction/investigation\")\nchr[\"Prison leaver, probation or offending\"] = dpw_snapshots[\"hsn_prison_leaveron_probationrisk_of_offending\"]\nfor x in [cols_3d, cols_5d]: x.append(\"Prison leaver, probation or offending\")\n# Substance misuse\nchr[\"Use/used drugs or alcohol\"] = (\n    dpw_snapshots[\"risk_drugs_alcohol_current\"].eq(\"Current Risk\").mask(dpw_snapshots[\"risk_drugs_alcohol_yn\"].isna()))\nfor x in [risk_rows]: x.append(\"Use/used drugs or alcohol\")\nchr[\"Drug use or Alcohol use\"] = dpw_snapshots[\"hsn_drug_use\"] | dpw_snapshots[\"hsn_alcohol_use\"]\nfor x in [cols_3d, cols_5d, cols_pd]: x.append(\"Drug use or Alcohol use\")\n# Homelessness\nchr[\"Homelessness\"] = True\nfor x in [cols_3d, cols_5d, cols_pd]: x.append(\"Homelessness\")\n# Domestic violence and abuse\nchr[\"Domestic abuse (victim)\"] = dpw_snapshots[\"hsn_risk_of_violenceabuse_victim\"]\nfor x in [cols_5d, cols_pd]: x.append(\"Domestic abuse (victim)\")\n# Mental health problems\nchr[\"Mental health concerns\"] = (\n    dpw_snapshots[\"risk_mental_health_current\"].eq(\"Current Risk\").mask(dpw_snapshots[\"risk_mental_health_yn\"].isna()))\nfor x in [risk_rows]: x.append(\"Mental health concerns\")\nchr[\"Mental health\"] = dpw_snapshots[\"hsn_mental_health\"]\nfor x in [cols_5d, cols_pd]: x.append(\"Mental health\")\nchr = chr.convert_dtypes()  # Use nullable booleans so that NA propogates in &gt;=\nchr[\"SMD(3D)2 O/~SU/H\"] = chr[\"Prison leaver, probation or offending\"] * (~chr[\"Drug use or Alcohol use\"]) * chr[\"Homelessness\"]\nchr[\"SMD(3D)2 ~O/SU/H\"] = (~chr[\"Prison leaver, probation or offending\"]) * chr[\"Drug use or Alcohol use\"] * chr[\"Homelessness\"]\nchr[\"SMD(3D)3 O/SU/H\"] = chr[\"Prison leaver, probation or offending\"] * chr[\"Drug use or Alcohol use\"] * chr[\"Homelessness\"]\nchr[\"SMD(3D)2-3\"] = (chr.loc[:, cols_3d].sum(axis=1, skipna=False) &gt;= 2)\nchr[\"SMD(5D)2-5\"] = (chr.loc[:, cols_5d].sum(axis=1, skipna=False) &gt;= 2)\nchr[\"SMD(5D)3-5\"] = (chr.loc[:, cols_5d].sum(axis=1, skipna=False) &gt;= 3)\nchr[\"PD2-4\"] = (chr.loc[:, cols_pd].sum(axis=1, skipna=False) &gt;= 2)\nchr[\"R &lt;N/A&gt; (n)\"] = dpw_snapshots[\"risk_mental_health_yn\"].isna()\nchr[\"S &lt;N/A&gt; (n)\"] = dpw_snapshots[\"hsn_mental_health\"].isna()\nchr[\"Total (n)\"] = 1\nn = chr.groupby(\"snapshot_dt\", sort=False).sum().T\npct = chr.groupby(\"snapshot_dt\", sort=False).apply(lambda x: x.sum()/x.count()).T\nfor t in [n, pct]: t[\"C/S Mean\"] = t[t.columns[t.columns.str.startswith(\"20\")]].mean(axis=1)\npct = pct.mul(100).map(lambda p: f\"{p:.1f}%\")\npct.columns = pct.columns.astype(\"string\")\npct[pct.index.str.contains(\"(n)\", regex=False)] = n.loc[n.index.str.contains(\"(n)\")].map(lambda x: f\"{x:,.0f}\")\npct.insert(0, \"PD\", \" \")\npct.insert(0, \"5D\", \" \")\npct.insert(0, \"3D\", \" \")\npct.insert(0, \"Src\", \"S\")\npct.loc[risk_rows, \"Src\"] = \"R\"\npct.loc[cols_3d, \"3D\"] = \"*\"\npct.loc[cols_5d, \"5D\"] = \"*\"\npct.loc[cols_pd, \"PD\"] = \"*\"\npct.loc[\"Homelessness\", \"Src\"] = \"1\"\npct.loc[risk_rows, \"Src\"] = \"R\"\npct.loc[\"Homelessness\", \"Src\"] = \"1\"\nfor t in [n, pct]: c = t.pop(\"Longitudinal\"); t.insert(0, \"Longitudinal\", c)\ncharacteristics[\"Current SMD\"] = pct\ndisplay(pct)\n\n\n\n\n\n\n\n\nsnapshot_dt\nLongitudinal\nSrc\n3D\n5D\nPD\n2019-04-29\n2020-10-28\n2022-04-30\n2023-10-30\n2025-04-30\nC/S Mean\n\n\n\n\nCriminal conviction/investigation\n23.4%\nR\n\n\n\n15.2%\n20.0%\n19.8%\n18.5%\n17.3%\n18.2%\n\n\nPrison leaver, probation or offending\n17.0%\nS\n*\n*\n\n10.4%\n13.8%\n14.8%\n13.1%\n12.8%\n13.0%\n\n\nUse/used drugs or alcohol\n55.6%\nR\n\n\n\n49.4%\n56.9%\n59.3%\n59.2%\n58.7%\n56.7%\n\n\nDrug use or Alcohol use\n42.7%\nS\n*\n*\n*\n41.8%\n53.9%\n55.1%\n54.8%\n51.5%\n51.4%\n\n\nHomelessness\n100.0%\n1\n*\n*\n*\n100.0%\n100.0%\n100.0%\n100.0%\n100.0%\n100.0%\n\n\nDomestic abuse (victim)\n17.7%\nS\n\n*\n*\n13.1%\n14.1%\n15.0%\n17.2%\n17.3%\n15.3%\n\n\nMental health concerns\n81.6%\nR\n\n\n\n78.1%\n82.1%\n81.2%\n81.7%\n82.8%\n81.2%\n\n\nMental health\n70.5%\nS\n\n*\n*\n64.9%\n70.8%\n70.2%\n72.9%\n74.3%\n70.6%\n\n\nSMD(3D)2 O/~SU/H\n5.4%\nS\n\n\n\n3.5%\n4.5%\n5.1%\n3.7%\n4.7%\n4.3%\n\n\nSMD(3D)2 ~O/SU/H\n40.3%\nS\n\n\n\n40.7%\n46.6%\n47.2%\n48.3%\n46.8%\n45.9%\n\n\nSMD(3D)3 O/SU/H\n11.6%\nS\n\n\n\n7.0%\n9.2%\n9.7%\n9.4%\n8.1%\n8.7%\n\n\nSMD(3D)2-3\n57.3%\nS\n\n\n\n51.1%\n60.4%\n62.1%\n61.4%\n59.6%\n58.9%\n\n\nSMD(5D)2-5\n86.6%\nS\n\n\n\n80.8%\n86.2%\n86.4%\n88.5%\n90.4%\n86.4%\n\n\nSMD(5D)3-5\n52.9%\nS\n\n\n\n43.0%\n52.9%\n54.1%\n54.3%\n54.7%\n51.8%\n\n\nPD2-4\n84.8%\nS\n\n\n\n79.8%\n84.5%\n84.4%\n86.9%\n89.0%\n84.9%\n\n\nR &lt;N/A&gt; (n)\n603\nS\n\n\n\n94\n15\n5\n4\n2\n24\n\n\nS &lt;N/A&gt; (n)\n661\nS\n\n\n\n101\n29\n25\n37\n47\n48\n\n\nTotal (n)\n3,755\nS\n\n\n\n819\n829\n787\n741\n763\n788\n\n\n\n\n\n\n\n\n\nReligion/beliefs\n\n\nCode\nimport numpy as np\nchr = (\n    dpw_snapshots[[\"snapshot_dt\", \"religionbeliefs\"]]\n    .replace([\"Does not wish to disclose\", \"Not known\", \"Rather not state\", \"Don't Know\"], \"Unknown\")\n    .replace(\"Any other religion\", \"Other religion\")\n)\nchr.religionbeliefs = chr.religionbeliefs.cat.reorder_categories(sorted(chr.religionbeliefs.cat.categories))\nn = chr.groupby(\"snapshot_dt\", sort=False).value_counts(sort=False, dropna=False).unstack(0)\npct = chr.groupby(\"snapshot_dt\", sort=False).value_counts(sort=False, normalize=True).unstack(0)\nfor t in [n, pct]: t[\"C/S Mean\"] = t[t.columns[t.columns.str.startswith(\"20\")]].mean(axis=1)\npct = pct.mul(100).map(lambda p: f\"{p:.1f}%\")\npct.columns = pct.columns.astype(\"string\")\npct.loc[\"&lt;NA&gt; (n)\", :] = n.loc[n.index.isna()].iloc[0, :].map(lambda x: f\"{x:,.0f}\")\npct.loc[\"Total (n)\", :] = n.sum().map(lambda x: f\"{x:,.0f}\")\nfor t in [n, pct]: c = t.pop(\"Longitudinal\"); t.insert(0, \"Longitudinal\", c)\ncharacteristics[\"Religions/beliefs\"] = pct\ndisplay(pct)\n\n\n\n\n\n\n\n\nsnapshot_dt\nLongitudinal\n2019-04-29\n2020-10-28\n2022-04-30\n2023-10-30\n2025-04-30\nC/S Mean\n\n\nreligionbeliefs\n\n\n\n\n\n\n\n\n\n\n\nBuddhist\n0.7%\n1.1%\n0.4%\n1.0%\n0.9%\n0.8%\n0.8%\n\n\nChristian\n19.3%\n21.2%\n23.9%\n21.6%\n19.0%\n20.0%\n21.2%\n\n\nHindu\n0.1%\n0.2%\n0.2%\n0.2%\n0.0%\n0.2%\n0.2%\n\n\nJewish\n0.0%\n0.0%\n0.0%\n0.0%\n0.0%\n0.0%\n0.0%\n\n\nMuslim\n11.6%\n9.0%\n12.6%\n12.0%\n12.0%\n11.4%\n11.4%\n\n\nNo religion\n7.9%\n11.8%\n7.3%\n6.5%\n9.4%\n8.4%\n8.7%\n\n\nOther religion\n3.5%\n2.4%\n3.1%\n3.1%\n4.1%\n4.2%\n3.4%\n\n\nSikh\n0.2%\n0.4%\n0.6%\n0.0%\n0.2%\n0.0%\n0.2%\n\n\nUnknown\n56.6%\n53.8%\n51.9%\n55.7%\n54.4%\n54.9%\n54.1%\n\n\n&lt;NA&gt; (n)\n1,310\n287\n307\n277\n283\n264\n284\n\n\nTotal (n)\n3,755\n819\n829\n787\n741\n763\n788\n\n\n\n\n\n\n\n\n\nSexual orientation\n\n\nCode\nimport numpy as np\nchr = (\n    dpw_snapshots[[\"snapshot_dt\", \"sexual_orientation\"]]\n    .replace({\"Lesbian\": \"Gay/Lesbian\", \"Don't Know\": \"Unknown\", \"Rather not state\": \"Unknown\", \"other\": \"Other\", \"Bi-sexual\": \"Bisexual\"})\n)\ncat_order = [\"Heterosexual\", \"Bisexual\", \"Gay/Lesbian\", \"Other\", \"Unknown\"]\nchr.sexual_orientation = chr.sexual_orientation.cat.reorder_categories(cat_order)\nn = chr.groupby(\"snapshot_dt\", sort=False).value_counts(sort=False, dropna=False).unstack(0)\npct = chr.groupby(\"snapshot_dt\", sort=False).value_counts(sort=False, normalize=True).unstack(0)\nfor t in [n, pct]: t[\"C/S Mean\"] = t[t.columns[t.columns.str.startswith(\"20\")]].mean(axis=1)\npct = pct.mul(100).map(lambda p: f\"{p:.1f}%\")\npct.columns = pct.columns.astype(\"string\")\npct.loc[\"&lt;NA&gt; (n)\", :] = n.loc[n.index.isna()].iloc[0, :].map(lambda x: f\"{x:,.0f}\")\npct.loc[\"Total (n)\", :] = n.sum().map(lambda x: f\"{x:,.0f}\")\nfor t in [n, pct]: c = t.pop(\"Longitudinal\"); t.insert(0, \"Longitudinal\", c)\ncharacteristics[\"Sexual orientations\"] = pct\ndisplay(pct)\n\n\n\n\n\n\n\n\nsnapshot_dt\nLongitudinal\n2019-04-29\n2020-10-28\n2022-04-30\n2023-10-30\n2025-04-30\nC/S Mean\n\n\nsexual_orientation\n\n\n\n\n\n\n\n\n\n\n\nHeterosexual\n71.2%\n75.9%\n76.5%\n72.0%\n74.1%\n67.4%\n73.2%\n\n\nBisexual\n2.4%\n2.0%\n1.9%\n1.8%\n2.8%\n3.1%\n2.3%\n\n\nGay/Lesbian\n2.2%\n1.8%\n1.4%\n2.5%\n1.8%\n1.8%\n1.9%\n\n\nOther\n0.8%\n0.9%\n0.6%\n1.3%\n0.8%\n0.4%\n0.8%\n\n\nUnknown\n23.4%\n19.4%\n19.5%\n22.4%\n20.5%\n27.3%\n21.8%\n\n\n&lt;NA&gt; (n)\n1\n0\n0\n0\n1\n0\n0\n\n\nTotal (n)\n3,755\n819\n829\n787\n741\n763\n788\n\n\n\n\n\n\n\n\n\nNationality\n\n\nCode\nimport numpy as np\nchr = (\n    dpw_snapshots[[\"snapshot_dt\", \"nationality\"]]\n)\nn = chr.groupby(\"snapshot_dt\", sort=False).value_counts(sort=False, dropna=False).unstack(0)\npct = chr.groupby(\"snapshot_dt\", sort=False).value_counts(sort=False, normalize=True).unstack(0)\nfor t in [n, pct]: t[\"C/S Mean\"] = t[t.columns[t.columns.str.startswith(\"20\")]].mean(axis=1)\nrow_order = [i for i in pct[\"C/S Mean\"].sort_values(ascending=False).index if i not in [\"Other country\", \"Unknown\"]] + [\"Other country\", \"Unknown\"]\npct = pct.reindex(row_order)\npct = pct.mul(100).map(lambda p: f\"{p:.1f}%\")\npct.columns = pct.columns.astype(\"string\")\npct.loc[\"&lt;NA&gt; (n)\", :] = n.loc[n.index.isna()].iloc[0, :].map(lambda x: f\"{x:,.0f}\")\npct.loc[\"Total (n)\", :] = n.sum().map(lambda x: f\"{x:,.0f}\")\nfor t in [n, pct]: c = t.pop(\"Longitudinal\"); t.insert(0, \"Longitudinal\", c)\ncharacteristics[\"Nationality\"] = pct\ndisplay(pct)\n\n\n\n\n\n\n\n\nsnapshot_dt\nLongitudinal\n2019-04-29\n2020-10-28\n2022-04-30\n2023-10-30\n2025-04-30\nC/S Mean\n\n\nnationality\n\n\n\n\n\n\n\n\n\n\n\nUnited Kingdom\n81.5%\n91.7%\n84.0%\n81.7%\n82.2%\n77.8%\n83.5%\n\n\nPoland\n2.7%\n2.3%\n2.6%\n4.2%\n3.1%\n3.5%\n3.1%\n\n\nSudan\n2.0%\n1.5%\n2.0%\n2.1%\n2.3%\n2.3%\n2.0%\n\n\nSomalia\n2.0%\n0.8%\n1.3%\n2.5%\n3.1%\n1.9%\n1.9%\n\n\nIran\n0.6%\n0.0%\n1.0%\n0.2%\n0.4%\n1.2%\n0.6%\n\n\nRepublic of Ireland\n0.1%\n0.8%\n0.7%\n0.2%\n0.2%\n0.4%\n0.4%\n\n\nRomania\n0.7%\n0.0%\n0.0%\n0.4%\n0.4%\n1.2%\n0.4%\n\n\nEritrea\n0.5%\n0.0%\n0.3%\n0.6%\n0.6%\n0.4%\n0.4%\n\n\nPortugal\n0.5%\n0.0%\n0.3%\n0.6%\n0.4%\n0.4%\n0.4%\n\n\nHungary\n0.1%\n0.0%\n0.3%\n0.2%\n0.2%\n0.4%\n0.2%\n\n\nLithuania\n0.4%\n0.0%\n0.0%\n0.2%\n0.2%\n0.6%\n0.2%\n\n\nSpain\n0.1%\n0.0%\n0.0%\n0.2%\n0.4%\n0.2%\n0.2%\n\n\nSlovakia\n0.1%\n0.0%\n0.0%\n0.2%\n0.2%\n0.2%\n0.1%\n\n\nItaly\n0.1%\n0.0%\n0.3%\n0.0%\n0.0%\n0.2%\n0.1%\n\n\nLatvia\n0.2%\n0.0%\n0.0%\n0.0%\n0.2%\n0.2%\n0.1%\n\n\nNetherlands\n0.2%\n0.0%\n0.0%\n0.0%\n0.0%\n0.4%\n0.1%\n\n\nBulgaria\n0.2%\n0.0%\n0.0%\n0.2%\n0.0%\n0.0%\n0.0%\n\n\nCzech Republic\n0.1%\n0.0%\n0.0%\n0.0%\n0.0%\n0.0%\n0.0%\n\n\nOther country\n6.3%\n3.0%\n5.5%\n4.8%\n5.0%\n7.2%\n5.1%\n\n\nUnknown\n1.6%\n0.0%\n1.6%\n1.7%\n1.0%\n1.7%\n1.2%\n\n\n&lt;NA&gt; (n)\n2,094\n686\n522\n307\n258\n246\n404\n\n\nTotal (n)\n3,755\n819\n829\n787\n741\n763\n788\n\n\n\n\n\n\n\n\n\nUK immigration status\n\n\nCode\nimport numpy as np\nchr = (\n    dpw_snapshots[[\"snapshot_dt\", \"uk_immigration_status\"]]\n)\nn = chr.groupby(\"snapshot_dt\", sort=False).value_counts(sort=False, dropna=False).unstack(0)\npct = chr.groupby(\"snapshot_dt\", sort=False).value_counts(sort=False, normalize=True).unstack(0)\nfor t in [n, pct]: t[\"C/S Mean\"] = t[t.columns[t.columns.str.startswith(\"20\")]].mean(axis=1)\nrow_order = pct[\"C/S Mean\"].sort_values(ascending=False).index\npct = pct.reindex(row_order)\npct = pct.mul(100).map(lambda p: f\"{p:.1f}%\")\npct.columns = pct.columns.astype(\"string\")\nif n.index.isna().sum() != 0: pct.loc[\"&lt;NA&gt; (n)\", :] = n.loc[n.index.isna()].iloc[0, :].map(lambda x: f\"{x:,.0f}\")\npct.loc[\"Total (n)\", :] = n.sum().map(lambda x: f\"{x:,.0f}\")\nfor t in [n, pct]: c = t.pop(\"Longitudinal\"); t.insert(0, \"Longitudinal\", c)\ncharacteristics[\"English 2nd Language\"] = pct\ndisplay(pct)\n\n\n\n\n\n\n\n\nsnapshot_dt\nLongitudinal\n2019-04-29\n2020-10-28\n2022-04-30\n2023-10-30\n2025-04-30\nC/S Mean\n\n\nuk_immigration_status\n\n\n\n\n\n\n\n\n\n\n\nUK/Irish National\n86.3%\n88.0%\n86.1%\n85.1%\n86.1%\n82.6%\n85.6%\n\n\nIndefinite Leave to Remain\n6.3%\n5.6%\n6.0%\n7.2%\n6.9%\n8.6%\n6.9%\n\n\nEEA National\n3.5%\n3.2%\n3.4%\n4.2%\n3.3%\n4.0%\n3.6%\n\n\nLimited Leave to Remain\n2.7%\n2.2%\n3.4%\n2.6%\n2.0%\n2.8%\n2.6%\n\n\nOther Protection Leave to Remain\n0.6%\n0.6%\n0.9%\n0.5%\n1.1%\n1.5%\n0.9%\n\n\nOther Non-EEA National\n0.7%\n0.4%\n0.1%\n0.4%\n0.5%\n0.7%\n0.4%\n\n\n&lt;NA&gt; (n)\n601\n92\n15\n7\n5\n5\n25\n\n\nTotal (n)\n3,755\n819\n829\n787\n741\n763\n788\n\n\n\n\n\n\n\n\n\nEnglish 2nd language\n\n\nCode\nimport numpy as np\nchr = (\n    dpw_snapshots[[\"snapshot_dt\", \"english_2nd_language\"]]\n)\nn = chr.groupby(\"snapshot_dt\", sort=False).value_counts(sort=False, dropna=False).unstack(0)\npct = chr.groupby(\"snapshot_dt\", sort=False).value_counts(sort=False, normalize=True).unstack(0)\nfor t in [n, pct]: t[\"C/S Mean\"] = t[t.columns[t.columns.str.startswith(\"20\")]].mean(axis=1)\n# row_order = [i for i in pct[\"C/S Mean\"].sort_values(ascending=False).index if i not in [\"Other country\", \"Unknown\"]] + [\"Other country\", \"Unknown\"]\n# pct = pct.reindex(row_order)\npct = pct.mul(100).map(lambda p: f\"{p:.1f}%\")\npct.columns = pct.columns.astype(\"string\")\nif n.index.isna().sum() != 0: pct.loc[\"&lt;NA&gt; (n)\", :] = n.loc[n.index.isna()].iloc[0, :].map(lambda x: f\"{x:,.0f}\")\npct.loc[\"Total (n)\", :] = n.sum().map(lambda x: f\"{x:,.0f}\")\nfor t in [n, pct]: c = t.pop(\"Longitudinal\"); t.insert(0, \"Longitudinal\", c)\ncharacteristics[\"English 2nd Language\"] = pct\ndisplay(pct)\n\n\n\n\n\n\n\n\nsnapshot_dt\nLongitudinal\n2019-04-29\n2020-10-28\n2022-04-30\n2023-10-30\n2025-04-30\nC/S Mean\n\n\nenglish_2nd_language\n\n\n\n\n\n\n\n\n\n\n\nNo\n89.3%\n89.9%\n89.7%\n89.8%\n89.7%\n87.7%\n89.4%\n\n\nYes\n10.7%\n10.1%\n10.3%\n10.2%\n10.3%\n12.3%\n10.6%\n\n\nTotal (n)\n3,755\n819\n829\n787\n741\n763\n788\n\n\n\n\n\n\n\n\n\nReferrer\n\n\nCode\nimport numpy as np\nchr = dpw_snapshots[[\"snapshot_dt\", \"ref_agency\"]]\ncats = chr[\"ref_agency\"].cat.categories\ncat_map = {\n    \"B.C.C. Homelessness Prevention Team\": [\"B.C.C. Homelessness Prevention Team\"],\n    \"Other B.C.C.\": list(cats[cats.str.startswith((\"B.C.C\", \"Bcc\")) & (cats != \"B.C.C. Homelessness Prevention Team\")]),\n    \"St. Mungo's\": [\"St. Mungo's\", \"St Mungo's\", \"Outreach\"],\n    \"A.R.A.\": [\"A.R.A\"],\n    \"Other BAHSA providers\": [\"E.C.H.G. Riverside\", \"Elim\", \"Livewest\", \"Missing Link\", \"Places For People\",\n                              \"Salvation Army\", \"Second Step\", \"Self Help\"]\n}\ncat_map[\"Others\"] = cats[~cats.isin(sum(list(cat_map.values()), []))]  # Add any other categories to \"Others\"\nmapper = {k:v for v,k in pd.Series(cat_map).explode().items()}\nchr[\"ref_agency\"] = chr[\"ref_agency\"].replace(mapper).astype(\"string\").astype(\"category\")\nn = chr.groupby(\"snapshot_dt\", sort=False).value_counts(sort=False, dropna=False).unstack(0)\npct = chr.groupby(\"snapshot_dt\", sort=False).value_counts(sort=False, normalize=True).unstack(0)\nfor t in [n, pct]: t[\"C/S Mean\"] = t[t.columns[t.columns.str.startswith(\"20\")]].mean(axis=1)\npct = pct.sort_values(\"Longitudinal\", ascending=False)\nbcc_rows = pct.index.str.contains(\"B.C.C.\")\npct.index = pd.MultiIndex.from_tuples([(\"B.C.C. (combined)\", k) if \"B.C.C\" in k else (k, \"\") for k, v in pct.iterrows()])\nbcc_pct = pct[bcc_rows].groupby(level=0).sum()\nbcc_pct.index = pd.MultiIndex.from_tuples([(k, \"\") for k, v in bcc_pct.iterrows()])\npct = pd.concat([bcc_pct, pct[bcc_rows], pct[~bcc_rows]]).rename_axis([\"Referral agency\", None])\npct = pct.mul(100).map(lambda p: f\"{p:.1f}%\")\npct.columns = pct.columns.astype(\"string\")\npct.loc[(\"&lt;NA&gt; (n)\", \"\"), :] = n.loc[n.index.isna()].iloc[0, :].map(lambda x: f\"{x:,.0f}\")\npct.loc[(\"Total (n)\", \"\"), :] = n.sum().map(lambda x: f\"{x:,.0f}\")\nfor t in [n, pct]: c = t.pop(\"Longitudinal\"); t.insert(0, \"Longitudinal\", c)\ncharacteristics[\"Referral agencies\"] = pct\ndisplay(pct)\n\n\n\n\n\n\n\n\n\nsnapshot_dt\nLongitudinal\n2019-04-29\n2020-10-28\n2022-04-30\n2023-10-30\n2025-04-30\nC/S Mean\n\n\nReferral agency\n\n\n\n\n\n\n\n\n\n\n\n\nB.C.C. (combined)\n\n48.5%\n33.2%\n41.5%\n47.1%\n49.9%\n45.1%\n43.4%\n\n\nB.C.C. Homelessness Prevention Team\n45.3%\n30.6%\n38.4%\n44.7%\n48.5%\n43.0%\n41.0%\n\n\nOther B.C.C.\n3.1%\n2.6%\n3.1%\n2.4%\n1.4%\n2.1%\n2.3%\n\n\nSt. Mungo's\n\n21.6%\n29.6%\n26.2%\n21.7%\n18.6%\n18.5%\n22.9%\n\n\nA.R.A.\n\n17.5%\n22.1%\n21.3%\n17.2%\n16.6%\n20.0%\n19.4%\n\n\nOther BAHSA providers\n\n8.2%\n11.5%\n7.9%\n10.3%\n12.6%\n14.3%\n11.3%\n\n\nOthers\n\n4.2%\n3.6%\n3.1%\n3.7%\n2.3%\n2.1%\n3.0%\n\n\n&lt;NA&gt; (n)\n\n226\n313\n313\n322\n312\n342\n320\n\n\nTotal (n)\n\n3,755\n819\n829\n787\n741\n763\n788\n\n\n\n\n\n\n\n\n\nCombine together for Excel file\n\n\nCode\nextra_cols_start = [\"PD\", \"5D\", \"3D\", \"Src\"]\nextra_cols_end = [\"Ref_HSH\", \"Ref_HSH cat\"]\nfor k, v in characteristics.items():\n    for ec in extra_cols_start:\n        if ec not in v.columns:\n            v.insert(0, ec, \"\", allow_duplicates=True)\n    for ec in extra_cols_end:\n        if ec not in v.columns:\n            v.insert(len(v.columns), ec, \"\", allow_duplicates=True)\n    if not isinstance(v.index, pd.MultiIndex):\n        v.index = pd.MultiIndex.from_tuples([(k, \"\") for k, v in v.iterrows()])\ncombined = pd.concat(characteristics.values(), keys=characteristics.keys())\nexcel_output[\"Table A.1: Individual characteristics of BAHSA population with comparable reference data\"] = combined\n\n\n\n\n\nFigure C.1: Years of birth (whole BAHSA population)\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nysob = dpw_clis.groupby(\"o_cli_id\").head(1).yob.rename(\"Year of birth\")\nsns.set()\nsns.histplot(ysob, binwidth=3)\nfig_c_1 = \"img_output/Figure_C.1.svg\"\nplt.savefig(fig_c_1)"
  },
  {
    "objectID": "Table_A.1.html#tables-in-an-excel-file",
    "href": "Table_A.1.html#tables-in-an-excel-file",
    "title": "A (Appendix) - Table A.1",
    "section": "Tables in an Excel file",
    "text": "Tables in an Excel file\n\nCode\nimport os\nsheetname = \"Sheet1\"\nqmdfile = os.getenv(\"QUARTO_DOCUMENT_FILE\")\nfilename = \"img_output/\"+qmdfile[:-4]+\".xlsx\"\nwith pd.ExcelWriter(filename) as writer:\n    next_row = 0\n    for k, v in excel_output.items():\n        pd.DataFrame([k]).to_excel(writer, sheet_name=sheetname, header=False, index=False, startrow=next_row)\n        next_row += 1\n        v.to_excel(writer, sheet_name=sheetname, startrow=next_row, float_format=\"%.1f\")\n        next_row += (\n            (1 if not isinstance(v.columns, pd.MultiIndex) else 1 + len(v.columns.levels))\n            + len(v) + 1)\nprint(f\"[Excel file](./{filename})\")\n\nExcel file"
  }
]